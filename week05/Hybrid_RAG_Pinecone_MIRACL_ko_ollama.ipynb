{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 고급 RAG 실습 (v2): .env + Pinecone + Ollama\n",
        "- 날짜: 2025-09-29\n",
        "- 이 노트북은 **.env**에서 환경변수를 불러와 **Pinecone**과 **Ollama**를 함께 사용하는 하이브리드 RAG 파이프라인을 제공합니다.\n",
        "- 구성: **MIRACL-ko 공개 데이터** → Dense(BGE-M3) + Sparse(BM25) → RRF/Weighted → **BGE Reranker** → **Ollama 생성**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) .env 템플릿 (로컬에 파일 생성)\n",
        "아래 내용을 프로젝트 루트의 `.env` 파일에 저장하세요.\n",
        "\n",
        "```\n",
        "PINECONE_API_KEY=pc_********************************\n",
        "PC_CLOUD=aws\n",
        "PC_REGION=us-east-1\n",
        "PC_INDEX_NAME=hybrid-miracl-ko\n",
        "\n",
        "# Ollama 기본 호스트 (로컬 실행 시)\n",
        "OLLAMA_HOST=http://localhost:11434\n",
        "# 선호 모델 예: llama3.1:8b-instruct, qwen2.5:7b-instruct 등\n",
        "OLLAMA_MODEL=llama3.1:8b-instruct\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install -U pip\n",
        "!pip -q install -U python-dotenv requests pinecone pinecone-text FlagEmbedding datasets rank_bm25 matplotlib tqdm\n",
        "# 선택: torch 최신 (환경에 따라 생략 가능)\n",
        "# !pip -q install -U torch --index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9f77f239",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pinecone 7.3.0\n",
            "Uninstalling pinecone-7.3.0:\n",
            "  Successfully uninstalled pinecone-7.3.0\n",
            "\u001b[33mWARNING: Skipping pinecone-client as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pinecone-plugin-inference as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pinecone\n",
            "  Using cached pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from pinecone) (2025.8.3)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from pinecone) (1.8.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from pinecone) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "Requirement already satisfied: six>=1.5 in /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Using cached pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "Installing collected packages: pinecone\n",
            "Successfully installed pinecone-7.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y pinecone pinecone-client pinecone-plugin-inference\n",
        "!pip install -U pinecone\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 환경 변수 로드 (.env) & 구성\n",
        "- `.env`를 읽어 **PINECONE_API_KEY**, **OLLAMA_HOST**, 기타 파라미터를 설정합니다.\n",
        "- α 프리셋 / N·k 템플릿도 환경 변수 또는 기본값으로 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone: hybrid-miracl-ko aws us-east-1 dotproduct\n",
            "Ollama: http://localhost:11434 llama3.1:8b-instruct\n",
            "Scale: 8000 30  | α= 0.5  N= 100  K= 5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Pinecone\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
        "PC_CLOUD  = os.getenv(\"PC_CLOUD\", \"aws\")\n",
        "PC_REGION = os.getenv(\"PC_REGION\", \"us-east-1\")\n",
        "INDEX_NAME = os.getenv(\"PC_INDEX_NAME\", \"hybrid-miracl-ko\")\n",
        "METRIC = os.getenv(\"PC_METRIC\", \"dotproduct\")\n",
        "\n",
        "# Ollama\n",
        "OLLAMA_HOST  = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
        "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.1:8b-instruct\")\n",
        "\n",
        "# 실습 스케일\n",
        "CORPUS_MAX = int(os.getenv(\"CORPUS_MAX\", \"8000\"))\n",
        "N_QUERIES  = int(os.getenv(\"N_QUERIES\", \"30\"))\n",
        "\n",
        "# α 프리셋\n",
        "ALPHA_PRESETS = {\"balanced\":0.5, \"semantic_heavy\":0.7, \"keyword_heavy\":0.3}\n",
        "ALPHA = float(os.getenv(\"ALPHA\", ALPHA_PRESETS[\"balanced\"]))\n",
        "\n",
        "# N/k 템플릿\n",
        "TEMPLATES = {\n",
        "    \"speed\":   {\"N\": 50,  \"k\": 3},\n",
        "    \"balanced\":{\"N\": 100, \"k\": 5},\n",
        "    \"quality\": {\"N\": 200, \"k\": 10}\n",
        "}\n",
        "N = int(os.getenv(\"RETRIEVER_N\", TEMPLATES[\"balanced\"][\"N\"]))\n",
        "K = int(os.getenv(\"LLM_K\",       TEMPLATES[\"balanced\"][\"k\"]))\n",
        "\n",
        "print(\"Pinecone:\", INDEX_NAME, PC_CLOUD, PC_REGION, METRIC)\n",
        "print(\"Ollama:\", OLLAMA_HOST, OLLAMA_MODEL)\n",
        "print(\"Scale:\", CORPUS_MAX, N_QUERIES, \" | α=\", ALPHA, \" N=\", N, \" K=\", K)\n",
        "assert PINECONE_API_KEY, \"PINECONE_API_KEY is required (set in .env)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Ollama 채팅 함수 (요청하신 패턴 반영)\n",
        "- 아래 함수는 `.env`의 `OLLAMA_HOST`를 기본으로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, time, requests\n",
        "from typing import List, Dict\n",
        "\n",
        "def chat_ollama(model: str, messages: List[Dict], stream: bool = False, host: str = None):\n",
        "    host = host or os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
        "    url = f\"{host}/api/chat\"\n",
        "    headers = {\"Content-Type\":\"application/json\"}\n",
        "    payload = {\"model\": model, \"messages\": messages, \"stream\": stream}\n",
        "    t0 = time.time()\n",
        "    resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)\n",
        "    dt = time.time() - t0\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    text = data.get(\"message\", {}).get(\"content\", \"\")\n",
        "    return text, dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) 공개 데이터 로드 (MIRACL-ko)\n",
        "- 쿼리/정답(dev), 코퍼스(train)을 로드합니다.\n",
        "- 코퍼스는 `CORPUS_MAX` 만큼만 샘플링합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['docid', 'title', 'text'],\n",
            "    num_rows: 500000\n",
            "})\n",
            "{'docid': '5#0', 'title': '지미 카터', 'text': '제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.'}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5#0</td>\n",
              "      <td>지미 카터</td>\n",
              "      <td>제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5#1</td>\n",
              "      <td>지미 카터</td>\n",
              "      <td>지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  doc_id  title                                               text\n",
              "0    5#0  지미 카터  제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미...\n",
              "1    5#1  지미 카터  지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 ..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "queries_ds = load_dataset(\"Cohere/miracl-ko-queries-22-12\", split=\"dev\")\n",
        "# MIRACL 한국어 코퍼스 샤드 (docs-0.jsonl.gz 등)\n",
        "url = \"https://huggingface.co/datasets/miracl/miracl-corpus/resolve/main/miracl-corpus-v1.0-ko/docs-0.jsonl.gz\"\n",
        "\n",
        "corpus_ds = load_dataset(\"json\", data_files=url, split=\"train\")\n",
        "\n",
        "print(corpus_ds)\n",
        "print(corpus_ds[0])\n",
        "\n",
        "corpus_ds = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"https://huggingface.co/datasets/miracl/miracl-corpus/resolve/main/miracl-corpus-v1.0-ko/docs-*.jsonl.gz\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "\n",
        "if CORPUS_MAX and len(corpus_ds) > CORPUS_MAX:\n",
        "    corpus_ds = corpus_ds.select(range(CORPUS_MAX))\n",
        "\n",
        "corpus_df = corpus_ds.to_pandas().rename(columns={\"docid\":\"doc_id\"})\n",
        "corpus_df[\"doc_id\"] = corpus_df[\"doc_id\"].astype(str)\n",
        "\n",
        "# 검색 후 문서 텍스트를 회수하기 위해 lookup dict 준비\n",
        "ID2TEXT  = dict(zip(corpus_df[\"doc_id\"], corpus_df[\"text\"].fillna(\"\").astype(str)))\n",
        "ID2TITLE = dict(zip(corpus_df[\"doc_id\"], corpus_df[\"title\"].fillna(\"\").astype(str)))\n",
        "\n",
        "corpus_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2d7eae3e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(corpus_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Sparse (BM25) 인코더 적합"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8000/8000 [00:03<00:00, 2131.37it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(8000, 8000)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pinecone_text.sparse import BM25Encoder\n",
        "from tqdm import trange\n",
        "import numpy as np, pickle, os\n",
        "\n",
        "texts = (corpus_df[\"title\"].fillna(\"\") + \" \" + corpus_df[\"text\"].fillna(\"\")).tolist()\n",
        "bm25 = BM25Encoder()\n",
        "bm25.fit(texts)\n",
        "sparse_vectors = bm25.encode_documents(texts)\n",
        "\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "with open(\"artifacts/bm25_encoder.pkl\",\"wb\") as f:\n",
        "    pickle.dump(bm25, f)\n",
        "\n",
        "len(sparse_vectors), len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Dense 임베딩: BGE-M3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8dd8dc25",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math, numpy as np\n",
        "\n",
        "CANDIDATE_TEXT_KEYS = (\"text\", \"content\", \"body\", \"doc\", \"passage\", \"sentence\")\n",
        "\n",
        "def to_text(x):\n",
        "    \"\"\"입력 x를 안전한 str로 변환(실패 시 None).\"\"\"\n",
        "    if x is None:\n",
        "        return None\n",
        "    # NaN 처리 (float/np.float 계열)\n",
        "    if isinstance(x, (float, np.floating)) and math.isnan(x):\n",
        "        return None\n",
        "    # bytes -> str\n",
        "    if isinstance(x, (bytes, bytearray)):\n",
        "        try:\n",
        "            return x.decode(\"utf-8\", errors=\"ignore\")\n",
        "        except Exception:\n",
        "            return None\n",
        "    # dict에서 텍스트 필드 추출\n",
        "    if isinstance(x, dict):\n",
        "        for k in CANDIDATE_TEXT_KEYS:\n",
        "            if k in x:\n",
        "                return to_text(x[k])\n",
        "        return None\n",
        "    # (토큰 리스트가 아니라) 문자열 조합 필요한 경우\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        # 이미 토큰화된 list[str]은 허용되지만 여기선 concat해 하나의 문장으로 만듦\n",
        "        if all(isinstance(t, str) for t in x):\n",
        "            return \" \".join(x)\n",
        "        # 그렇지 않으면 문자열화 시도\n",
        "        try:\n",
        "            return \" \".join(map(str, x))\n",
        "        except Exception:\n",
        "            return None\n",
        "    # 넘파이 스칼라/정수 등 → 문자열\n",
        "    if isinstance(x, (int, np.integer)) or isinstance(x, (np.floating,)):\n",
        "        return str(x)\n",
        "    # 그 외 객체 → 문자열화 시도\n",
        "    if not isinstance(x, str):\n",
        "        try:\n",
        "            x = str(x)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "12e945da",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_texts = texts  # 기존 리스트/시리즈/제너레이터 등\n",
        "\n",
        "cleaned_texts = []\n",
        "bad_indices = []\n",
        "for i, x in enumerate(raw_texts):\n",
        "    s = to_text(x)\n",
        "    if s is None:\n",
        "        bad_indices.append(i)\n",
        "        continue\n",
        "    s = s.strip()\n",
        "    if not s:\n",
        "        bad_indices.append(i)\n",
        "        continue\n",
        "    cleaned_texts.append(s)\n",
        "\n",
        "if bad_indices:\n",
        "    print(f\"[warn] {len(bad_indices)}개의 레코드가 문자열이 아니거나 비어 있어 제외됨. 예시 인덱스: {bad_indices[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c27cb64",
      "metadata": {},
      "source": [
        "### 아래 임베딩 모델 로드 및 임베딩 문서 저장 시 메모리 부족이 뜰 수 있어서, Batch 32개씩 진행 (53분 소요), 따라서 더 빠르게 고사양으로 진행하고자 한다면, Batch를 64, 128로 진행해서 시간을 최적화할 것!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 234318.66it/s]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 83.48it/s]\n",
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.64s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 101.29it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:11<00:00, 11.56s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 84.92it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.84s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 134.67it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.48s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 120.05it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.52s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 106.57it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:12<00:00, 12.33s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 74.69it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:13<00:00, 13.52s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 101.15it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.78s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 113.04it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 115.69it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.60s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 148.91it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.27s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 301.23it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.55s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 115.01it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.84s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 115.86it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.86s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 109.16it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.22s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 91.21it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:16<00:00, 16.14s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 58.01it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.51s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 109.86it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.84s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 158.60it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 196.39it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.43s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 218.29it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.53s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 151.12it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 228.00it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.11s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 287.99it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 103.56it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:11<00:00, 11.25s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 40.47it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.61s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 90.81it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.69s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 136.11it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.55s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 171.26it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.51s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 76.49it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.02s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 307.03it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.27s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 235.50it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.18s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 129.51it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.44s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 158.13it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.52s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 187.97it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.09s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 147.88it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:12<00:00, 12.28s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 283.11it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.08s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 173.19it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.10s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 208.99it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 286.63it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 188.71it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.25s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 116.43it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:14<00:00, 14.78s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 65.81it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00, 10.00s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 257.73it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  3.00s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 217.95it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.45s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 107.34it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.60s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 89.61it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:11<00:00, 11.45s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 56.54it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:11<00:00, 11.42s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 83.59it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:11<00:00, 11.14s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 100.30it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.87s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 125.91it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.97s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 113.91it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 110.10it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.53s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 120.07it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.58s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 198.49it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.77s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 170.71it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.05s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 87.91it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.23s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 153.09it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 189.04it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:11<00:00, 11.40s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 179.24it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.36s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 211.77it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.87s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 308.54it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 562.47it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 383.57it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 174.94it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.54s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 163.74it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.64s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 114.09it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:10<00:00, 10.13s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 133.96it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.83s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 189.54it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.83s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 177.94it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.89s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 197.59it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.23s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 189.53it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.85s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 147.84it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.35s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 174.65it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.68s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 161.29it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.71s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 231.10it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.75s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 165.01it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 188.86it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.68s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 215.10it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.12s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 171.90it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.69s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 156.78it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.69s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 201.48it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.70s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 184.54it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.34s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 148.17it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.49s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 105.51it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:12<00:00, 12.77s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 63.28it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.65s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 61.05it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:12<00:00, 12.80s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 164.87it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.52s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 168.69it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.73s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 167.53it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.68s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 137.30it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.39s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 179.27it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.69s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 157.70it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 161.29it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.45s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 63.64it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:19<00:00, 19.33s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 49.22it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.13s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 100.10it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.78s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 134.08it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.32s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 130.04it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:10<00:00, 10.00s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 57.07it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 79.61it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.82s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 101.66it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 205.80it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 334.90it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 152.63it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.76s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 204.16it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.99s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 190.73it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.89s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 98.41it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.60s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 126.62it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 65.96it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 74.41it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.84s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 48.27it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:13<00:00, 13.66s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 79.47it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.97s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 125.74it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.77s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 127.80it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.47s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 172.51it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:10<00:00, 10.75s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 123.45it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.57s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 145.74it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.71s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 172.65it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.75s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 169.17it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.17s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 165.92it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.23s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 109.41it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:18<00:00, 18.89s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 47.08it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.87s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 118.12it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.29s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 152.05it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.33s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 173.10it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.04s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 147.08it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.66s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 149.61it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.57s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 152.20it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.65s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 236.86it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.75s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 108.58it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.73s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 120.10it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.70s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 187.86it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.07s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 120.44it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 168.98it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.07s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 121.34it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.48s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 142.00it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.51s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 48.43it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.05s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 118.18it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.45s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 237.03it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.48s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 118.06it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.39s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 146.28it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.05s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 133.02it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.91s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 93.25it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:11<00:00, 11.85s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 72.89it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.91s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 181.29it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.79s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 110.32it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.91s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 111.77it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.36s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 111.99it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:12<00:00, 12.72s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 154.56it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:18<00:00, 18.35s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 28.08it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:20<00:00, 20.75s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 54.64it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.76s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 101.80it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 90.20it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:13<00:00, 13.67s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 45.26it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:12<00:00, 12.01s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 159.33it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.29s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 102.70it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.83s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 196.26it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.66s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 172.87it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.79s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 137.02it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.21s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 105.06it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.94s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 108.24it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 93.73it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.36s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 119.64it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.15s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 123.58it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.75s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 147.14it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.12s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 142.63it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.55s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 118.75it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.60s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 158.93it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.32s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 150.94it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.72s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 133.26it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 174.79it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 123.14it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.99s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 78.83it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:14<00:00, 14.49s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 49.67it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 114.98it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.68s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 181.09it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.31s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 87.66it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:15<00:00, 15.88s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 33.17it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:10<00:00, 10.13s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 78.06it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:12<00:00, 12.71s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 182.22it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.80s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 86.29it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.88s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 114.27it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.65s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 157.85it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.27s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 182.71it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.87s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 131.01it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 101.30it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.42s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 178.12it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.62s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 127.65it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:10<00:00, 10.26s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 258.54it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 314.84it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.94s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 187.72it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.22s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 153.04it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.24s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 120.54it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.80s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 78.72it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:12<00:00, 12.78s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 50.03it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.64s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 91.48it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.39s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 66.29it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:15<00:00, 15.15s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 45.56it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.86s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 71.73it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.19s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 79.06it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:14<00:00, 14.32s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 52.11it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:13<00:00, 13.03s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 47.54it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.67s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 71.42it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.98s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 81.11it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.86s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 136.69it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.93s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 124.70it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.07s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 58.69it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.28s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 109.44it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.86s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 162.71it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 172.74it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.03s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 101.66it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.28s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 15.56it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.64s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 76.13it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.65s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 72.26it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.87s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 118.91it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:12<00:00, 12.90s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 57.19it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.52s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 131.16it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.32s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 108.69it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.78s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 20.91it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.51s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 82.41it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:08<00:00,  8.79s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 116.46it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.61s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 183.49it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.60s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 201.79it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 363.24it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 250.06it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.42s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 468.48it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 390.46it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 563.75it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 453.34it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 576.06it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 549.78it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 480.56it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 570.81it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 74.61it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 162.07it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.61s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 110.87it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.05s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 114.90it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.00s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 162.17it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.13s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 109.30it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.36s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 147.75it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.82s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 118.08it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:10<00:00, 10.71s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 35.52it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 88.52it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 100.55it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.90s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 121.74it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.25s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 112.97it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 65.11it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.53s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 144.97it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.24s/it]\n",
            "pre tokenize: 100%|██████████| 1/1 [00:00<00:00, 112.25it/s]\n",
            "Inference Embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.60s/it]\n",
            "100%|██████████| 250/250 [53:32<00:00, 12.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8000, 1024) 1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os, torch, numpy as np\n",
        "from tqdm import trange\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "\n",
        "# 🤫 토크나이저 멀티프로세스 경고 억제용 환경변수.\n",
        "# 병렬 토크나이징으로 인한 경고/출력 섞임을 피하고 싶을 때 False로 둡니다.\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ⚙️ 사용 가능한 CUDA(GPU)가 있는지 확인\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# 장치 선택: GPU가 있으면 \"cuda:0\", 없으면 \"cpu\"\n",
        "device = \"cuda:0\" if use_cuda else \"cpu\"\n",
        "\n",
        "# FP16(half precision) 사용 여부: 보통 GPU가 있을 때만 켭니다.\n",
        "# - 장점: 메모리 절약, 속도 향상 가능\n",
        "# - 단점: 아주 드문 경우 수치 정밀도 이슈\n",
        "use_fp16 = use_cuda\n",
        "\n",
        "# 🧠 BGE-M3 임베딩 모델 로드\n",
        "# - \"BAAI/bge-m3\" 허깅페이스 허브에서 받아옵니다(최초 1회 캐시 후 재사용).\n",
        "# - use_fp16: 위에서 결정한 half precision 사용\n",
        "# - devices: 어떤 디바이스에 올릴지 지정 (예: \"cuda:0\" 또는 \"cpu\")\n",
        "bge = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=use_fp16, devices=device)\n",
        "\n",
        "# ✂️ 최대 토큰 길이: 512~1024 권장(길수록 더 긴 문맥을 커버하지만 속도/VRAM 증가)\n",
        "MAX_LEN = 1024\n",
        "\n",
        "# 📦 배치 크기: 16~32부터 시작해서 VRAM/메모리에 맞춰 조정\n",
        "BATCH   = 32\n",
        "\n",
        "# 모든 문서의 dense 임베딩을 담아둘 리스트 (배치별로 쌓았다가 마지막에 합칩니다)\n",
        "dense_vecs = []\n",
        "\n",
        "# 🔒 추론 모드: 자동 미분 OFF → 메모리 절약 & 속도 향상\n",
        "with torch.inference_mode():\n",
        "    # trange: 진행상황(progress bar)을 보여주는 range\n",
        "    for i in trange(0, len(cleaned_texts), BATCH):\n",
        "        # 현재 배치 슬라이싱 (✅ 반드시 list[str] 형태여야 합니다)\n",
        "        batch = cleaned_texts[i:i+BATCH]\n",
        "\n",
        "        # ✅ 임베딩 추출\n",
        "        # - batch_size: 내부 처리 배치 크기 (보통 BATCH와 동일하게 둠)\n",
        "        # - max_length: 토큰 최대 길이(초과분은 모델 토크나이저에서 잘립니다)\n",
        "        # - return_dense: dense 임베딩(ANN, 벡터DB용) 반환\n",
        "        # - return_sparse: BM25 유사한 sparse(토큰 기반) 벡터 반환 여부 (여기선 미사용)\n",
        "        # - return_colbert_vecs: ColBERT 스타일 토큰 단위 벡터 반환 여부 (여기선 미사용)\n",
        "        out = bge.encode(\n",
        "            batch,\n",
        "            batch_size=BATCH,\n",
        "            max_length=MAX_LEN,\n",
        "            return_dense=True,\n",
        "            return_sparse=False,\n",
        "            return_colbert_vecs=False,\n",
        "        )\n",
        "\n",
        "        # 모델 출력에서 dense 임베딩(Numpy ndarray)을 꺼냅니다. shape: (batch, dim)\n",
        "        dense = out[\"dense_vecs\"]\n",
        "\n",
        "        # 🔄 L2 정규화: 각 벡터를 단위벡터로(normalize) 만들어 코사인 유사도 계산을 안정화\n",
        "        # - np.linalg.norm(..., axis=1, keepdims=True): 각 행(문서)별 L2 노름\n",
        "        # - 분모가 0인(edge) 경우가 드물지만 있을 수 있으니, 필요시 eps를 더해 방어코드 추가 가능\n",
        "        dense = dense / np.linalg.norm(dense, axis=1, keepdims=True)\n",
        "\n",
        "        # 배치 결과를 리스트에 쌓아둠\n",
        "        dense_vecs.append(dense)\n",
        "\n",
        "# 🔧 여러 배치로 쌓인 배열들을 한 번에 세로로 이어붙이기\n",
        "# - 결과 shape: (num_texts, dim)\n",
        "dense_vecs = np.vstack(dense_vecs).astype(\"float32\")  # 벡터DB 호환/메모리 절약 위해 float32 캐스팅\n",
        "\n",
        "# 벡터 차원(dimension) 확인\n",
        "dense_dim = dense_vecs.shape[1]\n",
        "\n",
        "# 전체 개수와 차원 출력 (예: (N, 1024) 1024)\n",
        "print(dense_vecs.shape, dense_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Pinecone 서버리스 인덱스 생성 & 업서트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da02ff74",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dimension': 1024,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'dotproduct',\n",
            " 'namespaces': {'': {'vector_count': 3600}},\n",
            " 'total_vector_count': 3600,\n",
            " 'vector_type': 'dense'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 600/8000 [00:53<10:30, 11.74it/s]"
          ]
        }
      ],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=int(dense_dim),\n",
        "        metric=METRIC,\n",
        "        spec=ServerlessSpec(cloud=PC_CLOUD, region=PC_REGION)\n",
        "    )\n",
        "\n",
        "index = pc.Index(INDEX_NAME)\n",
        "print(index.describe_index_stats())\n",
        "\n",
        "\n",
        "batch = []\n",
        "for i in trange(len(corpus_df)):\n",
        "    _id = corpus_df.iloc[i][\"doc_id\"]\n",
        "    meta = {\"title\": ID2TITLE[_id], \"lang\":\"ko\", \"source\":\"miracl-ko-wiki\"}\n",
        "    batch.append({\n",
        "        \"id\": _id,\n",
        "        \"values\": dense_vecs[i].tolist(),\n",
        "        \"sparse_values\": sparse_vectors[i],\n",
        "        \"metadata\": meta\n",
        "    })\n",
        "    if len(batch) >= 200:\n",
        "        index.upsert(vectors=batch)\n",
        "        batch = []\n",
        "if batch:\n",
        "    index.upsert(vectors=batch)\n",
        "\n",
        "print(\"Upsert complete.\")\n",
        "print(index.describe_index_stats())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) 검색 함수: Dense / Sparse / Weighted(α) / RRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "from collections import defaultdict\n",
        "\n",
        "def encode_query_dense(q: str):\n",
        "    return bge.encode([q], max_length=8192)[\"dense_vecs\"][0].astype(\"float32\")\n",
        "\n",
        "def encode_query_sparse(q: str):\n",
        "    return bm25.encode_queries([q])[0]\n",
        "\n",
        "def dense_only_search(query: str, top_k=20):\n",
        "    dv = encode_query_dense(query).tolist()\n",
        "    res = index.query(vector=dv, top_k=top_k, include_metadata=True)\n",
        "    return res.matches\n",
        "\n",
        "def sparse_only_search(query: str, top_k=20):\n",
        "    sv = encode_query_sparse(query)\n",
        "    res = index.query(sparse_vector=sv, top_k=top_k, include_metadata=True)\n",
        "    return res.matches\n",
        "\n",
        "def hybrid_weighted_search(query: str, top_k=20, alpha=ALPHA):\n",
        "    dv = encode_query_dense(query)\n",
        "    sv = encode_query_sparse(query)\n",
        "    dv = (dv * alpha).tolist()\n",
        "    sv_scaled = {\"indices\": sv[\"indices\"], \"values\": [v*(1.0-alpha) for v in sv[\"values\"]]}\n",
        "    res = index.query(vector=dv, sparse_vector=sv_scaled, top_k=top_k, include_metadata=True)\n",
        "    return res.matches\n",
        "\n",
        "def rrf_fusion(query: str, top_k=20, per_list_k=50, k_const=60):\n",
        "    dres = dense_only_search(query, top_k=per_list_k)\n",
        "    sres = sparse_only_search(query, top_k=per_list_k)\n",
        "    meta = {}\n",
        "    ranks = defaultdict(list)\n",
        "    for rlist in [dres, sres]:\n",
        "        for rank, m in enumerate(rlist, start=1):\n",
        "            meta[m.id] = m.metadata\n",
        "            ranks[m.id].append(rank)\n",
        "    scores = []\n",
        "    for _id, rks in ranks.items():\n",
        "        sc = sum(1.0 / (k_const + rk) for rk in rks)\n",
        "        scores.append((_id, sc))\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [{\"id\": _id, \"score\": sc, \"metadata\": meta.get(_id, {})} for _id, sc in scores[:top_k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Re-rank (Cross-Encoder): BGE Reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from FlagEmbedding import FlagReranker\n",
        "\n",
        "reranker = FlagReranker(\"BAAI/bge-reranker-v2-m3\", use_fp16=use_fp16)\n",
        "\n",
        "def rerank_ce(query: str, candidates: List[Dict], top_k=K):\n",
        "    pairs, id2meta = [], {}\n",
        "    for m in candidates:\n",
        "        if hasattr(m, \"id\"):\n",
        "            _id, meta = m.id, m.metadata\n",
        "        else:\n",
        "            _id, meta = m[\"id\"], m.get(\"metadata\", {})\n",
        "        title = meta.get(\"title\",\"\")\n",
        "        pairs.append([query, title])\n",
        "        id2meta[_id] = meta\n",
        "    scores = reranker.compute_score(pairs)\n",
        "    items = [{\"id\": _id, \"score\": float(sc), \"metadata\": id2meta[_id]} for (_id, sc) in zip(id2meta.keys(), scores)]\n",
        "    items.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    return items[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) 생성(Answer): Ollama + 근거 컨텍스트 주입"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_context(doc_ids: List[str], max_chars: int = 3000) -> str:\n",
        "    parts, total = [], 0\n",
        "    for did in doc_ids:\n",
        "        t = ID2TITLE.get(did, \"\")\n",
        "        x = ID2TEXT.get(did, \"\")\n",
        "        snippet = (x[:700] + \"...\") if len(x) > 700 else x\n",
        "        block = f\"[{did}] {t}\\n{snippet}\"\n",
        "        if total + len(block) > max_chars:\n",
        "            break\n",
        "        parts.append(block)\n",
        "        total += len(block)\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "def answer_with_ollama(query: str, topk_items: List[Dict], model: str = None):\n",
        "    model = model or os.getenv(\"OLLAMA_MODEL\", \"llama3.1:8b-instruct\")\n",
        "    doc_ids = [m[\"id\"] if isinstance(m, dict) else m.id for m in topk_items]\n",
        "    ctx = collect_context(doc_ids, max_chars=3000)\n",
        "\n",
        "    system = (\n",
        "        \"당신은 한국어 RAG 어시스턴트입니다. 아래 '근거 컨텍스트'에 포함된 내용만 사용하여 간결하고 정확하게 답하세요. \"\n",
        "        \"확실하지 않으면 모른다고 말하고 추가 정보를 요청하세요. \"\n",
        "        \"필요하면 문서 ID로 각 근거를 표기하세요.\"\n",
        "    )\n",
        "    user = f\"질문: {query}\\n\\n[근거 컨텍스트]\\n{ctx}\"\n",
        "    messages = [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
        "    text, dt = chat_ollama(model=model, messages=messages, stream=False)\n",
        "    return text, dt, ctx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) 데모: 질의 → Hybrid+CE → Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_queries = [q for q in queries_ds[\"query\"][:10] if len(q) > 5][:3]\n",
        "\n",
        "for q in demo_queries:\n",
        "    print(\"=\"*100)\n",
        "    print(\"질문:\", q)\n",
        "    cand = hybrid_weighted_search(q, top_k=N, alpha=ALPHA)\n",
        "    topk = rerank_ce(q, cand, top_k=K)\n",
        "    try:\n",
        "        answer, dt, ctx = answer_with_ollama(q, topk)\n",
        "        print(\"[생성 소요]\", f\"{dt:.2f}s\")\n",
        "        print(\"[답변]\\n\", answer)\n",
        "        print(\"\\n[근거 컨텍스트]\\n\", ctx[:1000], \"...\")\n",
        "    except Exception as e:\n",
        "        print(\"Ollama 호출 실패:\", e)\n",
        "        print(\"→ OLLAMA_HOST 설정 및 서버 실행 여부를 확인하세요.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2df3f7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ajou-llmops",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
