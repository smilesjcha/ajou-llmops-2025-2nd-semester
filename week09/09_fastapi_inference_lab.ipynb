{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030786d4",
   "metadata": {},
   "source": [
    "# Week09 — 추론 최적화 × FastAPI × 로드테스트 실습\n",
    "**구성:** 이론(60') + 실습(40') + 과제\n",
    "\n",
    "이 노트북은 다음을 단계별로 실습할 수 있게 구성되어 있습니다.\n",
    "\n",
    "1. 환경 변수(.env) 템플릿 생성 및 로딩\n",
    "2. 미니 챗봇 API(FastAPI) — 캐시/레이트리밋/스트리밍\n",
    "3. (기본) Mock LLM / (옵션) OpenAI 연동 자리\n",
    "4. 부하 테스트 스크립트(Locust, k6) 자동 생성\n",
    "5. Postman/Insomnia 컬렉션 자동 생성\n",
    "6. 캐시 키 실습, 레이트리밋 토큰버킷 실습\n",
    "7. 실행/배포/측정 가이드 및 과제 안내\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2254ea",
   "metadata": {},
   "source": [
    "## 0) 사전 준비 파일 자동 생성\n",
    "이 셀을 실행하면 **같은 폴더**에 아래 파일들이 생성됩니다.\n",
    "- `.env.sample` — 환경 변수 템플릿\n",
    "- `requirements_week09.txt` — 권장 패키지 목록\n",
    "- `locustfile.py` — Locust 부하 테스트 스크립트\n",
    "- `k6-loadtest.js` — k6 부하 테스트 스크립트\n",
    "- `week09_postman_collection.json` — Postman 컬렉션\n",
    "- `week09_insomnia_export.json` — Insomnia 워크스페이스(뼈대)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92388a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts written:\n",
      "- .env.sample\n",
      "- requirements_week09.txt\n",
      "- locustfile.py\n",
      "- k6-loadtest.js\n",
      "- week09_postman_collection.json\n",
      "- week09_insomnia_export.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "import json, uuid, datetime\n",
    "\n",
    "# 경로(노트북과 같은 폴더에 생성)\n",
    "ENV_SAMPLE = Path(\".env.sample\")\n",
    "REQS = Path(\"requirements_week09.txt\")\n",
    "LOCUST = Path(\"locustfile.py\")\n",
    "K6 = Path(\"k6-loadtest.js\")\n",
    "POSTMAN = Path(\"week09_postman_collection.json\")\n",
    "INSOMNIA = Path(\"week09_insomnia_export.json\")\n",
    "\n",
    "# 1) .env.sample\n",
    "ENV_SAMPLE.write_text(dedent(\"\"\"\n",
    "# === Week09 Lab .env (샘플) ===\n",
    "OPENAI_API_KEY=\"sk-~~~\"\n",
    "LANGFUSE_PUBLIC_KEY=\"pk-lf-~~~\"\n",
    "LANGFUSE_SECRET_KEY=\"sk-lf-~~~\"\n",
    "LANGFUSE_HOST=\"https://cloud.langfuse.com\"\n",
    "PINECONE_API_KEY=\"pcsk_~~~\"\n",
    "\n",
    "# 선택: Redis (로컬 Docker 기본값)\n",
    "REDIS_URL=\"redis://localhost:6379/0\"\n",
    "\n",
    "# 모델/서비스 설정\n",
    "MODEL_PROVIDER=\"mock\"  # \"mock\" | \"openai\"\n",
    "MODEL_NAME=\"gpt-4o-mini\"  # openai 사용 시\n",
    "RESPONSE_TTL_SECONDS=300\n",
    "RATE_LIMIT_RPS=5\n",
    "RATE_LIMIT_BURST=10\n",
    "\"\"\"), encoding=\"utf-8\")\n",
    "\n",
    "# 2) requirements\n",
    "REQS.write_text(dedent(\"\"\"\n",
    "fastapi>=0.115\n",
    "uvicorn>=0.30\n",
    "pydantic>=2.7\n",
    "python-dotenv>=1.0\n",
    "httpx>=0.27\n",
    "redis>=5.0\n",
    "orjson>=3.10\n",
    "# openai>=1.52  # 필요 시 사용\n",
    "# sse-starlette>=2.1  # (선택) SSE 라이브러리, 본 실습은 수동 스트리밍로 처리\n",
    "\"\"\"), encoding=\"utf-8\")\n",
    "\n",
    "# 3) Locust\n",
    "LOCUST.write_text(dedent(\"\"\"\n",
    "from locust import HttpUser, task, between\n",
    "import json, random, string\n",
    "\n",
    "def rand_prompt(n=12):\n",
    "    return ''.join(random.choices(string.ascii_letters + ' ', k=n))\n",
    "\n",
    "class ChatUser(HttpUser):\n",
    "    wait_time = between(0.5, 1.5)\n",
    "    @task(3)\n",
    "    def chat(self):\n",
    "        payload = {\"userId\":\"locust\", \"message\": f\"Hello {rand_prompt()}\"}\n",
    "        self.client.post(\"/chat\", json=payload)\n",
    "    @task(1)\n",
    "    def stream(self):\n",
    "        payload = {\"userId\":\"locust\", \"message\": f\"Stream {rand_prompt()}\"}\n",
    "        self.client.post(\"/chat/stream\", json=payload)\n",
    "\"\"\"), encoding=\"utf-8\")\n",
    "\n",
    "# 4) k6\n",
    "K6.write_text(dedent(\"\"\"\n",
    "// k6 run k6-loadtest.js --vus 20 --duration 60s\n",
    "import http from 'k6/http';\n",
    "import { check, sleep } from 'k6';\n",
    "\n",
    "export default function () {\n",
    "  let payload = JSON.stringify({ userId: \"k6\", message: \"hello from k6\" });\n",
    "  let headers = { 'Content-Type': 'application/json' };\n",
    "  let res = http.post('http://localhost:8000/chat', payload, { headers });\n",
    "  check(res, { 'status was 200': (r) => r.status === 200 });\n",
    "  sleep(1);\n",
    "}\n",
    "\"\"\"), encoding=\"utf-8\")\n",
    "\n",
    "# 5) Postman 컬렉션\n",
    "collection = {\n",
    "  \"info\": {\n",
    "    \"name\": \"Week09 Mini Chatbot API\",\n",
    "    \"_postman_id\": str(uuid.uuid4()),\n",
    "    \"schema\": \"https://schema.getpostman.com/json/collection/v2.1.0/collection.json\"\n",
    "  },\n",
    "  \"item\": [\n",
    "    {\n",
    "      \"name\": \"Chat\",\n",
    "      \"request\": {\n",
    "        \"method\": \"POST\",\n",
    "        \"header\": [{\"key\": \"Content-Type\", \"value\": \"application/json\"}],\n",
    "        \"url\": \"http://localhost:8000/chat\",\n",
    "        \"body\": {\"mode\":\"raw\",\"raw\":\"{\\n  \\\"userId\\\": \\\"demo\\\",\\n  \\\"message\\\": \\\"Hello!\\\",\\n  \\\"params\\\": {}\\n}\"}\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Chat Stream\",\n",
    "      \"request\": {\n",
    "        \"method\": \"POST\",\n",
    "        \"header\": [{\"key\": \"Content-Type\", \"value\": \"application/json\"}],\n",
    "        \"url\": \"http://localhost:8000/chat/stream\",\n",
    "        \"body\": {\"mode\":\"raw\",\"raw\":\"{\\n  \\\"userId\\\": \\\"demo\\\",\\n  \\\"message\\\": \\\"Stream please\\\",\\n  \\\"params\\\": {}\\n}\"}\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "POSTMAN.write_text(json.dumps(collection, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# 6) Insomnia (간단 워크스페이스 골격)\n",
    "insomnia = {\n",
    "  \"_type\": \"export\",\n",
    "  \"__export_format\": 4,\n",
    "  \"__export_date\": datetime.datetime.now().isoformat(),\n",
    "  \"__export_source\": \"week09-generator\",\n",
    "  \"resources\": [{\n",
    "    \"_id\": \"wrk_\" + uuid.uuid4().hex,\n",
    "    \"_type\": \"workspace\",\n",
    "    \"name\": \"Week09 Mini Chatbot API\",\n",
    "    \"scope\": \"collection\"\n",
    "  }]\n",
    "}\n",
    "INSOMNIA.write_text(json.dumps(insomnia, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Artifacts written:\")\n",
    "print(\"-\", ENV_SAMPLE)\n",
    "print(\"-\", REQS)\n",
    "print(\"-\", LOCUST)\n",
    "print(\"-\", K6)\n",
    "print(\"-\", POSTMAN)\n",
    "print(\"-\", INSOMNIA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90269373",
   "metadata": {},
   "source": [
    "## 1) 환경 변수 로딩 (.env)\n",
    "> 수업용으로 `.env`가 없으면 `.env.sample` 내용을 메모리에 로드합니다. 실제 프로젝트에서는 `python-dotenv`를 사용하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380bbe16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OPENAI_API_KEY': 'sk-~~~',\n",
       " 'LANGFUSE_PUBLIC_KEY': 'pk-lf-~~~',\n",
       " 'LANGFUSE_SECRET_KEY': 'sk-lf-~~~',\n",
       " 'LANGFUSE_HOST': 'https://cloud.langfuse.com',\n",
       " 'PINECONE_API_KEY': 'pcsk_~~~',\n",
       " 'REDIS_URL': 'redis://localhost:6379/0',\n",
       " 'MODEL_PROVIDER': 'mock\"  # \"mock\" | \"openai',\n",
       " 'MODEL_NAME': 'gpt-4o-mini\"  # openai 사용 시',\n",
       " 'RESPONSE_TTL_SECONDS': 300,\n",
       " 'RATE_LIMIT_RPS': 5,\n",
       " 'RATE_LIMIT_BURST': 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path('.env').exists() and Path('.env.sample').exists():\n",
    "    for line in Path('.env.sample').read_text(encoding='utf-8').splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "        if '=' in line:\n",
    "            k, v = line.split('=', 1)\n",
    "            os.environ[k.strip()] = v.strip().strip('\"').strip(\"'\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\", \"\"),\n",
    "    \"LANGFUSE_PUBLIC_KEY\": os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"\"),\n",
    "    \"LANGFUSE_SECRET_KEY\": os.getenv(\"LANGFUSE_SECRET_KEY\", \"\"),\n",
    "    \"LANGFUSE_HOST\": os.getenv(\"LANGFUSE_HOST\", \"\"),\n",
    "    \"PINECONE_API_KEY\": os.getenv(\"PINECONE_API_KEY\", \"\"),\n",
    "    \"REDIS_URL\": os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\"),\n",
    "    \"MODEL_PROVIDER\": os.getenv(\"MODEL_PROVIDER\", \"mock\"),\n",
    "    \"MODEL_NAME\": os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\"),\n",
    "    \"RESPONSE_TTL_SECONDS\": int(os.getenv(\"RESPONSE_TTL_SECONDS\", \"300\")),\n",
    "    \"RATE_LIMIT_RPS\": int(os.getenv(\"RATE_LIMIT_RPS\", \"5\")),\n",
    "    \"RATE_LIMIT_BURST\": int(os.getenv(\"RATE_LIMIT_BURST\", \"10\")),\n",
    "}\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb842c",
   "metadata": {},
   "source": [
    "## 2) 미니 챗봇 API 서버 코드 생성 (FastAPI)\n",
    "- 엔드포인트:\n",
    "  - `POST /chat` : 일반 응답(캐시/레이트리밋 적용)\n",
    "  - `POST /chat/stream` : SSE 스타일 스트리밍 모사\n",
    "- 구현 요소:\n",
    "  - **응답 캐시**: `sha256(prompt+model+params)` 키 + TTL\n",
    "  - **레이트 리밋**: 메모리 토큰버킷(실서비스는 Redis 권장)\n",
    "  - **모의 LLM(MockLLM)**: OpenAI 없이도 동작\n",
    "\n",
    "> 아래 셀은 `/mnt/data/week09_app` 폴더에 서버 소스파일을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eabdc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "App files written under: week09_app\n",
      "- utils_hash.py, rate_limit.py, cache_mem.py, mock_llm.py, main.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "\n",
    "APP_DIR = Path('week09_app')\n",
    "APP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# utils_hash.py\n",
    "(APP_DIR / 'utils_hash.py').write_text(dedent(r'''\n",
    "import hashlib, json\n",
    "\n",
    "def cache_key(payload: dict, model_name: str, schema_version: str=\"v1\") -> str:\n",
    "    blob = json.dumps({\"payload\": payload, \"model\": model_name, \"schema\": schema_version}, sort_keys=True)\n",
    "    return hashlib.sha256(blob.encode(\"utf-8\")).hexdigest()\n",
    "'''), encoding='utf-8')\n",
    "\n",
    "# rate_limit.py (토큰버킷)\n",
    "(APP_DIR / 'rate_limit.py').write_text(dedent(r'''\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "class TokenBucket:\n",
    "    def __init__(self, rate_per_sec: int, burst: int):\n",
    "        self.rate = rate_per_sec\n",
    "        self.burst = burst\n",
    "        self.tokens = defaultdict(lambda: burst)\n",
    "        self.timestamps = defaultdict(lambda: time.time())\n",
    "\n",
    "    def allow(self, key: str) -> bool:\n",
    "        now = time.time()\n",
    "        last = self.timestamps[key]\n",
    "        delta = now - last\n",
    "        refill = delta * self.rate\n",
    "        self.tokens[key] = min(self.burst, self.tokens[key] + refill)\n",
    "        self.timestamps[key] = now\n",
    "        if self.tokens[key] >= 1:\n",
    "            self.tokens[key] -= 1\n",
    "            return True\n",
    "        return False\n",
    "'''), encoding='utf-8')\n",
    "\n",
    "# cache_mem.py (TTL 캐시)\n",
    "(APP_DIR / 'cache_mem.py').write_text(dedent(r'''\n",
    "import time\n",
    "\n",
    "class TTLCache:\n",
    "    def __init__(self, ttl_seconds: int = 300):\n",
    "        self.ttl = ttl_seconds\n",
    "        self.store = {}\n",
    "\n",
    "    def get(self, key: str):\n",
    "        item = self.store.get(key)\n",
    "        if not item: return None\n",
    "        value, exp = item\n",
    "        if exp < time.time():\n",
    "            self.store.pop(key, None)\n",
    "            return None\n",
    "        return value\n",
    "\n",
    "    def set(self, key: str, value):\n",
    "        self.store[key] = (value, time.time() + self.ttl)\n",
    "'''), encoding='utf-8')\n",
    "\n",
    "# mock_llm.py (지연/스트리밍 모사)\n",
    "(APP_DIR / 'mock_llm.py').write_text(dedent(r'''\n",
    "import asyncio\n",
    "\n",
    "class MockLLM:\n",
    "    def __init__(self, delay_ms_per_token: int = 40):\n",
    "        self.delay = delay_ms_per_token / 1000.0\n",
    "\n",
    "    async def generate(self, prompt: str, max_tokens: int = 64):\n",
    "        text = \"[MOCK] \" + (prompt[::-1])[:max_tokens]\n",
    "        await asyncio.sleep(self.delay * max(1, min(len(text)//6, 10)))\n",
    "        return text\n",
    "\n",
    "    async def stream(self, prompt: str, max_tokens: int = 64):\n",
    "        text = await self.generate(prompt, max_tokens)\n",
    "        chunks = max(1, min(8, len(text)//max(1, len(text)//8)))\n",
    "        step = max(1, len(text)//chunks)\n",
    "        for i in range(0, len(text), step):\n",
    "            await asyncio.sleep(self.delay)\n",
    "            yield text[i:i+step]\n",
    "'''), encoding='utf-8')\n",
    "\n",
    "# main.py (FastAPI 앱)\n",
    "(APP_DIR / 'main.py').write_text(dedent(r'''\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.responses import StreamingResponse, JSONResponse\n",
    "from pydantic import BaseModel, Field\n",
    "import os, json\n",
    "\n",
    "from utils_hash import cache_key\n",
    "from rate_limit import TokenBucket\n",
    "from cache_mem import TTLCache\n",
    "from mock_llm import MockLLM\n",
    "\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")\n",
    "RESPONSE_TTL_SECONDS = int(os.getenv(\"RESPONSE_TTL_SECONDS\", \"300\"))\n",
    "RATE_LIMIT_RPS = int(os.getenv(\"RATE_LIMIT_RPS\", \"5\"))\n",
    "RATE_LIMIT_BURST = int(os.getenv(\"RATE_LIMIT_BURST\", \"10\"))\n",
    "\n",
    "app = FastAPI(title=\"Week09 Mini Chatbot API\")\n",
    "\n",
    "bucket = TokenBucket(rate_per_sec=RATE_LIMIT_RPS, burst=RATE_LIMIT_BURST)\n",
    "cache = TTLCache(ttl_seconds=RESPONSE_TTL_SECONDS)\n",
    "llm = MockLLM()\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    userId: str = Field(..., description=\"사용자 식별자\")\n",
    "    message: str = Field(..., description=\"프롬프트/메시지\")\n",
    "    params: dict = Field(default_factory=dict, description=\"샘플링 파라미터 등 선택값\")\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat(req: ChatRequest, request: Request):\n",
    "    client_ip = request.client.host if request.client else \"unknown\"\n",
    "    rl_key = f\"{client_ip}:{req.userId}\"\n",
    "    if not bucket.allow(rl_key):\n",
    "        raise HTTPException(status_code=429, detail=\"Rate limit exceeded\")\n",
    "\n",
    "    key = cache_key({\"message\": req.message, \"params\": req.params}, MODEL_NAME)\n",
    "    cached = cache.get(key)\n",
    "    if cached:\n",
    "        return JSONResponse({\"cached\": True, \"model\": MODEL_NAME, \"output\": cached})\n",
    "\n",
    "    # (옵션) OpenAI 호출 자리 — 현재는 MockLLM 사용\n",
    "    text = await llm.generate(req.message, max_tokens=128)\n",
    "    cache.set(key, text)\n",
    "    return {\"cached\": False, \"model\": MODEL_NAME, \"output\": text}\n",
    "\n",
    "@app.post(\"/chat/stream\")\n",
    "async def chat_stream(req: ChatRequest, request: Request):\n",
    "    client_ip = request.client.host if request.client else \"unknown\"\n",
    "    rl_key = f\"{client_ip}:{req.userId}\"\n",
    "    if not bucket.allow(rl_key):\n",
    "        raise HTTPException(status_code=429, detail=\"Rate limit exceeded\")\n",
    "\n",
    "    async def streamer():\n",
    "        first = True\n",
    "        async for chunk in llm.stream(req.message, max_tokens=128):\n",
    "            data = json.dumps({\"delta\": chunk, \"done\": False if first else None})\n",
    "            first = False\n",
    "            yield f\"data: {data}\\n\\n\"\n",
    "        yield 'data: {\"done\": true}\\n\\n'\n",
    "\n",
    "    headers = {\"Content-Type\": \"text/event-stream\"}\n",
    "    return StreamingResponse(streamer(), headers=headers)\n",
    "'''), encoding='utf-8')\n",
    "\n",
    "print(f\"App files written under: {APP_DIR}\")\n",
    "print(\"- utils_hash.py, rate_limit.py, cache_mem.py, mock_llm.py, main.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c148b",
   "metadata": {},
   "source": [
    "## 3) 로컬 실행 방법\n",
    "```bash\n",
    "# 가상환경 & 설치\n",
    "python -m venv .venv && source .venv/bin/activate   # Windows: .venv\\Scripts\\activate\n",
    "pip install -r requirements_week09.txt\n",
    "\n",
    "# (옵션) Redis 실행\n",
    "docker run -d --name redis -p 6379:6379 redis:7-alpine\n",
    "\n",
    "# .env 준비\n",
    "cp .env.sample .env\n",
    "\n",
    "# 서버 실행 (노트북에서 생성된 경로)\n",
    "cd week09/week09_app\n",
    "uvicorn main:app --host 0.0.0.0 --port 8000 --workers 1 --reload\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a1452",
   "metadata": {},
   "source": [
    "## 4) 노트북에서 간단 요청 테스트 (옵션)\n",
    "> 아래 코드는 로컬에서 8000 포트로 서버가 이미 떠 있다고 가정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f095bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server /docs: 200\n",
      "POST /chat: 200\n",
      "{\n",
      "  \"cached\": true,\n",
      "  \"model\": \"gpt-4o-mini\",\n",
      "  \"output\": \"[MOCK] IPAtsaF 요세하녕안\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import httpx, json, os\n",
    "\n",
    "BASE = os.getenv(\"WEEK09_BASE_URL\", \"http://localhost:8000\")\n",
    "try:\n",
    "    r = httpx.get(f\"{BASE}/docs\", timeout=3.0)\n",
    "    print(\"Server /docs:\", r.status_code)\n",
    "except Exception as e:\n",
    "    print(\"서버가 떠있지 않은 것 같습니다. 로컬에서 uvicorn을 먼저 실행하세요.\\n\", e)\n",
    "\n",
    "payload = {\"userId\":\"notebook\",\"message\":\"안녕하세요 FastAPI\",\"params\":{}}\n",
    "try:\n",
    "    r = httpx.post(f\"{BASE}/chat\", json=payload, timeout=5.0)\n",
    "    print(\"POST /chat:\", r.status_code)\n",
    "    if r.status_code == 200:\n",
    "        print(json.dumps(r.json(), ensure_ascii=False, indent=2))\n",
    "except Exception as e:\n",
    "    print(\"요청 실패:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3cfe2e",
   "metadata": {},
   "source": [
    "## 5) 부하 테스트 (Locust / k6)\n",
    "**Locust**\n",
    "```bash\n",
    "locust -f locustfile.py --host http://localhost:8000\n",
    "# 브라우저에서 http://localhost:8089 접속 → VU/Spawn rate 설정 → Start\n",
    "```\n",
    "**k6**\n",
    "```bash\n",
    "k6 run k6-loadtest.js --vus 20 --duration 60s\n",
    "```\n",
    "> 성능 지표는 p50/p95/p99, 에러율, 처리량(req/s) 중심으로 기록하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b70911",
   "metadata": {},
   "source": [
    "## 6) 캐시 키 실습\n",
    "동일 페이로드(+모델)이면 캐시가 히트되어 두 번째 요청은 즉시 응답됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77a71174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key1: 2264763028d66f2f90cd6e24ee10ed2cbb870829869fc95749837ce3d5ee8042\n",
      "Key2: 93eb519ddb7bb6369328a79451e1bb44929369f93c96c27e9cb5464cf01d0ddb\n",
      "Same? -> False\n"
     ]
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "cache_mod = import_module('week09_app.utils_hash')\n",
    "\n",
    "p1 = {\"message\":\"Hello\",\"params\":{\"temp\":0.7}}\n",
    "p2 = {\"message\":\"Hello!\",\"params\":{\"temp\":0.7}}\n",
    "print(\"Key1:\", cache_mod.cache_key(p1, \"gpt-4o-mini\"))\n",
    "print(\"Key2:\", cache_mod.cache_key(p2, \"gpt-4o-mini\"))\n",
    "print(\"Same? ->\", cache_mod.cache_key(p1, \"gpt-4o-mini\") == cache_mod.cache_key(p2, \"gpt-4o-mini\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ec8b9",
   "metadata": {},
   "source": [
    "## 7) 레이트 리밋 토큰버킷 개념 실습\n",
    "동일 키(예: `client_ip:userId`)로 짧은 시간 안에 여러 번 호출하면 429가 발생합니다. 실서비스는 Redis/슬라이딩윈도우/분산 토큰버킷 권장.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435799e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "허용 여부: [True, True, True, False, True, False, False, True, False, False]\n",
      "설명: 처음 3개는 버스트로 허용, 이후에는 초당 3개 속도로 리필됩니다.\n"
     ]
    }
   ],
   "source": [
    "from importlib import import_module\n",
    "import time\n",
    "rl_mod = import_module('week09_app.rate_limit')\n",
    "\n",
    "bucket = rl_mod.TokenBucket(rate_per_sec=3, burst=3)\n",
    "key = \"demo\"\n",
    "allowed = []\n",
    "for i in range(10):\n",
    "    allowed.append(bucket.allow(key))\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"허용 여부:\", allowed)\n",
    "print(\"설명: 처음 3개는 버스트로 허용, 이후에는 초당 3개 속도로 리필됩니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7024a",
   "metadata": {},
   "source": [
    "## 8) 과제 가이드 (필수/가점)\n",
    "### 필수 제출물\n",
    "- **Mini Chatbot API(FastAPI)** — `/chat`, `/chat/stream`\n",
    "- **부하 테스트 결과** — p50/p95/p99, 에러율, 처리량(req/s) 표/그래프 정리\n",
    "\n",
    "### 가점 항목\n",
    "- **모델서버 분리** (vLLM/TGI/Ollama) — FastAPI는 게이트웨이\n",
    "- **캐시** — Redis 기반 응답 캐시 + 히트율 측정\n",
    "- **레이트 리밋** — 사용자/토큰 단위 분산 레이트 리밋\n",
    "- **스트리밍** — SSE vs WebSocket 비교 및 체감 지연 분석\n",
    "\n",
    "### 제출 형태\n",
    "- GitHub 저장소 링크 + README(구현/측정/분석) + 부하테스트 스크린샷\n",
    "- Postman/Insomnia 컬렉션 파일 포함\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajou-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
