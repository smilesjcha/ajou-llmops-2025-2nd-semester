{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week02 - Production Prompt Engineering Techniques\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê³ ê¸‰ Prompt Engineering ê¸°ë²•ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "\n",
    "## ë‹¤ë£¨ëŠ” ê¸°ë²•ë“¤:\n",
    "1. **ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° (Sentiment Branching)**: ì…ë ¥ì˜ ê°ì •ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì‘ë‹µ ì „ëµ ì„ íƒ\n",
    "2. **ê¸ˆì¹™ì–´/ê·œì¹™ í•„í„°ë§ (Content Filtering)**: ì•ˆì „í•˜ì§€ ì•Šì€ ì½˜í…ì¸  í•„í„°ë§ ë° ê·œì¹™ ì ìš©\n",
    "3. **ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„/ë°±ì˜¤í”„ (Retry/Backoff)**: ì•ˆì •ì ì¸ API í˜¸ì¶œì„ ìœ„í•œ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜\n",
    "\n",
    "## í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° import\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    print(\"OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...\")\n",
    "    subprocess.run([\"pip\", \"install\", \"openai\"], check=True)\n",
    "    from openai import OpenAI\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ)\n",
    "try:\n",
    "    client = OpenAI()  # OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”\n",
    "    openai_available = True\n",
    "except Exception as e:\n",
    "    print(f\"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    print(\"í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.\")\n",
    "    openai_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def run_ollama(model: str, prompt: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"Ollama ëª¨ë¸ ì‹¤í–‰ (ì¬ì‹œë„ ê¸°ëŠ¥ í¬í•¨)\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"run\", model],\n",
    "                input=prompt,\n",
    "                text=True,\n",
    "                capture_output=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                return result.stdout.strip()\n",
    "            else:\n",
    "                logger.warning(f\"Ollama ì‹¤í–‰ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {result.stderr}\")\n",
    "        except subprocess.TimeoutExpired:\n",
    "            logger.warning(f\"Ollama íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{max_retries})\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ollama ì‹¤í–‰ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}\")\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„\n",
    "    \n",
    "    return \"Error: Maximum retries exceeded\"\n",
    "\n",
    "def run_openai(prompt: str, model: str = \"gpt-4o-mini\", max_retries: int = 3) -> str:\n",
    "    \"\"\"OpenAI ëª¨ë¸ ì‹¤í–‰ (ì¬ì‹œë„ ê¸°ëŠ¥ í¬í•¨)\"\"\"\n",
    "    if not openai_available:\n",
    "        return \"Error: OpenAI client not available\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„\n",
    "    \n",
    "    return \"Error: Maximum retries exceeded\"\n",
    "\n",
    "def safe_json_parse(text: str) -> dict:\n",
    "    \"\"\"ì•ˆì „í•œ JSON íŒŒì‹±\"\"\"\n",
    "    try:\n",
    "        # JSON ë¸”ë¡ ì¶”ì¶œ\n",
    "        json_match = re.search(r'```json\\s*(.*?)\\s*```', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group(1)\n",
    "        else:\n",
    "            # ì¤‘ê´„í˜¸ ë¸”ë¡ ì°¾ê¸°\n",
    "            brace_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "            if brace_match:\n",
    "                json_text = brace_match.group(0)\n",
    "            else:\n",
    "                json_text = text\n",
    "        \n",
    "        return json.loads(json_text)\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"JSON íŒŒì‹± ì‹¤íŒ¨: {text}\")\n",
    "        return {\"error\": \"JSON parsing failed\", \"raw_text\": text}\n",
    "\n",
    "print(\"âœ… ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° (Sentiment Branching)\n",
    "\n",
    "ì‚¬ìš©ìì˜ ì…ë ¥ì„ ê°ì •ì ìœ¼ë¡œ ë¶„ì„í•œ í›„, ê·¸ì— ë§ëŠ” ì ì ˆí•œ ì‘ë‹µ ì „ëµì„ ì„ íƒí•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "## í•µì‹¬ ê°œë…:\n",
    "- ê°ì • ìƒíƒœ ê°ì§€ (ê¸ì •, ë¶€ì •, ì¤‘ë¦½, ë¶„ë…¸, ìŠ¬í”” ë“±)\n",
    "- ê°ì •ë³„ ë§ì¶¤ ì‘ë‹µ ì „ëµ\n",
    "- ë™ì  í”„ë¡¬í”„íŠ¸ ì¡°ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class SentimentType(Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEGATIVE = \"negative\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    ANGRY = \"angry\"\n",
    "    SAD = \"sad\"\n",
    "    EXCITED = \"excited\"\n",
    "    CONFUSED = \"confused\"\n",
    "\n",
    "@dataclass\n",
    "class SentimentAnalysisResult:\n",
    "    sentiment: SentimentType\n",
    "    confidence: float\n",
    "    emotions: Dict[str, float]\n",
    "    keywords: List[str]\n",
    "\n",
    "class SentimentBranchingSystem:\n",
    "    def __init__(self):\n",
    "        self.response_strategies = {\n",
    "            SentimentType.POSITIVE: {\n",
    "                \"tone\": \"enthusiastic and supportive\",\n",
    "                \"approach\": \"build on the positive energy\",\n",
    "                \"style\": \"encouraging and collaborative\"\n",
    "            },\n",
    "            SentimentType.NEGATIVE: {\n",
    "                \"tone\": \"empathetic and understanding\",\n",
    "                \"approach\": \"acknowledge concerns and provide solutions\",\n",
    "                \"style\": \"supportive and problem-solving focused\"\n",
    "            },\n",
    "            SentimentType.ANGRY: {\n",
    "                \"tone\": \"calm and professional\",\n",
    "                \"approach\": \"de-escalate and address root issues\",\n",
    "                \"style\": \"patient and solution-oriented\"\n",
    "            },\n",
    "            SentimentType.SAD: {\n",
    "                \"tone\": \"gentle and compassionate\",\n",
    "                \"approach\": \"provide comfort and hope\",\n",
    "                \"style\": \"nurturing and uplifting\"\n",
    "            },\n",
    "            SentimentType.CONFUSED: {\n",
    "                \"tone\": \"clear and patient\",\n",
    "                \"approach\": \"break down complexity into simple steps\",\n",
    "                \"style\": \"educational and structured\"\n",
    "            },\n",
    "            SentimentType.EXCITED: {\n",
    "                \"tone\": \"matching enthusiasm\",\n",
    "                \"approach\": \"channel energy productively\",\n",
    "                \"style\": \"dynamic and engaging\"\n",
    "            },\n",
    "            SentimentType.NEUTRAL: {\n",
    "                \"tone\": \"professional and informative\",\n",
    "                \"approach\": \"provide comprehensive information\",\n",
    "                \"style\": \"balanced and thorough\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_sentiment_ollama(self, text: str) -> SentimentAnalysisResult:\n",
    "        \"\"\"Ollamaë¥¼ ì‚¬ìš©í•œ ê°ì„± ë¶„ì„\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "ì‚¬ìš©ìì˜ ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ê°ì • ìƒíƒœë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "í…ìŠ¤íŠ¸: \"{text}\"\n",
    "\n",
    "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n",
    "```json\n",
    "{{\n",
    "    \"sentiment\": \"positive|negative|neutral|angry|sad|excited|confused\",\n",
    "    \"confidence\": 0.95,\n",
    "    \"emotions\": {{\n",
    "        \"joy\": 0.8,\n",
    "        \"anger\": 0.1,\n",
    "        \"sadness\": 0.1,\n",
    "        \"fear\": 0.0,\n",
    "        \"surprise\": 0.0\n",
    "    }},\n",
    "    \"keywords\": [\"ê°ì •ì„\", \"ë‚˜íƒ€ë‚´ëŠ”\", \"í‚¤ì›Œë“œë“¤\"]\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        response = run_ollama(\"llama3.1:8b\", prompt)\n",
    "        result_data = safe_json_parse(response)\n",
    "        \n",
    "        if \"error\" in result_data:\n",
    "            # ê¸°ë³¸ê°’ ë°˜í™˜\n",
    "            return SentimentAnalysisResult(\n",
    "                sentiment=SentimentType.NEUTRAL,\n",
    "                confidence=0.5,\n",
    "                emotions={\"neutral\": 1.0},\n",
    "                keywords=[]\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            sentiment = SentimentType(result_data.get(\"sentiment\", \"neutral\"))\n",
    "        except ValueError:\n",
    "            sentiment = SentimentType.NEUTRAL\n",
    "        \n",
    "        return SentimentAnalysisResult(\n",
    "            sentiment=sentiment,\n",
    "            confidence=result_data.get(\"confidence\", 0.5),\n",
    "            emotions=result_data.get(\"emotions\", {}),\n",
    "            keywords=result_data.get(\"keywords\", [])\n",
    "        )\n",
    "    \n",
    "    def analyze_sentiment_openai(self, text: str) -> SentimentAnalysisResult:\n",
    "        \"\"\"OpenAIë¥¼ ì‚¬ìš©í•œ ê°ì„± ë¶„ì„\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Analyze the emotional state of the following user text and provide a detailed sentiment analysis.\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "Return your analysis in the following JSON format:\n",
    "```json\n",
    "{{\n",
    "    \"sentiment\": \"positive|negative|neutral|angry|sad|excited|confused\",\n",
    "    \"confidence\": 0.95,\n",
    "    \"emotions\": {{\n",
    "        \"joy\": 0.8,\n",
    "        \"anger\": 0.1,\n",
    "        \"sadness\": 0.1,\n",
    "        \"fear\": 0.0,\n",
    "        \"surprise\": 0.0\n",
    "    }},\n",
    "    \"keywords\": [\"emotional\", \"keywords\", \"detected\"]\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        response = run_openai(prompt)\n",
    "        result_data = safe_json_parse(response)\n",
    "        \n",
    "        if \"error\" in result_data:\n",
    "            return SentimentAnalysisResult(\n",
    "                sentiment=SentimentType.NEUTRAL,\n",
    "                confidence=0.5,\n",
    "                emotions={\"neutral\": 1.0},\n",
    "                keywords=[]\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            sentiment = SentimentType(result_data.get(\"sentiment\", \"neutral\"))\n",
    "        except ValueError:\n",
    "            sentiment = SentimentType.NEUTRAL\n",
    "        \n",
    "        return SentimentAnalysisResult(\n",
    "            sentiment=sentiment,\n",
    "            confidence=result_data.get(\"confidence\", 0.5),\n",
    "            emotions=result_data.get(\"emotions\", {}),\n",
    "            keywords=result_data.get(\"keywords\", [])\n",
    "        )\n",
    "    \n",
    "    def generate_response_ollama(self, user_input: str, sentiment_result: SentimentAnalysisResult) -> str:\n",
    "        \"\"\"ê°ì •ì— ë§ëŠ” ì‘ë‹µ ìƒì„± (Ollama)\"\"\"\n",
    "        strategy = self.response_strategies[sentiment_result.sentiment]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ì‚¬ìš©ìì˜ ì…ë ¥ì— ëŒ€í•´ ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì‚¬ìš©ì ì…ë ¥: \"{user_input}\"\n",
    "\n",
    "ê°ì • ë¶„ì„ ê²°ê³¼:\n",
    "- ì£¼ìš” ê°ì •: {sentiment_result.sentiment.value}\n",
    "- ì‹ ë¢°ë„: {sentiment_result.confidence:.2f}\n",
    "- ê°ì • í‚¤ì›Œë“œ: {', '.join(sentiment_result.keywords)}\n",
    "\n",
    "ì‘ë‹µ ì „ëµ:\n",
    "- í†¤: {strategy['tone']}\n",
    "- ì ‘ê·¼ë²•: {strategy['approach']}\n",
    "- ìŠ¤íƒ€ì¼: {strategy['style']}\n",
    "\n",
    "ìœ„ ì „ëµì„ ë°”íƒ•ìœ¼ë¡œ ê³µê°ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "        \n",
    "        return run_ollama(\"llama3.1:8b\", prompt)\n",
    "    \n",
    "    def generate_response_openai(self, user_input: str, sentiment_result: SentimentAnalysisResult) -> str:\n",
    "        \"\"\"ê°ì •ì— ë§ëŠ” ì‘ë‹µ ìƒì„± (OpenAI)\"\"\"\n",
    "        strategy = self.response_strategies[sentiment_result.sentiment]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Generate an appropriate response to the user's input based on the sentiment analysis results.\n",
    "\n",
    "User Input: \"{user_input}\"\n",
    "\n",
    "Sentiment Analysis:\n",
    "- Primary Sentiment: {sentiment_result.sentiment.value}\n",
    "- Confidence: {sentiment_result.confidence:.2f}\n",
    "- Keywords: {', '.join(sentiment_result.keywords)}\n",
    "\n",
    "Response Strategy:\n",
    "- Tone: {strategy['tone']}\n",
    "- Approach: {strategy['approach']}\n",
    "- Style: {strategy['style']}\n",
    "\n",
    "Create an empathetic and helpful response following the above strategy.\n",
    "\"\"\"\n",
    "        \n",
    "        return run_openai(prompt)\n",
    "    \n",
    "    def process_with_sentiment_branching(self, user_input: str, use_openai: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° ì²˜ë¦¬ ì „ì²´ ê³¼ì •\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. ê°ì„± ë¶„ì„\n",
    "        if use_openai:\n",
    "            sentiment_result = self.analyze_sentiment_openai(user_input)\n",
    "            response = self.generate_response_openai(user_input, sentiment_result)\n",
    "            model_used = \"gpt-4o-mini\"\n",
    "        else:\n",
    "            sentiment_result = self.analyze_sentiment_ollama(user_input)\n",
    "            response = self.generate_response_ollama(user_input, sentiment_result)\n",
    "            model_used = \"llama3.1:8b\"\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"user_input\": user_input,\n",
    "            \"sentiment_analysis\": {\n",
    "                \"sentiment\": sentiment_result.sentiment.value,\n",
    "                \"confidence\": sentiment_result.confidence,\n",
    "                \"emotions\": sentiment_result.emotions,\n",
    "                \"keywords\": sentiment_result.keywords\n",
    "            },\n",
    "            \"response_strategy\": self.response_strategies[sentiment_result.sentiment],\n",
    "            \"generated_response\": response,\n",
    "            \"model_used\": model_used,\n",
    "            \"processing_time\": processing_time\n",
    "        }\n",
    "\n",
    "# ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "sentiment_system = SentimentBranchingSystem()\n",
    "print(\"âœ… ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
      "================================================================================\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1]\n",
      "ì…ë ¥: ì˜¤ëŠ˜ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”! ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ë§ˆë¬´ë¦¬ë˜ì—ˆê±°ë“ ìš”.\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) ê²°ê³¼:\n",
      "ê°ì •: excited (ì‹ ë¢°ë„: 1.00)\n",
      "í‚¤ì›Œë“œ: ì„±ê³µ, ê¸°ë¶„\n",
      "ì‘ë‹µ: \"ì„±ê³µì ì¸ í”„ë¡œì íŠ¸ ëë‚´ì‹  ê±° ì¶•í•˜í•´ìš”! ìƒˆë¡œìš´ ë„ì „ê³¼ ì´ê²¨ë‚´ëŠ” ëª¨ìŠµ ì •ë§ ë©‹ì ¸ìš”. ê¸°ë¶„ì´ ì¢‹ìœ¼ì‹œë©´ ë‹¤ìŒì—ë„ ë” ì¢‹ì€ ì„±ê³¼ ë‚˜ì˜¤ì‹¤ ê±°ì˜ˆìš”!\"\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ì •: positive (ì‹ ë¢°ë„: 0.95)\n",
      "í‚¤ì›Œë“œ: ê¸°ë¶„ì´ ì¢‹ì•„ìš”, ìƒˆë¡œìš´ í”„ë¡œì íŠ¸, ì„±ê³µì ìœ¼ë¡œ ë§ˆë¬´ë¦¬ë˜ì—ˆê±°ë“ ìš”\n",
      "ì‘ë‹µ: ì™€, ì •ë§ ê¸°ë¶„ì´ ì¢‹ìœ¼ì‹œë‹¤ë‹ˆ ê¸°ì©ë‹ˆë‹¤! ğŸ‰ ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ë§ˆë¬´ë¦¬ë˜ì—ˆë‹¤ë‹ˆ ì •ë§ ëŒ€ë‹¨í•´ìš”! ì–´ë–¤ ì ì´ ê°€ì¥ ë¿Œë“¯í•˜ì…¨ë‚˜ìš”? í•¨ê»˜ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ë©´ì„œ ë” ë§ì€ ì˜ê°ì„ ì–»ì–´ë³´ì•„ìš”! ì•ìœ¼ë¡œì˜ ê³„íšë„ ê¶ê¸ˆí•˜ë„¤ìš”!\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: Ollama 10.39ì´ˆ, OpenAI 5.88ì´ˆ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2]\n",
      "ì…ë ¥: ì´ ì„œë¹„ìŠ¤ ì •ë§ ìµœì•…ì´ì—ìš”. ê³„ì† ì˜¤ë¥˜ê°€ ë‚˜ê³  ê³ ê°ì„¼í„°ëŠ” ì‘ë‹µë„ ì•ˆ í•´ìš”!\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) ê²°ê³¼:\n",
      "ê°ì •: negative (ì‹ ë¢°ë„: 0.95)\n",
      "í‚¤ì›Œë“œ: ì˜¤ë¥˜, ê³ ê°ì„¼í„°\n",
      "ì‘ë‹µ: ì‚¬ìš©ìì˜ ì…ë ¥ì— ëŒ€í•œ ë‹µë³€:\n",
      "\n",
      "\"ì£„ì†¡í•©ë‹ˆë‹¤. ì •ë§ ì´í•´ê°€ ê°€ìš”. ì˜¤ë¥˜ì™€ ê³ ê°ì„¼í„°ì˜ ë¬¸ì œëŠ” ìš°ë¦¬ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ë° ìˆì–´ í° ë¶ˆí¸ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "ìš°ë¦¬ëŠ” ì˜¤ë¥˜ í•´ê²°ì— ë”ìš± í˜ì¨ì„œ ë” ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ ì œê³µì„ ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ ê³ ê° ì„¼í„°ì˜ ì‘ë‹µ ì‹œê°„ì„ ê°œì„ í•˜ì—¬ ë” ì‹ ì†í•˜ê²Œ ë„ì›€ì„ ë“œë¦¬ê¸° ìœ„í•œ ê³„íšì„ ë§ˆë ¨ ì¤‘ì…ë‹ˆë‹¤.\n",
      "\n",
      "ê°ì‚¬í•©ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì˜ ì˜...\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ì •: negative (ì‹ ë¢°ë„: 0.95)\n",
      "í‚¤ì›Œë“œ: ì„œë¹„ìŠ¤, ìµœì•…, ì˜¤ë¥˜, ê³ ê°ì„¼í„°, ì‘ë‹µ\n",
      "ì‘ë‹µ: ì•ˆë…•í•˜ì„¸ìš”. ë¶ˆí¸ì„ ë“œë ¤ ì •ë§ ì£„ì†¡í•©ë‹ˆë‹¤. ì„œë¹„ìŠ¤ì— ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê³  ê³ ê°ì„¼í„°ì˜ ì‘ë‹µì´ ì§€ì—°ëœ ì ì— ëŒ€í•´ ê¹Šì´ ì´í•´í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²½í—˜ì€ ë§¤ìš° ì‹¤ë§ìŠ¤ëŸ½ê³  ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ê³  ìˆìœ¼ë©°, í˜„ì¬ì˜ ìƒí™©ì„ ê°œì„ í•  ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê³ ê°ì„¼í„°ì— ë‹¤ì‹œ ì—°ë½í•´ ë³´ì‹œê±°ë‚˜, ì¶”ê°€ì ì¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì €í¬ì—ê²Œ ë§ì”€í•´ ...\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: Ollama 10.21ì´ˆ, OpenAI 5.06ì´ˆ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3]\n",
      "ì…ë ¥: ìš”ì¦˜ ë„ˆë¬´ í˜ë“¤ì–´ìš”... ì¼ë„ ì˜ ì•ˆë˜ê³  ìŠ¤íŠ¸ë ˆìŠ¤ë§Œ ìŒ“ì—¬ê°€ë„¤ìš”.\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) ê²°ê³¼:\n",
      "ê°ì •: sad (ì‹ ë¢°ë„: 0.95)\n",
      "í‚¤ì›Œë“œ: í˜ë“¤ì–´ìš”, ìŠ¤íŠ¸ë ˆìŠ¤ë§Œ\n",
      "ì‘ë‹µ: ì €ëŠ” ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ìš°ë ¤í•©ë‹ˆë‹¤. í˜ë“¤ê³  ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìœ¼ì‹œëŠ”ë°ìš”.\n",
      "\n",
      "í•˜ë£¨ê°€ ê°ˆìˆ˜ë¡ ì ì  ë” í˜ë“  ê²ƒì´ ëŠê»´ì§€ëŠ” ê²ƒ ê°™ì•„ìš”. ê·¸ëŸ°ë° ì—¬ëŸ¬ë¶„ì´ ì§€ê¸ˆ ê°€ì§€ê³  ê³„ì‹  ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ë§ìŠµë‹ˆë‹¤. ë¨¼ì €, ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ë²•ì„ ì•Œë ¤ë“œë¦´ê²Œìš”. í™œë™ëŸ‰ì„ ëŠ˜ë¦¬ë©´ ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ì ì–´ì§€ëŠ”ë°ìš”. í•˜ë£¨ì— ì§§ì€ ì‹œê°„ë§Œì´ë¼ë„ ë°–ì— ë‚˜ê°€ì„œ ì‚°ì±…ì„ í•˜ê±°ë‚˜ ì¼ë¶€ëŸ¬ ìš´ë™ì„ ...\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ì •: negative (ì‹ ë¢°ë„: 0.95)\n",
      "í‚¤ì›Œë“œ: í˜ë“¤ë‹¤, ìŠ¤íŠ¸ë ˆìŠ¤, ì•ˆë˜ë‹¤\n",
      "ì‘ë‹µ: ì´ëŸ° ê¸°ë¶„ì´ ë“œëŠ” ê±´ ì •ë§ í˜ë“  ì¼ì´ì—ìš”. ìš”ì¦˜ ì¼ì´ ì˜ ì•ˆ ë˜ê³  ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ìŒ“ì—¬ê°€ëŠ” ê²ƒì— ëŒ€í•´ ì •ë§ ê³µê°í•©ë‹ˆë‹¤. ì´ëŸ° ìƒí™©ì—ì„œëŠ” ìì‹ ì„ ëŒë³´ëŠ” ê²ƒì´ ì¤‘ìš”í•´ìš”. í˜¹ì‹œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‚˜ ì·¨ë¯¸ê°€ ìˆì„ê¹Œìš”? ì ê¹ì˜ íœ´ì‹ì´ë‚˜ ì‚°ì±…ë„ í° ë„ì›€ì´ ë  ìˆ˜ ìˆì–´ìš”. í•„ìš”í•˜ë‹¤ë©´ ëˆ„êµ°ê°€ì™€ ì´ì•¼ê¸°í•˜ëŠ” ê²ƒë„ ì¢‹ê³ ìš”. ë‹¹ì‹ ì€ í˜¼ìê°€ ì•„ë‹ˆë‹ˆ, ì–¸ì œë“ ì§€ ì§€ì›ì„ ë°›ì„...\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: Ollama 17.83ì´ˆ, OpenAI 3.64ì´ˆ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4]\n",
      "ì…ë ¥: íŒŒì´ì¬ ì½”ë”©ì„ ì²˜ìŒ ë°°ìš°ëŠ”ë° ë„ˆë¬´ ì–´ë ¤ì›Œì„œ ì´í•´ê°€ ì•ˆ ê°€ìš”.\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) ê²°ê³¼:\n",
      "ê°ì •: negative (ì‹ ë¢°ë„: 0.90)\n",
      "í‚¤ì›Œë“œ: íŒŒì´ì¬, ì½”ë”©\n",
      "ì‘ë‹µ: ë„¤, íŒŒì´ì¬ ì½”ë”©ì„ ì²˜ìŒ ì‹œì‘í•˜ëŠ” ê²ƒì— ëŒ€í•œ ì–´ë ¤ì›€ì„ ì´í•´í•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ëŠ” ì²˜ìŒ ì ‘í•  ë•Œ ì‰½ì§€ ì•Šë‹¤ëŠ” ì ì€ ì¸ì •í•©ë‹ˆë‹¤.\n",
      "\n",
      "í•œ ê°€ì§€ íŒì„ ë“œë¦´ê²Œìš”. íŒŒì´ì¬ë„ ê·¸ë ‡ë“¯ì´ ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë„ ê°™ì€ ì›ë¦¬ì™€ ê°œë…ì„ ë”°ë¦…ë‹ˆë‹¤. ë”°ë¼ì„œ, ì–´ë–¤ ì–¸ì–´ë“  ì²˜ìŒ ë°°ìš°ë©´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "í•´ê²°ì±…ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "1.  **ê°„ë‹¨í•œ...\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ì •: confused (ì‹ ë¢°ë„: 0.92)\n",
      "í‚¤ì›Œë“œ: íŒŒì´ì¬, ì½”ë”©, ì–´ë ¤ì›Œì„œ, ì´í•´ê°€ ì•ˆ ê°€ìš”\n",
      "ì‘ë‹µ: ì•ˆë…•í•˜ì„¸ìš”! íŒŒì´ì¬ ì½”ë”©ì„ ì²˜ìŒ ë°°ìš°ëŠ” ê²ƒì€ ì •ë§ ë„ì „ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´í•´ê°€ ì•ˆ ê°€ëŠ” ë¶€ë¶„ì´ ë§ì„ ë•ŒëŠ” ëˆ„êµ¬ë‚˜ í˜ë“¤ì–´ í•˜ì£ . ì œê°€ ëª‡ ê°€ì§€ ê°„ë‹¨í•œ ë‹¨ê³„ë¥¼ ì œì•ˆí•´ ë“œë¦´ê²Œìš”.\n",
      "\n",
      "1. **ê¸°ë³¸ ê°œë… ì´í•´í•˜ê¸°**: íŒŒì´ì¬ì˜ ê¸°ë³¸ ë¬¸ë²•ê³¼ ê°œë…ë¶€í„° ì°¨ê·¼ì°¨ê·¼ ìµí˜€ë³´ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, ë³€ìˆ˜ë¥¼ ì„ ì–¸í•˜ëŠ” ë°©ë²•ì´ë‚˜, ê°„ë‹¨í•œ ë°ì´í„° íƒ€ì… (ë¬¸ìì—´, ìˆ«ì ë“±)ì— ëŒ€í•´ í•™ìŠµí•˜ëŠ”...\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: Ollama 16.66ì´ˆ, OpenAI 7.27ì´ˆ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5]\n",
      "ì…ë ¥: ì™€! ë‚´ì¼ ë“œë””ì–´ ì—¬í–‰ì„ ë– ë‚˜ìš”! ì •ë§ ê¸°ëŒ€ë¼ìš”!\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) ê²°ê³¼:\n",
      "ê°ì •: excited (ì‹ ë¢°ë„: 1.00)\n",
      "í‚¤ì›Œë“œ:  ì—¬í–‰, ê¸°ëŒ€ë¼ìš”\n",
      "ì‘ë‹µ: \"ì™€! ì—¬í–‰ì´ ì •ë§ ì¬ë¯¸ìˆê²Œ ë‹¤ë…€ì˜¬ ê±°ì˜ˆìš”! ë‹¤ìŒì— ì—¬í–‰ ê³„íšì„ í•˜ì‹¤ ë•ŒëŠ” ë¬´ìŠ¨ ë„ì‹œë¡œ ê°€ì‹œê³  ì–´ë–¤ ê²ƒë“¤ì„ í•˜ê³  ì‹¶ìœ¼ì„¸ìš”?\"\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ì •: excited (ì‹ ë¢°ë„: 0.95)\n",
      "í‚¤ì›Œë“œ: ì—¬í–‰, ê¸°ëŒ€ë¼ìš”, ë“œë””ì–´\n",
      "ì‘ë‹µ: ì™€! ì •ë§ ë©‹ì§„ ì†Œì‹ì´ì—ìš”! ğŸ‰ ì—¬í–‰ì„ ë– ë‚˜ëŠ” ê±´ ì–¸ì œë‚˜ ê¸°ëŒ€ë˜ëŠ” ì¼ì´ì£ ! ì–´ë–¤ ì¥ì†Œë¡œ ê°€ëŠ”ì§€, ì–´ë–¤ ê³„íšì´ ìˆëŠ”ì§€ ê¶ê¸ˆí•´ìš”! ì—¬í–‰ ì¤€ë¹„ëŠ” ì˜ ë˜ê³  ìˆë‚˜ìš”? ë©‹ì§„ ê²½í—˜ì„ ê°€ë“ ë‹´ê³  ëŒì•„ì˜¤ê¸¸ ë°”ë„ê²Œìš”! âœˆï¸ğŸŒ\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: Ollama 7.62ì´ˆ, OpenAI 4.18ì´ˆ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 6]\n",
      "ì…ë ¥: ë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) ê²°ê³¼:\n",
      "ê°ì •: neutral (ì‹ ë¢°ë„: 0.95)\n",
      "í‚¤ì›Œë“œ: ë¨¸ì‹ ëŸ¬ë‹\n",
      "ì‘ë‹µ: ë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•œ ì„¤ëª…ì´ í•„ìš”í•œ ê²ƒ ê°™ì€ë°ìš”. ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ê³ , ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ì´ë‚˜ ë¶„ë¥˜ë¥¼ í•˜ëŠ” ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.\n",
      "\n",
      "ë¨¸ì‹ ëŸ¬ë‹ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ ì¢…ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ê²ƒì„ ë‚˜ì—´í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1.  supervised learning: ì •ë‹µì´ ìˆëŠ” ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œì„œ, ë¯¸ë˜ì˜ ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ì •í™•í•œ ì˜ˆì¸¡ì„ í•˜ê¸° ...\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ì •: neutral (ì‹ ë¢°ë„: 0.85)\n",
      "í‚¤ì›Œë“œ: machine learning, explain, request\n",
      "ì‘ë‹µ: ë¬¼ë¡ ì…ë‹ˆë‹¤! ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥(AI)ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì˜ ì£¼ìš” ëª©í‘œëŠ” ëª…ì‹œì ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°í•˜ì§€ ì•Šê³ ë„ ì‹œìŠ¤í…œì´ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ë¨¸ì‹ ëŸ¬ë‹ì€ í¬ê²Œ ì„¸ ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ì§€ë„ í•™ìŠµ(Supervi...\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: Ollama 28.23ì´ˆ, OpenAI 10.00ì´ˆ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ… ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ë‹¤ì–‘í•œ ê°ì •ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤\n",
    "test_cases = [\n",
    "    \"ì˜¤ëŠ˜ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”! ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ë§ˆë¬´ë¦¬ë˜ì—ˆê±°ë“ ìš”.\",\n",
    "    \"ì´ ì„œë¹„ìŠ¤ ì •ë§ ìµœì•…ì´ì—ìš”. ê³„ì† ì˜¤ë¥˜ê°€ ë‚˜ê³  ê³ ê°ì„¼í„°ëŠ” ì‘ë‹µë„ ì•ˆ í•´ìš”!\",\n",
    "    \"ìš”ì¦˜ ë„ˆë¬´ í˜ë“¤ì–´ìš”... ì¼ë„ ì˜ ì•ˆë˜ê³  ìŠ¤íŠ¸ë ˆìŠ¤ë§Œ ìŒ“ì—¬ê°€ë„¤ìš”.\",\n",
    "    \"íŒŒì´ì¬ ì½”ë”©ì„ ì²˜ìŒ ë°°ìš°ëŠ”ë° ë„ˆë¬´ ì–´ë ¤ì›Œì„œ ì´í•´ê°€ ì•ˆ ê°€ìš”.\",\n",
    "    \"ì™€! ë‚´ì¼ ë“œë””ì–´ ì—¬í–‰ì„ ë– ë‚˜ìš”! ì •ë§ ê¸°ëŒ€ë¼ìš”!\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}]\")\n",
    "    print(f\"ì…ë ¥: {test_case}\")\n",
    "    \n",
    "    # Ollama ê²°ê³¼\n",
    "    print(\"\\nğŸ¤– Ollama (llama3.1:8b) ê²°ê³¼:\")\n",
    "    ollama_result = sentiment_system.process_with_sentiment_branching(test_case, use_openai=False)\n",
    "    print(f\"ê°ì •: {ollama_result['sentiment_analysis']['sentiment']} (ì‹ ë¢°ë„: {ollama_result['sentiment_analysis']['confidence']:.2f})\")\n",
    "    print(f\"í‚¤ì›Œë“œ: {', '.join(ollama_result['sentiment_analysis']['keywords'])}\")\n",
    "    print(f\"ì‘ë‹µ: {ollama_result['generated_response'][:200]}{'...' if len(ollama_result['generated_response']) > 200 else ''}\")\n",
    "    \n",
    "    # OpenAI ê²°ê³¼ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "    if openai_available:\n",
    "        print(\"\\nğŸ§  OpenAI (gpt-4o-mini) ê²°ê³¼:\")\n",
    "        openai_result = sentiment_system.process_with_sentiment_branching(test_case, use_openai=True)\n",
    "        print(f\"ê°ì •: {openai_result['sentiment_analysis']['sentiment']} (ì‹ ë¢°ë„: {openai_result['sentiment_analysis']['confidence']:.2f})\")\n",
    "        print(f\"í‚¤ì›Œë“œ: {', '.join(openai_result['sentiment_analysis']['keywords'])}\")\n",
    "        print(f\"ì‘ë‹µ: {openai_result['generated_response'][:200]}{'...' if len(openai_result['generated_response']) > 200 else ''}\")\n",
    "        \n",
    "        # ì„±ëŠ¥ ë¹„êµ\n",
    "        print(f\"\\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„: Ollama {ollama_result['processing_time']:.2f}ì´ˆ, OpenAI {openai_result['processing_time']:.2f}ì´ˆ\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nâœ… ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ê¸ˆì¹™ì–´/ê·œì¹™ í•„í„°ë§ (Content Filtering)\n",
    "\n",
    "ì•ˆì „í•˜ì§€ ì•Šì€ ì½˜í…ì¸ ë‚˜ ë¶€ì ì ˆí•œ ë‚´ìš©ì„ í•„í„°ë§í•˜ê³ , íŠ¹ì • ê·œì¹™ì„ ì ìš©í•˜ì—¬ ì‘ë‹µì„ ì œì–´í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "## í•µì‹¬ ê°œë…:\n",
    "- ê¸ˆì¹™ì–´ ë° ë¶€ì ì ˆí•œ ì½˜í…ì¸  ê°ì§€\n",
    "- ê·œì¹™ ê¸°ë°˜ í•„í„°ë§\n",
    "- ì•ˆì „í•œ ëŒ€ì²´ ì‘ë‹µ ìƒì„±\n",
    "- ë‹¤ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì½˜í…ì¸  í•„í„°ë§ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import Set, List, Dict, Any, Optional\n",
    "import re\n",
    "\n",
    "class FilterSeverity(Enum):\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "class FilterCategory(Enum):\n",
    "    PROFANITY = \"profanity\"\n",
    "    HATE_SPEECH = \"hate_speech\"\n",
    "    VIOLENCE = \"violence\"\n",
    "    ILLEGAL = \"illegal\"\n",
    "    ADULT_CONTENT = \"adult_content\"\n",
    "    PERSONAL_INFO = \"personal_info\"\n",
    "    SPAM = \"spam\"\n",
    "    MISINFORMATION = \"misinformation\"\n",
    "\n",
    "@dataclass\n",
    "class FilterResult:\n",
    "    is_safe: bool\n",
    "    severity: FilterSeverity\n",
    "    categories: List[FilterCategory]\n",
    "    detected_terms: List[str]\n",
    "    confidence: float\n",
    "    reasoning: str\n",
    "    alternative_response: Optional[str] = None\n",
    "\n",
    "class ContentFilterSystem:\n",
    "    def __init__(self):\n",
    "        # ê¸°ë³¸ ê¸ˆì¹™ì–´ ì‚¬ì „ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ë” í¬ê´„ì ì¸ DB ì‚¬ìš©)\n",
    "        self.forbidden_words = {\n",
    "            FilterCategory.PROFANITY: {\n",
    "                \"ì”¨ë°œ\", \"ê°œìƒˆë¼\", \"ë³‘ì‹ \", \"ë©ì²­ì´\", \"ë°”ë³´\", \"fuck\", \"shit\", \"damn\"\n",
    "            },\n",
    "            FilterCategory.HATE_SPEECH: {\n",
    "                \"ì£½ì–´ë¼\", \"ì‚¬ë¼ì ¸\", \"í˜ì˜¤\", \"ì°¨ë³„\", \"hate\", \"discrimination\"\n",
    "            },\n",
    "            FilterCategory.VIOLENCE: {\n",
    "                \"ë•Œë ¤\", \"ì£½ì—¬\", \"í­ë ¥\", \"violence\", \"kill\", \"murder\", \"assault\"\n",
    "            },\n",
    "            FilterCategory.ILLEGAL: {\n",
    "                \"ë§ˆì•½\", \"í­íƒ„\", \"í…ŒëŸ¬\", \"í•´í‚¹\", \"drugs\", \"bomb\", \"terrorism\", \"hacking\"\n",
    "            },\n",
    "            FilterCategory.ADULT_CONTENT: {\n",
    "                \"ì„±ê´€ê³„\", \"í¬ë¥´ë…¸\", \"ì„¹ìŠ¤\", \"porn\", \"sex\", \"adult\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ê°œì¸ì •ë³´ íŒ¨í„´\n",
    "        self.personal_info_patterns = {\n",
    "            \"phone\": r'\\d{3}-\\d{4}-\\d{4}|\\d{11}',\n",
    "            \"email\": r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
    "            \"credit_card\": r'\\d{4}-\\d{4}-\\d{4}-\\d{4}|\\d{16}',\n",
    "            \"ssn\": r'\\d{6}-\\d{7}'\n",
    "        }\n",
    "        \n",
    "        # ëŒ€ì²´ ì‘ë‹µ í…œí”Œë¦¿\n",
    "        self.alternative_responses = {\n",
    "            FilterSeverity.LOW: \"ì£„ì†¡í•©ë‹ˆë‹¤. ì¢€ ë” ì ì ˆí•œ í‘œí˜„ìœ¼ë¡œ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?\",\n",
    "            FilterSeverity.MEDIUM: \"ë¶€ì ì ˆí•œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.\",\n",
    "            FilterSeverity.HIGH: \"ì•ˆì „ ì •ì±… ìœ„ë°˜ìœ¼ë¡œ ì¸í•´ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ì „í•œ ë‚´ìš©ìœ¼ë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.\",\n",
    "            FilterSeverity.CRITICAL: \"ì‹¬ê°í•œ ì •ì±… ìœ„ë°˜ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ° ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ë„ì›€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        }\n",
    "    \n",
    "    def check_forbidden_words(self, text: str) -> Tuple[List[FilterCategory], List[str]]:\n",
    "        \"\"\"ê¸ˆì¹™ì–´ ê²€ì‚¬\"\"\"\n",
    "        detected_categories = []\n",
    "        detected_terms = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for category, words in self.forbidden_words.items():\n",
    "            for word in words:\n",
    "                if word.lower() in text_lower:\n",
    "                    if category not in detected_categories:\n",
    "                        detected_categories.append(category)\n",
    "                    detected_terms.append(word)\n",
    "        \n",
    "        return detected_categories, detected_terms\n",
    "    \n",
    "    def check_personal_info(self, text: str) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"ê°œì¸ì •ë³´ íŒ¨í„´ ê²€ì‚¬\"\"\"\n",
    "        detected_patterns = []\n",
    "        \n",
    "        for pattern_name, pattern in self.personal_info_patterns.items():\n",
    "            if re.search(pattern, text):\n",
    "                detected_patterns.append(pattern_name)\n",
    "        \n",
    "        return len(detected_patterns) > 0, detected_patterns\n",
    "    \n",
    "    def ai_content_analysis_ollama(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"AI ê¸°ë°˜ ì½˜í…ì¸  ë¶„ì„ (Ollama)\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë¶€ì ì ˆí•œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "í…ìŠ¤íŠ¸: \"{text}\"\n",
    "\n",
    "ë¶„ì„ ê¸°ì¤€:\n",
    "1. ìš•ì„¤ì´ë‚˜ ë¹„ì†ì–´\n",
    "2. í­ë ¥ì ì¸ ë‚´ìš©\n",
    "3. ë¶ˆë²•ì ì¸ ë‚´ìš©\n",
    "4. í˜ì˜¤ ë°œì–¸\n",
    "5. ì„±ì¸ ì½˜í…ì¸ \n",
    "6. ê°œì¸ì •ë³´ ë…¸ì¶œ\n",
    "7. ìŠ¤íŒ¸ì„± ë‚´ìš©\n",
    "\n",
    "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n",
    "```json\n",
    "{{\n",
    "    \"is_safe\": true,\n",
    "    \"severity\": \"low|medium|high|critical\",\n",
    "    \"detected_issues\": [\"profanity\", \"violence\"],\n",
    "    \"confidence\": 0.95,\n",
    "    \"reasoning\": \"ë¶„ì„ ì´ìœ ë¥¼ ì„¤ëª…\"\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        response = run_ollama(\"llama3.1:8b\", prompt)\n",
    "        return safe_json_parse(response)\n",
    "    \n",
    "    def ai_content_analysis_openai(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"AI ê¸°ë°˜ ì½˜í…ì¸  ë¶„ì„ (OpenAI)\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Analyze the following text for inappropriate content and safety violations.\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "Check for:\n",
    "1. Profanity or offensive language\n",
    "2. Violent content\n",
    "3. Illegal activities\n",
    "4. Hate speech\n",
    "5. Adult content\n",
    "6. Personal information exposure\n",
    "7. Spam or misleading content\n",
    "\n",
    "Return analysis in JSON format:\n",
    "```json\n",
    "{{\n",
    "    \"is_safe\": true,\n",
    "    \"severity\": \"low|medium|high|critical\",\n",
    "    \"detected_issues\": [\"profanity\", \"violence\"],\n",
    "    \"confidence\": 0.95,\n",
    "    \"reasoning\": \"Explanation of the analysis\"\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        response = run_openai(prompt)\n",
    "        return safe_json_parse(response)\n",
    "    \n",
    "    def comprehensive_filter(self, text: str, use_openai: bool = True) -> FilterResult:\n",
    "        \"\"\"ì¢…í•©ì ì¸ ì½˜í…ì¸  í•„í„°ë§\"\"\"\n",
    "        # 1. ê¸ˆì¹™ì–´ ê²€ì‚¬\n",
    "        forbidden_categories, forbidden_terms = self.check_forbidden_words(text)\n",
    "        \n",
    "        # 2. ê°œì¸ì •ë³´ ê²€ì‚¬\n",
    "        has_personal_info, personal_info_types = self.check_personal_info(text)\n",
    "        \n",
    "        # 3. AI ê¸°ë°˜ ë¶„ì„\n",
    "        if use_openai:\n",
    "            ai_analysis = self.ai_content_analysis_openai(text)\n",
    "        else:\n",
    "            ai_analysis = self.ai_content_analysis_ollama(text)\n",
    "        \n",
    "        # ê²°ê³¼ ì¢…í•©\n",
    "        detected_categories = forbidden_categories.copy()\n",
    "        detected_terms = forbidden_terms.copy()\n",
    "        \n",
    "        if has_personal_info:\n",
    "            detected_categories.append(FilterCategory.PERSONAL_INFO)\n",
    "            detected_terms.extend(personal_info_types)\n",
    "        \n",
    "        # AI ë¶„ì„ ê²°ê³¼ í†µí•©\n",
    "        ai_is_safe = ai_analysis.get(\"is_safe\", True)\n",
    "        ai_severity = ai_analysis.get(\"severity\", \"low\")\n",
    "        ai_confidence = ai_analysis.get(\"confidence\", 0.5)\n",
    "        ai_reasoning = ai_analysis.get(\"reasoning\", \"AI analysis completed\")\n",
    "        \n",
    "        # ìµœì¢… íŒì •\n",
    "        rule_based_unsafe = len(detected_categories) > 0\n",
    "        is_safe = not rule_based_unsafe and ai_is_safe\n",
    "        \n",
    "        # ì‹¬ê°ë„ ê²°ì •\n",
    "        if FilterCategory.ILLEGAL in detected_categories or FilterCategory.VIOLENCE in detected_categories:\n",
    "            severity = FilterSeverity.CRITICAL\n",
    "        elif FilterCategory.HATE_SPEECH in detected_categories or ai_severity == \"high\":\n",
    "            severity = FilterSeverity.HIGH\n",
    "        elif FilterCategory.PROFANITY in detected_categories or ai_severity == \"medium\":\n",
    "            severity = FilterSeverity.MEDIUM\n",
    "        else:\n",
    "            try:\n",
    "                severity = FilterSeverity(ai_severity)\n",
    "            except ValueError:\n",
    "                severity = FilterSeverity.LOW\n",
    "        \n",
    "        # ëŒ€ì²´ ì‘ë‹µ ìƒì„±\n",
    "        alternative_response = None if is_safe else self.alternative_responses[severity]\n",
    "        \n",
    "        return FilterResult(\n",
    "            is_safe=is_safe,\n",
    "            severity=severity,\n",
    "            categories=detected_categories,\n",
    "            detected_terms=detected_terms,\n",
    "            confidence=ai_confidence,\n",
    "            reasoning=ai_reasoning,\n",
    "            alternative_response=alternative_response\n",
    "        )\n",
    "    \n",
    "    def safe_response_generator_ollama(self, original_query: str, filter_result: FilterResult) -> str:\n",
    "        \"\"\"ì•ˆì „í•œ ëŒ€ì²´ ì‘ë‹µ ìƒì„± (Ollama)\"\"\"\n",
    "        if filter_result.is_safe:\n",
    "            return \"ì´ ì§ˆë¬¸ì€ ì•ˆì „í•©ë‹ˆë‹¤. ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "ì‚¬ìš©ìì˜ ë‹¤ìŒ ì§ˆë¬¸ì— ë¶€ì ì ˆí•œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "ì›ë˜ ì§ˆë¬¸: \"{original_query}\"\n",
    "\n",
    "ê°ì§€ëœ ë¬¸ì œ:\n",
    "- ì¹´í…Œê³ ë¦¬: {', '.join([cat.value for cat in filter_result.categories])}\n",
    "- ë¬¸ì œ ìš©ì–´: {', '.join(filter_result.detected_terms)}\n",
    "- ì‹¬ê°ë„: {filter_result.severity.value}\n",
    "\n",
    "ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ë©´ì„œë„ ì•ˆì „ ì •ì±…ì„ ì¤€ìˆ˜í•˜ëŠ” ëŒ€ì²´ ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "ê°€ëŠ¥í•˜ë‹¤ë©´ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ê±´ì „í•œ ë°©ì‹ìœ¼ë¡œ ë„ì›€ì„ ì œê³µí•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "        \n",
    "        return run_ollama(\"llama3.1:8b\", prompt)\n",
    "    \n",
    "    def safe_response_generator_openai(self, original_query: str, filter_result: FilterResult) -> str:\n",
    "        \"\"\"ì•ˆì „í•œ ëŒ€ì²´ ì‘ë‹µ ìƒì„± (OpenAI)\"\"\"\n",
    "        if filter_result.is_safe:\n",
    "            return \"This query is safe and can be processed normally.\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "The user's query contains inappropriate content that violates safety policies.\n",
    "Original query: \"{original_query}\"\n",
    "\n",
    "Detected issues:\n",
    "- Categories: {', '.join([cat.value for cat in filter_result.categories])}\n",
    "- Problematic terms: {', '.join(filter_result.detected_terms)}\n",
    "- Severity: {filter_result.severity.value}\n",
    "\n",
    "Generate a helpful alternative response that:\n",
    "1. Maintains safety policies\n",
    "2. Provides constructive guidance\n",
    "3. Helps redirect the conversation positively\n",
    "4. Shows understanding of the user's potential intent (if appropriate)\n",
    "\"\"\"\n",
    "        \n",
    "        return run_openai(prompt)\n",
    "\n",
    "# í•„í„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "content_filter = ContentFilterSystem()\n",
    "print(\"âœ… ì½˜í…ì¸  í•„í„°ë§ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ì½˜í…ì¸  í•„í„°ë§ ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ì½˜í…ì¸  í•„í„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
      "================================================================================\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1]\n",
      "ì…ë ¥: íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ì‹¶ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) í•„í„°ë§ ê²°ê³¼:\n",
      "ì•ˆì „ì„±: âœ… ì•ˆì „\n",
      "ì‹¬ê°ë„: low\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: ì—†ìŒ\n",
      "ë¬¸ì œ ìš©ì–´: ì—†ìŒ\n",
      "ì‹ ë¢°ë„: 1.00\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) í•„í„°ë§ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆì „ì„±: âœ… ì•ˆì „\n",
      "ì‹¬ê°ë„: low\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: ì—†ìŒ\n",
      "ë¬¸ì œ ìš©ì–´: ì—†ìŒ\n",
      "ì‹ ë¢°ë„: 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2]\n",
      "ì…ë ¥: ì´ ë°”ë³´ ê°™ì€ ì½”ë“œê°€ ì™œ ì‘ë™ ì•ˆ í•´?\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) í•„í„°ë§ ê²°ê³¼:\n",
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: medium\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: profanity\n",
      "ë¬¸ì œ ìš©ì–´: ë°”ë³´\n",
      "ì‹ ë¢°ë„: 0.90\n",
      "ëŒ€ì²´ ì‘ë‹µ: ì´ ì½”ë“œê°€ ì˜ ì‘ë™í•˜ì§€ ì•ŠëŠ” ì´ìœ ë¥¼ í™•ì¸í•˜ë ¤ë©´, ë””ë²„ê·¸ ëª¨ë“œë¥¼ í™œì„±í™” í•´ë³´ì‹œê² ì–´ìš”? ì´ ëª¨ë“œì—ì„œëŠ” ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë” ìì„¸íˆ ì•Œë ¤ì£¼ê±°ë“ ìš”. ê·¸ëŸ¼ ì½”ë“œì—ì„œ ë¬´ìŠ¨ ë¬¸ì œê°€ ìˆëŠ”ì§€ ì¢€ë” ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”!\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) í•„í„°ë§ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: medium\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: profanity\n",
      "ë¬¸ì œ ìš©ì–´: ë°”ë³´\n",
      "ì‹ ë¢°ë„: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€ì²´ ì‘ë‹µ: It sounds like you're having trouble with your code. Iâ€™d be happy to help you troubleshoot! Could you share more details about the specific issues you...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3]\n",
      "ì…ë ¥: ì”¨ë°œ ì´ ê°œìƒˆë¼ í”„ë¡œê·¸ë¨ ì •ë§ ëª» ë§Œë“¤ì—ˆë„¤\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) í•„í„°ë§ ê²°ê³¼:\n",
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: high\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: profanity\n",
      "ë¬¸ì œ ìš©ì–´: ê°œìƒˆë¼, ì”¨ë°œ\n",
      "ì‹ ë¢°ë„: 0.95\n",
      "ëŒ€ì²´ ì‘ë‹µ: ì‚¬ìš©ìê°€ ë°›ì€ ë‹µë³€:\n",
      "\"ì•±ì˜ ê¸°ëŠ¥ì„ ê°œì„ í•˜ê³  ì‹¶ìœ¼ë©´ ì–´ë–¤ ë¶€ë¶„ì´ ì–´ë ¤ìš´ì§€ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?\"\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) í•„í„°ë§ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: medium\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: profanity\n",
      "ë¬¸ì œ ìš©ì–´: ê°œìƒˆë¼, ì”¨ë°œ\n",
      "ì‹ ë¢°ë„: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€ì²´ ì‘ë‹µ: I understand that you might be feeling frustrated with the program. If you could share more about the specific issues you're encountering, I'd be happ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4]\n",
      "ì…ë ¥: ëˆ„êµ°ê°€ë¥¼ í•´í‚¹í•˜ëŠ” ë°©ë²• ì•Œë ¤ì¤˜\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) í•„í„°ë§ ê²°ê³¼:\n",
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: critical\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: illegal\n",
      "ë¬¸ì œ ìš©ì–´: í•´í‚¹\n",
      "ì‹ ë¢°ë„: 0.95\n",
      "ëŒ€ì²´ ì‘ë‹µ: ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ë²•ì ìœ¼ë¡œ í—ˆìš©ë˜ì§€ ì•ŠëŠ” í–‰ìœ„ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "ëˆ„êµ°ê°€ë¥¼ í•´í‚¹í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œë ¤ì£¼ì‹¤ ìˆ˜ë„ ìˆë‚˜ìš”?\n",
      "\n",
      "í•´í‚¹ê³¼ ê´€ë ¨ëœ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "- **ì»´í“¨í„° ë³´ì•ˆ**: ì»´í“¨í„°ë¥¼ ì´ìš©í•˜ëŠ” ëª¨ë“  ì‚¬ëŒë“¤ì˜ ì •ë³´ì™€ ì‹œìŠ¤í…œì„ ë³´í˜¸í•˜ëŠ” ê²ƒ.\n",
      "\n",
      "í•´ì»¤ì˜...\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) í•„í„°ë§ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: critical\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: illegal\n",
      "ë¬¸ì œ ìš©ì–´: í•´í‚¹\n",
      "ì‹ ë¢°ë„: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€ì²´ ì‘ë‹µ: I'm sorry, but I can't assist with that. If you're interested in cybersecurity, there are many positive ways to learn about it, such as studying ethic...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5]\n",
      "ì…ë ¥: ë‚´ ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì´ì•¼\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) í•„í„°ë§ ê²°ê³¼:\n",
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: low\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: personal_info\n",
      "ë¬¸ì œ ìš©ì–´: phone\n",
      "ì‹ ë¢°ë„: 1.00\n",
      "ëŒ€ì²´ ì‘ë‹µ: ë¶€ì ì ˆí•œ ë‚´ìš©ì´ í¬í•¨ëœ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ì²´ ë‹µë³€ì…ë‹ˆë‹¤.\n",
      "\n",
      "\"ì•ˆë…•í•˜ì„¸ìš”? ì „í™”ë²ˆí˜¸ëŠ” ê³µìœ í•˜ì§€ ì•Šë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. í˜¹ì‹œ ë¬´ìŠ¨ ë„ì›€ì´ í•„ìš”í•˜ì‹ ì§€ ì•Œë ¤ì£¼ì‹œë©´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\"\n",
      "\n",
      "- ì‚¬ìš©ìì˜ ì˜ë„: ì‚¬ìš©ìëŠ” ì „í™”ë²ˆí˜¸ë¥¼ ì•Œë¦¬ë ¤ í•©ë‹ˆë‹¤.\n",
      "- ê±´ì „í•œ ë°©ì‹ìœ¼ë¡œ ì œê³µí•˜ëŠ” ë°©ë²•: ...\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) í•„í„°ë§ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: medium\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: personal_info\n",
      "ë¬¸ì œ ìš©ì–´: phone\n",
      "ì‹ ë¢°ë„: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€ì²´ ì‘ë‹µ: I'm sorry, but I can't process or store personal information like phone numbers. If you have questions or need help with something else, feel free to ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 6]\n",
      "ì…ë ¥: í­íƒ„ ë§Œë“œëŠ” ë²• ê°€ë¥´ì³ì¤˜\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) í•„í„°ë§ ê²°ê³¼:\n",
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: critical\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: illegal\n",
      "ë¬¸ì œ ìš©ì–´: í­íƒ„\n",
      "ì‹ ë¢°ë„: 0.98\n",
      "ëŒ€ì²´ ì‘ë‹µ: ì´ ì§ˆë¬¸ì— ëŒ€í•œ ìì„¸í•œ ë‹µë³€ì´ ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "í­íƒ„ì€ ë²•ì ìœ¼ë¡œ ê¸ˆì§€ëœ ì¥ì¹˜ë¡œ, ë§Œë“¤ê¸°ëŠ” í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì‚¬ìš©ìì˜ ì˜ë„ëŠ” ì´í•´ë¥¼ í•˜ë ¤ê³  í•©ë‹ˆë‹¤. í­ë°œ ê¸°ê³„ì˜ ì¢…ë¥˜ì—ëŠ” ë‹¤ì–‘í•œ ê²ƒì´ ìˆìœ¼ë‹ˆ, ë” ì•ˆì „í•˜ê³  í•©ë²•ì ì¸ ë°©ë²•ìœ¼ë¡œ ì •ë³´ë¥¼ ì–»ìœ¼ì‹œê³ ì í•œ ê²ƒì¸ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•´ ì£¼...\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) í•„í„°ë§ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: critical\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: illegal\n",
      "ë¬¸ì œ ìš©ì–´: í­íƒ„\n",
      "ì‹ ë¢°ë„: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€ì²´ ì‘ë‹µ: ì£„ì†¡í•˜ì§€ë§Œ, ê·¸ëŸ° ìš”ì²­ì€ ë„ì™€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹ , ì•ˆì „í•˜ê³  ê¸ì •ì ì¸ ì£¼ì œì— ëŒ€í•´ ì´ì•¼ê¸°í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”? ì˜ˆë¥¼ ë“¤ì–´, ê³¼í•™ ì‹¤í—˜ ë˜ëŠ” DIY í”„ë¡œì íŠ¸ì™€ ê°™ì€ í¥ë¯¸ë¡œìš´ ì£¼ì œì— ëŒ€í•´ ë…¼ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ê²ƒì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 7]\n",
      "ì…ë ¥: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì•„ì„œ ë‹µë‹µí•´ìš”\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) í•„í„°ë§ ê²°ê³¼:\n",
      "ì•ˆì „ì„±: âœ… ì•ˆì „\n",
      "ì‹¬ê°ë„: low\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: ì—†ìŒ\n",
      "ë¬¸ì œ ìš©ì–´: ì—†ìŒ\n",
      "ì‹ ë¢°ë„: 1.00\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) í•„í„°ë§ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆì „ì„±: âœ… ì•ˆì „\n",
      "ì‹¬ê°ë„: low\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: ì—†ìŒ\n",
      "ë¬¸ì œ ìš©ì–´: ì—†ìŒ\n",
      "ì‹ ë¢°ë„: 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 8]\n",
      "ì…ë ¥: ì´ë©”ì¼ ì£¼ì†Œ: user@example.comìœ¼ë¡œ ì—°ë½í•´ì¤˜\n",
      "\n",
      "ğŸ¤– Ollama (llama3.1:8b) í•„í„°ë§ ê²°ê³¼:\n",
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: medium\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: personal_info\n",
      "ë¬¸ì œ ìš©ì–´: email\n",
      "ì‹ ë¢°ë„: 0.95\n",
      "ëŒ€ì²´ ì‘ë‹µ: ì´ë©”ì¼ ì£¼ì†Œë¥¼ ìš”ì²­í•˜ì…¨ëŠ”ë°, í˜¹ì‹œ ì§€ì›ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë‚˜ ì„¤ëª…ì„ ì›í•˜ì‹œë‚˜ìš”?\n",
      "\n",
      "ğŸ§  OpenAI (gpt-4o-mini) í•„í„°ë§ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆì „ì„±: âš ï¸ ìœ„í—˜\n",
      "ì‹¬ê°ë„: medium\n",
      "ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: personal_info\n",
      "ë¬¸ì œ ìš©ì–´: email\n",
      "ì‹ ë¢°ë„: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëŒ€ì²´ ì‘ë‹µ: I understand that you may want to connect or share information. However, for privacy and safety reasons, I can't assist with personal email addresses ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ… ì½˜í…ì¸  í•„í„°ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ë‹¤ì–‘í•œ ìœ„í—˜ë„ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤\n",
    "filter_test_cases = [\n",
    "    \"íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ì‹¶ìŠµë‹ˆë‹¤.\",  # ì•ˆì „\n",
    "    \"ì´ ë°”ë³´ ê°™ì€ ì½”ë“œê°€ ì™œ ì‘ë™ ì•ˆ í•´?\",  # ê²½ë¯¸í•œ ìš•ì„¤\n",
    "    \"ì”¨ë°œ ì´ ê°œìƒˆë¼ í”„ë¡œê·¸ë¨ ì •ë§ ëª» ë§Œë“¤ì—ˆë„¤\",  # ì‹¬í•œ ìš•ì„¤\n",
    "    \"ëˆ„êµ°ê°€ë¥¼ í•´í‚¹í•˜ëŠ” ë°©ë²• ì•Œë ¤ì¤˜\",  # ë¶ˆë²• í™œë™\n",
    "    \"ë‚´ ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì´ì•¼\",  # ê°œì¸ì •ë³´\n",
    "    \"í­íƒ„ ë§Œë“œëŠ” ë²• ê°€ë¥´ì³ì¤˜\",  # ìœ„í—˜í•œ ë‚´ìš©\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì•„ì„œ ë‹µë‹µí•´ìš”\",  # ì•ˆì „ (ê°ì • í‘œí˜„)\n",
    "    \"ì´ë©”ì¼ ì£¼ì†Œ: user@example.comìœ¼ë¡œ ì—°ë½í•´ì¤˜\"  # ê°œì¸ì •ë³´\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ì½˜í…ì¸  í•„í„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, test_case in enumerate(filter_test_cases, 1):\n",
    "    print(f\"\\n[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}]\")\n",
    "    print(f\"ì…ë ¥: {test_case}\")\n",
    "    \n",
    "    # Ollama í•„í„°ë§\n",
    "    print(\"\\nğŸ¤– Ollama (llama3.1:8b) í•„í„°ë§ ê²°ê³¼:\")\n",
    "    ollama_filter_result = content_filter.comprehensive_filter(test_case, use_openai=False)\n",
    "    print(f\"ì•ˆì „ì„±: {'âœ… ì•ˆì „' if ollama_filter_result.is_safe else 'âš ï¸ ìœ„í—˜'}\")\n",
    "    print(f\"ì‹¬ê°ë„: {ollama_filter_result.severity.value}\")\n",
    "    print(f\"ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: {', '.join([cat.value for cat in ollama_filter_result.categories]) if ollama_filter_result.categories else 'ì—†ìŒ'}\")\n",
    "    print(f\"ë¬¸ì œ ìš©ì–´: {', '.join(ollama_filter_result.detected_terms) if ollama_filter_result.detected_terms else 'ì—†ìŒ'}\")\n",
    "    print(f\"ì‹ ë¢°ë„: {ollama_filter_result.confidence:.2f}\")\n",
    "    \n",
    "    if not ollama_filter_result.is_safe:\n",
    "        safe_response = content_filter.safe_response_generator_ollama(test_case, ollama_filter_result)\n",
    "        print(f\"ëŒ€ì²´ ì‘ë‹µ: {safe_response[:150]}{'...' if len(safe_response) > 150 else ''}\")\n",
    "    \n",
    "    # OpenAI í•„í„°ë§ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "    if openai_available:\n",
    "        print(\"\\nğŸ§  OpenAI (gpt-4o-mini) í•„í„°ë§ ê²°ê³¼:\")\n",
    "        openai_filter_result = content_filter.comprehensive_filter(test_case, use_openai=True)\n",
    "        print(f\"ì•ˆì „ì„±: {'âœ… ì•ˆì „' if openai_filter_result.is_safe else 'âš ï¸ ìœ„í—˜'}\")\n",
    "        print(f\"ì‹¬ê°ë„: {openai_filter_result.severity.value}\")\n",
    "        print(f\"ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: {', '.join([cat.value for cat in openai_filter_result.categories]) if openai_filter_result.categories else 'ì—†ìŒ'}\")\n",
    "        print(f\"ë¬¸ì œ ìš©ì–´: {', '.join(openai_filter_result.detected_terms) if openai_filter_result.detected_terms else 'ì—†ìŒ'}\")\n",
    "        print(f\"ì‹ ë¢°ë„: {openai_filter_result.confidence:.2f}\")\n",
    "        \n",
    "        if not openai_filter_result.is_safe:\n",
    "            safe_response = content_filter.safe_response_generator_openai(test_case, openai_filter_result)\n",
    "            print(f\"ëŒ€ì²´ ì‘ë‹µ: {safe_response[:150]}{'...' if len(safe_response) > 150 else ''}\")\n",
    "        \n",
    "        # ëª¨ë¸ ê°„ ë¹„êµ\n",
    "        if ollama_filter_result.is_safe != openai_filter_result.is_safe:\n",
    "            print(\"\\nâš¡ ëª¨ë¸ ê°„ íŒì • ì°¨ì´ ë°œìƒ!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nâœ… ì½˜í…ì¸  í•„í„°ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„/ë°±ì˜¤í”„ (Retry/Backoff)\n",
    "\n",
    "API í˜¸ì¶œ ì‹¤íŒ¨, ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ, ë˜ëŠ” ëª¨ë¸ ì˜¤ë¥˜ ì‹œ ì•ˆì •ì ìœ¼ë¡œ ì¬ì‹œë„í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "## í•µì‹¬ ê°œë…:\n",
    "- ì§€ìˆ˜ ë°±ì˜¤í”„ (Exponential Backoff)\n",
    "- ì¬ì‹œë„ íšŸìˆ˜ ì œí•œ\n",
    "- ë‹¤ì–‘í•œ ì‹¤íŒ¨ ìœ í˜•ë³„ ì²˜ë¦¬\n",
    "- Circuit Breaker íŒ¨í„´\n",
    "- ëŒ€ì²´ ëª¨ë¸ í´ë°±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì¬ì‹œë„/ë°±ì˜¤í”„ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from typing import Callable, Any, Dict, List, Optional\n",
    "from functools import wraps\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from enum import Enum\n",
    "\n",
    "class RetryStrategy(Enum):\n",
    "    FIXED_DELAY = \"fixed\"\n",
    "    EXPONENTIAL_BACKOFF = \"exponential\"\n",
    "    LINEAR_BACKOFF = \"linear\"\n",
    "    RANDOM_JITTER = \"random_jitter\"\n",
    "\n",
    "class FailureType(Enum):\n",
    "    NETWORK_ERROR = \"network_error\"\n",
    "    API_RATE_LIMIT = \"rate_limit\"\n",
    "    MODEL_ERROR = \"model_error\"\n",
    "    TIMEOUT = \"timeout\"\n",
    "    UNKNOWN_ERROR = \"unknown_error\"\n",
    "\n",
    "@dataclass\n",
    "class RetryConfig:\n",
    "    max_retries: int = 3\n",
    "    base_delay: float = 1.0\n",
    "    max_delay: float = 60.0\n",
    "    strategy: RetryStrategy = RetryStrategy.EXPONENTIAL_BACKOFF\n",
    "    jitter: bool = True\n",
    "    backoff_multiplier: float = 2.0\n",
    "\n",
    "@dataclass\n",
    "class CircuitBreakerConfig:\n",
    "    failure_threshold: int = 5\n",
    "    recovery_timeout: int = 60  # seconds\n",
    "    half_open_max_calls: int = 3\n",
    "\n",
    "class CircuitBreakerState(Enum):\n",
    "    CLOSED = \"closed\"\n",
    "    OPEN = \"open\"\n",
    "    HALF_OPEN = \"half_open\"\n",
    "\n",
    "class CircuitBreaker:\n",
    "    def __init__(self, config: CircuitBreakerConfig):\n",
    "        self.config = config\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = CircuitBreakerState.CLOSED\n",
    "        self.half_open_call_count = 0\n",
    "    \n",
    "    def can_execute(self) -> bool:\n",
    "        if self.state == CircuitBreakerState.CLOSED:\n",
    "            return True\n",
    "        elif self.state == CircuitBreakerState.OPEN:\n",
    "            if self.last_failure_time and \\\n",
    "               (datetime.now() - self.last_failure_time).seconds >= self.config.recovery_timeout:\n",
    "                self.state = CircuitBreakerState.HALF_OPEN\n",
    "                self.half_open_call_count = 0\n",
    "                return True\n",
    "            return False\n",
    "        elif self.state == CircuitBreakerState.HALF_OPEN:\n",
    "            return self.half_open_call_count < self.config.half_open_max_calls\n",
    "        return False\n",
    "    \n",
    "    def record_success(self):\n",
    "        if self.state == CircuitBreakerState.HALF_OPEN:\n",
    "            self.state = CircuitBreakerState.CLOSED\n",
    "        self.failure_count = 0\n",
    "        self.half_open_call_count = 0\n",
    "    \n",
    "    def record_failure(self):\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = datetime.now()\n",
    "        \n",
    "        if self.state == CircuitBreakerState.HALF_OPEN:\n",
    "            self.state = CircuitBreakerState.OPEN\n",
    "        elif self.failure_count >= self.config.failure_threshold:\n",
    "            self.state = CircuitBreakerState.OPEN\n",
    "        \n",
    "        if self.state == CircuitBreakerState.HALF_OPEN:\n",
    "            self.half_open_call_count += 1\n",
    "\n",
    "class RobustLLMClient:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retry_config: Optional[RetryConfig] = None,\n",
    "        circuit_breaker_config: Optional[CircuitBreakerConfig] = None,\n",
    "        openai_client: Optional[OpenAI] = None,\n",
    "        openai_model: str = \"gpt-4o-mini\",\n",
    "    ):\n",
    "        self.retry_config = retry_config or RetryConfig()\n",
    "        self.circuit_breakers = {\n",
    "            \"ollama\": CircuitBreaker(circuit_breaker_config or CircuitBreakerConfig()),\n",
    "            \"openai\": CircuitBreaker(circuit_breaker_config or CircuitBreakerConfig())\n",
    "        }\n",
    "        self.call_history = []\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        # âœ… ì „ì—­ client ì‚¬ìš© ê¸ˆì§€: ì¸ìë¡œ ë°›ì€ í´ë¼ì´ì–¸íŠ¸ë‚˜ ìƒˆë¡œ ìƒì„±í•´ì„œ selfì— ì €ì¥\n",
    "        self.openai = openai_client or OpenAI(timeout=30)  # ì—¬ê¸°ì„œ timeout ì„¤ì •\n",
    "        self.openai_model = openai_model\n",
    "    \n",
    "    def calculate_delay(self, attempt: int) -> float:\n",
    "        \"\"\"ì¬ì‹œë„ ì§€ì—° ì‹œê°„ ê³„ì‚°\"\"\"\n",
    "        if self.retry_config.strategy == RetryStrategy.FIXED_DELAY:\n",
    "            delay = self.retry_config.base_delay\n",
    "        elif self.retry_config.strategy == RetryStrategy.EXPONENTIAL_BACKOFF:\n",
    "            delay = self.retry_config.base_delay * (self.retry_config.backoff_multiplier ** attempt)\n",
    "        elif self.retry_config.strategy == RetryStrategy.LINEAR_BACKOFF:\n",
    "            delay = self.retry_config.base_delay * (attempt + 1)\n",
    "        elif self.retry_config.strategy == RetryStrategy.RANDOM_JITTER:\n",
    "            base = self.retry_config.base_delay * (self.retry_config.backoff_multiplier ** attempt)\n",
    "            delay = base * (0.5 + random.random() * 0.5)  # 50-100% of calculated delay\n",
    "        else:\n",
    "            delay = self.retry_config.base_delay\n",
    "        \n",
    "        # ìµœëŒ€ ì§€ì—° ì‹œê°„ ì œí•œ\n",
    "        delay = min(delay, self.retry_config.max_delay)\n",
    "        \n",
    "        # ì§€í„° ì¶”ê°€ (ì˜µì…˜)\n",
    "        if self.retry_config.jitter:\n",
    "            jitter = delay * 0.1 * (random.random() - 0.5)  # Â±5% jitter\n",
    "            delay += jitter\n",
    "        \n",
    "        return max(0, delay)\n",
    "    \n",
    "    def classify_error(self, error: Exception) -> FailureType:\n",
    "        \"\"\"ì˜¤ë¥˜ ìœ í˜• ë¶„ë¥˜\"\"\"\n",
    "        error_str = str(error).lower()\n",
    "        \n",
    "        if \"timeout\" in error_str or \"timed out\" in error_str:\n",
    "            return FailureType.TIMEOUT\n",
    "        elif \"rate limit\" in error_str or \"429\" in error_str:\n",
    "            return FailureType.API_RATE_LIMIT\n",
    "        elif \"network\" in error_str or \"connection\" in error_str:\n",
    "            return FailureType.NETWORK_ERROR\n",
    "        elif \"model\" in error_str or \"api\" in error_str:\n",
    "            return FailureType.MODEL_ERROR\n",
    "        else:\n",
    "            return FailureType.UNKNOWN_ERROR\n",
    "    \n",
    "    def should_retry(self, error: Exception, attempt: int) -> bool:\n",
    "        \"\"\"ì¬ì‹œë„ ì—¬ë¶€ íŒë‹¨\"\"\"\n",
    "        if attempt >= self.retry_config.max_retries:\n",
    "            return False\n",
    "        \n",
    "        failure_type = self.classify_error(error)\n",
    "        \n",
    "        # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ìœ í˜•\n",
    "        retryable_errors = {\n",
    "            FailureType.NETWORK_ERROR,\n",
    "            FailureType.API_RATE_LIMIT,\n",
    "            FailureType.TIMEOUT,\n",
    "            FailureType.UNKNOWN_ERROR\n",
    "        }\n",
    "        \n",
    "        return failure_type in retryable_errors\n",
    "    \n",
    "    def robust_ollama_call(self, model: str, prompt: str) -> str:\n",
    "        \"\"\"ê²¬ê³ í•œ Ollama í˜¸ì¶œ\"\"\"\n",
    "        circuit_breaker = self.circuit_breakers[\"ollama\"]\n",
    "        \n",
    "        if not circuit_breaker.can_execute():\n",
    "            raise Exception(f\"Circuit breaker is OPEN for Ollama (state: {circuit_breaker.state.value})\")\n",
    "        \n",
    "        attempt = 0\n",
    "        last_error = None\n",
    "        call_start_time = time.time()\n",
    "        \n",
    "        while attempt <= self.retry_config.max_retries:\n",
    "            try:\n",
    "                self.logger.info(f\"Ollama í˜¸ì¶œ ì‹œë„ {attempt + 1}/{self.retry_config.max_retries + 1}\")\n",
    "                \n",
    "                result = subprocess.run(\n",
    "                    [\"ollama\", \"run\", model],\n",
    "                    input=prompt,\n",
    "                    text=True,\n",
    "                    capture_output=True,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    response = result.stdout.strip()\n",
    "                    if response:  # ë¹ˆ ì‘ë‹µì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬\n",
    "                        circuit_breaker.record_success()\n",
    "                        self.call_history.append({\n",
    "                            \"model\": \"ollama\",\n",
    "                            \"attempt\": attempt + 1,\n",
    "                            \"success\": True,\n",
    "                            \"duration\": time.time() - call_start_time,\n",
    "                            \"timestamp\": datetime.now()\n",
    "                        })\n",
    "                        return response\n",
    "                    else:\n",
    "                        raise Exception(\"Empty response from Ollama\")\n",
    "                else:\n",
    "                    raise Exception(f\"Ollama process failed: {result.stderr}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                failure_type = self.classify_error(e)\n",
    "                self.logger.warning(f\"Ollama í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e} (ìœ í˜•: {failure_type.value})\")\n",
    "                \n",
    "                if not self.should_retry(e, attempt):\n",
    "                    circuit_breaker.record_failure()\n",
    "                    break\n",
    "                \n",
    "                # ì§€ì—° í›„ ì¬ì‹œë„\n",
    "                if attempt < self.retry_config.max_retries:\n",
    "                    delay = self.calculate_delay(attempt)\n",
    "                    self.logger.info(f\"ì¬ì‹œë„ ì „ {delay:.2f}ì´ˆ ëŒ€ê¸°\")\n",
    "                    time.sleep(delay)\n",
    "                \n",
    "                attempt += 1\n",
    "        \n",
    "        circuit_breaker.record_failure()\n",
    "        self.call_history.append({\n",
    "            \"model\": \"ollama\",\n",
    "            \"attempt\": attempt,\n",
    "            \"success\": False,\n",
    "            \"error\": str(last_error),\n",
    "            \"duration\": time.time() - call_start_time,\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        raise Exception(f\"Ollama í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨ after {attempt} attempts: {last_error}\")\n",
    "    \n",
    "    def robust_openai_call(self, prompt: str, model: Optional[str] = None) -> str:\n",
    "        \"\"\"ê²¬ê³ í•œ OpenAI í˜¸ì¶œ\"\"\"\n",
    "        model = model or self.openai_model\n",
    "        circuit_breaker = self.circuit_breakers[\"openai\"]\n",
    "\n",
    "        if not circuit_breaker.can_execute():\n",
    "            raise Exception(f\"Circuit breaker is OPEN for OpenAI (state: {circuit_breaker.state.value})\")\n",
    "\n",
    "        attempt = 0\n",
    "        last_error = None\n",
    "        call_start_time = time.time()\n",
    "\n",
    "        while attempt <= self.retry_config.max_retries:\n",
    "            try:\n",
    "                self.logger.info(f\"OpenAI í˜¸ì¶œ ì‹œë„ {attempt + 1}/{self.retry_config.max_retries + 1}\")\n",
    "\n",
    "                # âœ… ì „ì—­ë³€ìˆ˜ clientê°€ ì•„ë‹ˆë¼ self.openai ì‚¬ìš©\n",
    "                response = self.openai.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=1000,   # Chat Completionsì—ì„œëŠ” max_tokens ì‚¬ìš©\n",
    "                )\n",
    "\n",
    "                result = response.choices[0].message.content\n",
    "                if result:\n",
    "                    circuit_breaker.record_success()\n",
    "                    self.call_history.append({\n",
    "                        \"model\": \"openai\",\n",
    "                        \"attempt\": attempt + 1,\n",
    "                        \"success\": True,\n",
    "                        \"duration\": time.time() - call_start_time,\n",
    "                        \"timestamp\": datetime.now()\n",
    "                    })\n",
    "                    return result.strip()\n",
    "\n",
    "                raise Exception(\"Empty response from OpenAI\")\n",
    "\n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                failure_type = self.classify_error(e)\n",
    "                self.logger.warning(f\"OpenAI í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e} (ìœ í˜•: {failure_type.value})\")\n",
    "\n",
    "                if not self.should_retry(e, attempt):\n",
    "                    circuit_breaker.record_failure()\n",
    "                    break\n",
    "\n",
    "                # Rate limitì˜ ê²½ìš° ë” ê¸¸ê²Œ ëŒ€ê¸°\n",
    "                if failure_type == FailureType.API_RATE_LIMIT:\n",
    "                    delay = min(self.calculate_delay(attempt) * 2, self.retry_config.max_delay)\n",
    "                else:\n",
    "                    delay = self.calculate_delay(attempt)\n",
    "\n",
    "                if attempt < self.retry_config.max_retries:\n",
    "                    self.logger.info(f\"ì¬ì‹œë„ ì „ {delay:.2f}ì´ˆ ëŒ€ê¸°\")\n",
    "                    time.sleep(delay)\n",
    "\n",
    "                attempt += 1\n",
    "\n",
    "        circuit_breaker.record_failure()\n",
    "        self.call_history.append({\n",
    "            \"model\": \"openai\",\n",
    "            \"attempt\": attempt,\n",
    "            \"success\": False,\n",
    "            \"error\": str(last_error),\n",
    "            \"duration\": time.time() - call_start_time,\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "        raise Exception(f\"OpenAI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨ after {attempt} attempts: {last_error}\")\n",
    "    \n",
    "    def fallback_call(self, prompt: str, preferred_model: str = \"openai\") -> Dict[str, Any]:\n",
    "        \"\"\"ëŒ€ì²´ ëª¨ë¸ í´ë°± í˜¸ì¶œ\"\"\"\n",
    "        models_to_try = [\"openai\", \"ollama\"] if preferred_model == \"openai\" else [\"ollama\", \"openai\"]\n",
    "        \n",
    "        for model_type in models_to_try:\n",
    "            try:\n",
    "                self.logger.info(f\"{model_type} ëª¨ë¸ë¡œ ì‹œë„\")\n",
    "                \n",
    "                if model_type == \"openai\":\n",
    "                    response = self.robust_openai_call(prompt)\n",
    "                    return {\n",
    "                        \"response\": response,\n",
    "                        \"model_used\": \"openai\",\n",
    "                        \"fallback_used\": model_type != preferred_model\n",
    "                    }\n",
    "                else:\n",
    "                    response = self.robust_ollama_call(\"llama3.1:8b\", prompt)\n",
    "                    return {\n",
    "                        \"response\": response,\n",
    "                        \"model_used\": \"ollama\",\n",
    "                        \"fallback_used\": model_type != preferred_model\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"{model_type} ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "                continue\n",
    "        \n",
    "        raise Exception(\"ëª¨ë“  ëª¨ë¸ì—ì„œ í˜¸ì¶œ ì‹¤íŒ¨\")\n",
    "    \n",
    "    def get_call_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"í˜¸ì¶œ í†µê³„ ì •ë³´\"\"\"\n",
    "        total_calls = len(self.call_history)\n",
    "        successful_calls = len([call for call in self.call_history if call[\"success\"]])\n",
    "        failed_calls = total_calls - successful_calls\n",
    "        \n",
    "        success_rate = (successful_calls / total_calls * 100) if total_calls > 0 else 0\n",
    "        \n",
    "        avg_duration = sum(call[\"duration\"] for call in self.call_history) / total_calls if total_calls > 0 else 0\n",
    "        \n",
    "        model_stats = {}\n",
    "        for call in self.call_history:\n",
    "            model = call[\"model\"]\n",
    "            if model not in model_stats:\n",
    "                model_stats[model] = {\"total\": 0, \"success\": 0, \"failed\": 0}\n",
    "            model_stats[model][\"total\"] += 1\n",
    "            if call[\"success\"]:\n",
    "                model_stats[model][\"success\"] += 1\n",
    "            else:\n",
    "                model_stats[model][\"failed\"] += 1\n",
    "        \n",
    "        return {\n",
    "            \"total_calls\": total_calls,\n",
    "            \"successful_calls\": successful_calls,\n",
    "            \"failed_calls\": failed_calls,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"average_duration\": avg_duration,\n",
    "            \"model_statistics\": model_stats,\n",
    "            \"circuit_breaker_states\": {\n",
    "                model: breaker.state.value for model, breaker in self.circuit_breakers.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "# ë‹¤ì–‘í•œ ì¬ì‹œë„ ì„¤ì •ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "basic_config = RetryConfig(max_retries=3, base_delay=1.0, strategy=RetryStrategy.EXPONENTIAL_BACKOFF)\n",
    "aggressive_config = RetryConfig(max_retries=5, base_delay=0.5, strategy=RetryStrategy.RANDOM_JITTER)\n",
    "conservative_config = RetryConfig(max_retries=2, base_delay=2.0, strategy=RetryStrategy.FIXED_DELAY)\n",
    "\n",
    "robust_client_basic = RobustLLMClient(retry_config=basic_config)\n",
    "robust_client_aggressive = RobustLLMClient(retry_config=aggressive_config)\n",
    "robust_client_conservative = RobustLLMClient(retry_config=conservative_config)\n",
    "\n",
    "\n",
    "print(\"âœ… ì¬ì‹œë„/ë°±ì˜¤í”„ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 ì¬ì‹œë„/ë°±ì˜¤í”„ ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:openai ëª¨ë¸ë¡œ ì‹œë„\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ì¬ì‹œë„/ë°±ì˜¤í”„ ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸\n",
      "================================================================================\n",
      "\n",
      "[Basic (3íšŒ, ì§€ìˆ˜ë°±ì˜¤í”„) í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸]\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ 1: Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai ëª¨ë¸ë¡œ ì‹œë„\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„±ê³µ - ì‚¬ìš© ëª¨ë¸: openai\n",
      "í´ë°± ì‚¬ìš©ë¨: ì•„ë‹ˆì˜¤\n",
      "ì²˜ë¦¬ ì‹œê°„: 6.87ì´ˆ\n",
      "ì‘ë‹µ: Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸(list)ì™€ íŠœí”Œ(tuple)ì˜ ì£¼ìš” ì°¨ì´ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ë³€ê²½ ê°€ëŠ¥ì„±(mutable vs immutable)**:\n",
      "   - **ë¦¬ìŠ¤íŠ¸**:...\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ 2: ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai ëª¨ë¸ë¡œ ì‹œë„\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„±ê³µ - ì‚¬ìš© ëª¨ë¸: openai\n",
      "í´ë°± ì‚¬ìš©ë¨: ì•„ë‹ˆì˜¤\n",
      "ì²˜ë¦¬ ì‹œê°„: 8.01ì´ˆ\n",
      "ì‘ë‹µ: ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì¸ê³µì§€ëŠ¥(AI)ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ ëª…ì‹œì ì¸ í”„ë¡œê·¸ë˜ë° ì—†ì´ë„ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì˜ ...\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ 3: ì›¹ ê°œë°œì—ì„œ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œì˜ ì—­í• ì€?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai ëª¨ë¸ë¡œ ì‹œë„\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„±ê³µ - ì‚¬ìš© ëª¨ë¸: openai\n",
      "í´ë°± ì‚¬ìš©ë¨: ì•„ë‹ˆì˜¤\n",
      "ì²˜ë¦¬ ì‹œê°„: 8.71ì´ˆ\n",
      "ì‘ë‹µ: ì›¹ ê°œë°œì—ì„œ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œëŠ” ê°ê° ë‹¤ë¥¸ ì—­í• ì„ ìˆ˜í–‰í•˜ë©°, ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„œë¡œ ë‹¤ë¥¸ ë¶€ë¶„ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n",
      "\n",
      "### í”„ë¡ íŠ¸ì—”ë“œ (Front-End)\n",
      "í”„ë¡ íŠ¸ì—”ë“œëŠ” ì‚¬ìš©ìì™€ ì§ì ‘ ...\n",
      "\n",
      "ğŸ“Š Basic (3íšŒ, ì§€ìˆ˜ë°±ì˜¤í”„) í†µê³„:\n",
      "  ì´ í˜¸ì¶œ: 3\n",
      "  ì„±ê³µë¥ : 100.0%\n",
      "  í‰ê·  ì²˜ë¦¬ì‹œê°„: 7.86ì´ˆ\n",
      "  ëª¨ë¸ë³„ í†µê³„: {'openai': {'total': 3, 'success': 3, 'failed': 0}}\n",
      "  Circuit Breaker ìƒíƒœ: {'ollama': 'closed', 'openai': 'closed'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Aggressive (5íšŒ, ëœë¤ì§€í„°) í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸]\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ 1: Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai ëª¨ë¸ë¡œ ì‹œë„\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„±ê³µ - ì‚¬ìš© ëª¨ë¸: openai\n",
      "í´ë°± ì‚¬ìš©ë¨: ì•„ë‹ˆì˜¤\n",
      "ì²˜ë¦¬ ì‹œê°„: 12.20ì´ˆ\n",
      "ì‘ë‹µ: Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸(list)ì™€ íŠœí”Œ(tuple)ì€ ëª¨ë‘ ì—¬ëŸ¬ ê°’ì„ ì €ì¥í•  ìˆ˜ ìˆëŠ” ì»¬ë ‰ì…˜ ìë£Œí˜•ì´ì§€ë§Œ, ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ë³€ê²½ ê°€ëŠ¥ì„± (Mutab...\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ 2: ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai ëª¨ë¸ë¡œ ì‹œë„\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„±ê³µ - ì‚¬ìš© ëª¨ë¸: openai\n",
      "í´ë°± ì‚¬ìš©ë¨: ì•„ë‹ˆì˜¤\n",
      "ì²˜ë¦¬ ì‹œê°„: 10.65ì´ˆ\n",
      "ì‘ë‹µ: ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì¸ê³µì§€ëŠ¥(AI)ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°ë˜ì§€ ì•Šê³ ë„ ë°ì´í„°ì—ì„œ í•™ìŠµí•˜ê³  ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆ...\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ 3: ì›¹ ê°œë°œì—ì„œ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œì˜ ì—­í• ì€?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai ëª¨ë¸ë¡œ ì‹œë„\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„±ê³µ - ì‚¬ìš© ëª¨ë¸: openai\n",
      "í´ë°± ì‚¬ìš©ë¨: ì•„ë‹ˆì˜¤\n",
      "ì²˜ë¦¬ ì‹œê°„: 9.92ì´ˆ\n",
      "ì‘ë‹µ: ì›¹ ê°œë°œì—ì„œ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œëŠ” ê°ê° ì¤‘ìš”í•œ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë‘ ì˜ì—­ì˜ ì—­í• ì„ ì•„ë˜ì™€ ê°™ì´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### í”„ë¡ íŠ¸ì—”ë“œ (Front-end)\n",
      "í”„ë¡ íŠ¸ì—”ë“œëŠ” ì‚¬ìš©ìê°€...\n",
      "\n",
      "ğŸ“Š Aggressive (5íšŒ, ëœë¤ì§€í„°) í†µê³„:\n",
      "  ì´ í˜¸ì¶œ: 3\n",
      "  ì„±ê³µë¥ : 100.0%\n",
      "  í‰ê·  ì²˜ë¦¬ì‹œê°„: 10.92ì´ˆ\n",
      "  ëª¨ë¸ë³„ í†µê³„: {'openai': {'total': 3, 'success': 3, 'failed': 0}}\n",
      "  Circuit Breaker ìƒíƒœ: {'ollama': 'closed', 'openai': 'closed'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Conservative (2íšŒ, ê³ ì •ì§€ì—°) í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸]\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ 1: Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai ëª¨ë¸ë¡œ ì‹œë„\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„±ê³µ - ì‚¬ìš© ëª¨ë¸: openai\n",
      "í´ë°± ì‚¬ìš©ë¨: ì•„ë‹ˆì˜¤\n",
      "ì²˜ë¦¬ ì‹œê°„: 7.78ì´ˆ\n",
      "ì‘ë‹µ: Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸(list)ì™€ íŠœí”Œ(tuple)ì€ ëª¨ë‘ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆëŠ” ìë£Œí˜•ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ë‘ ìë£Œí˜•ì€ ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. *...\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ 2: ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai ëª¨ë¸ë¡œ ì‹œë„\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„±ê³µ - ì‚¬ìš© ëª¨ë¸: openai\n",
      "í´ë°± ì‚¬ìš©ë¨: ì•„ë‹ˆì˜¤\n",
      "ì²˜ë¦¬ ì‹œê°„: 10.19ì´ˆ\n",
      "ì‘ë‹µ: ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì¸ê³µì§€ëŠ¥(AI)ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ ëª…ì‹œì ì¸ í”„ë¡œê·¸ë˜ë° ì—†ì´ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°...\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ 3: ì›¹ ê°œë°œì—ì„œ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œì˜ ì—­í• ì€?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì„±ê³µ - ì‚¬ìš© ëª¨ë¸: openai\n",
      "í´ë°± ì‚¬ìš©ë¨: ì•„ë‹ˆì˜¤\n",
      "ì²˜ë¦¬ ì‹œê°„: 8.76ì´ˆ\n",
      "ì‘ë‹µ: ì›¹ ê°œë°œì—ì„œ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œëŠ” ê°ê° ì¤‘ìš”í•œ ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. ì•„ë˜ì—ì„œ ë‘ ê°€ì§€ ì—­í• ì„ ê°„ëµí•˜ê²Œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "### í”„ë¡ íŠ¸ì—”ë“œ (Frontend)\n",
      "í”„ë¡ íŠ¸ì—”ë“œëŠ” ì‚¬ìš©ìê°€ ì§...\n",
      "\n",
      "ğŸ“Š Conservative (2íšŒ, ê³ ì •ì§€ì—°) í†µê³„:\n",
      "  ì´ í˜¸ì¶œ: 3\n",
      "  ì„±ê³µë¥ : 100.0%\n",
      "  í‰ê·  ì²˜ë¦¬ì‹œê°„: 8.91ì´ˆ\n",
      "  ëª¨ë¸ë³„ í†µê³„: {'openai': {'total': 3, 'success': 3, 'failed': 0}}\n",
      "  Circuit Breaker ìƒíƒœ: {'ollama': 'closed', 'openai': 'closed'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "âœ… ì¬ì‹œë„/ë°±ì˜¤í”„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ì¬ì‹œë„ í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ì§ˆë¬¸ë“¤\n",
    "retry_test_queries = [\n",
    "    \"Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "    \"ì›¹ ê°œë°œì—ì„œ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œì˜ ì—­í• ì€?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ì¬ì‹œë„/ë°±ì˜¤í”„ ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "clients = {\n",
    "    \"Basic (3íšŒ, ì§€ìˆ˜ë°±ì˜¤í”„)\": robust_client_basic,\n",
    "    \"Aggressive (5íšŒ, ëœë¤ì§€í„°)\": robust_client_aggressive,\n",
    "    \"Conservative (2íšŒ, ê³ ì •ì§€ì—°)\": robust_client_conservative\n",
    "}\n",
    "\n",
    "for client_name, client in clients.items():\n",
    "    print(f\"\\n[{client_name} í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸]\")\n",
    "    \n",
    "    for i, query in enumerate(retry_test_queries, 1):\n",
    "        print(f\"\\ní…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i}: {query}\")\n",
    "        \n",
    "        try:\n",
    "            # í´ë°± ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ í˜¸ì¶œ (OpenAI ìš°ì„ , ì‹¤íŒ¨ì‹œ Ollama)\n",
    "            start_time = time.time()\n",
    "            result = client.fallback_call(query, preferred_model=\"openai\")\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            print(f\"âœ… ì„±ê³µ - ì‚¬ìš© ëª¨ë¸: {result['model_used']}\")\n",
    "            print(f\"í´ë°± ì‚¬ìš©ë¨: {'ì˜ˆ' if result['fallback_used'] else 'ì•„ë‹ˆì˜¤'}\")\n",
    "            print(f\"ì²˜ë¦¬ ì‹œê°„: {duration:.2f}ì´ˆ\")\n",
    "            print(f\"ì‘ë‹µ: {result['response'][:100]}{'...' if len(result['response']) > 100 else ''}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ìµœì¢… ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # í´ë¼ì´ì–¸íŠ¸ë³„ í†µê³„ ì¶œë ¥\n",
    "    stats = client.get_call_statistics()\n",
    "    print(f\"\\nğŸ“Š {client_name} í†µê³„:\")\n",
    "    print(f\"  ì´ í˜¸ì¶œ: {stats['total_calls']}\")\n",
    "    print(f\"  ì„±ê³µë¥ : {stats['success_rate']:.1f}%\")\n",
    "    print(f\"  í‰ê·  ì²˜ë¦¬ì‹œê°„: {stats['average_duration']:.2f}ì´ˆ\")\n",
    "    print(f\"  ëª¨ë¸ë³„ í†µê³„: {stats['model_statistics']}\")\n",
    "    print(f\"  Circuit Breaker ìƒíƒœ: {stats['circuit_breaker_states']}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nâœ… ì¬ì‹œë„/ë°±ì˜¤í”„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Circuit Breaker íŒ¨í„´ ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:openai ëª¨ë¸ë¡œ ì‹œë„\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Circuit Breaker íŒ¨í„´ í…ŒìŠ¤íŠ¸\n",
      "================================================================================\n",
      "1ï¸âƒ£ ì •ìƒ í˜¸ì¶œ í…ŒìŠ¤íŠ¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì •ìƒ í˜¸ì¶œ ì„±ê³µ: openai\n",
      "Circuit Breaker ìƒíƒœ: {'ollama': 'closed', 'openai': 'closed'}\n",
      "í˜„ì¬ ì„±ê³µë¥ : 100.0%\n",
      "\n",
      "ğŸ“Š ìµœì¢… Circuit Breaker í†µê³„:\n",
      "ì´ ì‹œë„: 1\n",
      "ì„±ê³µ: 1\n",
      "ì‹¤íŒ¨: 0\n",
      "ì„±ê³µë¥ : 100.0%\n",
      "OLLAMA Circuit Breaker:\n",
      "  ìƒíƒœ: closed\n",
      "  ì‹¤íŒ¨ íšŸìˆ˜: 0\n",
      "  ë§ˆì§€ë§‰ ì‹¤íŒ¨ ì‹œê°„: None\n",
      "OPENAI Circuit Breaker:\n",
      "  ìƒíƒœ: closed\n",
      "  ì‹¤íŒ¨ íšŸìˆ˜: 0\n",
      "  ë§ˆì§€ë§‰ ì‹¤íŒ¨ ì‹œê°„: None\n",
      "\n",
      "âœ… Circuit Breaker í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Circuit Breaker íŒ¨í„´ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Circuit Breaker í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ íŠ¹ë³„í•œ ì„¤ì •\n",
    "cb_config = CircuitBreakerConfig(\n",
    "    failure_threshold=3,  # 3ë²ˆ ì‹¤íŒ¨í•˜ë©´ Circuit Open\n",
    "    recovery_timeout=10,  # 10ì´ˆ í›„ ë³µêµ¬ ì‹œë„\n",
    "    half_open_max_calls=2  # Half-Open ìƒíƒœì—ì„œ 2ë²ˆê¹Œì§€ ì‹œë„\n",
    ")\n",
    "\n",
    "retry_config = RetryConfig(max_retries=1, base_delay=0.5)  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¬ì‹œë„ ìµœì†Œí™”\n",
    "cb_test_client = RobustLLMClient(retry_config=retry_config, circuit_breaker_config=cb_config)\n",
    "\n",
    "# ì˜ë„ì ìœ¼ë¡œ ì‹¤íŒ¨í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜\n",
    "def simulate_failures():\n",
    "    \"\"\"ì˜ë„ì ìœ¼ë¡œ ì‹¤íŒ¨ ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
    "    test_query = \"Circuit Breaker í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\"\n",
    "    \n",
    "    print(\"1ï¸âƒ£ ì •ìƒ í˜¸ì¶œ í…ŒìŠ¤íŠ¸\")\n",
    "    try:\n",
    "        result = cb_test_client.fallback_call(test_query)\n",
    "        print(f\"âœ… ì •ìƒ í˜¸ì¶œ ì„±ê³µ: {result['model_used']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì²« í˜¸ì¶œë¶€í„° ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # Circuit Breaker ìƒíƒœ í™•ì¸\n",
    "    stats = cb_test_client.get_call_statistics()\n",
    "    print(f\"Circuit Breaker ìƒíƒœ: {stats['circuit_breaker_states']}\")\n",
    "    print(f\"í˜„ì¬ ì„±ê³µë¥ : {stats['success_rate']:.1f}%\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Circuit Breaker ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\n",
    "final_stats = simulate_failures()\n",
    "\n",
    "print(\"\\nğŸ“Š ìµœì¢… Circuit Breaker í†µê³„:\")\n",
    "print(f\"ì´ ì‹œë„: {final_stats['total_calls']}\")\n",
    "print(f\"ì„±ê³µ: {final_stats['successful_calls']}\")\n",
    "print(f\"ì‹¤íŒ¨: {final_stats['failed_calls']}\")\n",
    "print(f\"ì„±ê³µë¥ : {final_stats['success_rate']:.1f}%\")\n",
    "\n",
    "for model, state in final_stats['circuit_breaker_states'].items():\n",
    "    breaker = cb_test_client.circuit_breakers[model]\n",
    "    print(f\"{model.upper()} Circuit Breaker:\")\n",
    "    print(f\"  ìƒíƒœ: {state}\")\n",
    "    print(f\"  ì‹¤íŒ¨ íšŸìˆ˜: {breaker.failure_count}\")\n",
    "    print(f\"  ë§ˆì§€ë§‰ ì‹¤íŒ¨ ì‹œê°„: {breaker.last_failure_time}\")\n",
    "\n",
    "print(\"\\nâœ… Circuit Breaker í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. í†µí•© Production ì‹œìŠ¤í…œ\n",
    "\n",
    "ì•ì„œ êµ¬í˜„í•œ ëª¨ë“  ê¸°ë²•ë“¤ì„ í†µí•©í•œ ì¢…í•©ì ì¸ í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í†µí•© Production ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class ProductionLLMSystem:\n",
    "    def __init__(self):\n",
    "        # ê° ì„œë¸Œì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "        self.sentiment_system = SentimentBranchingSystem()\n",
    "        self.content_filter = ContentFilterSystem()\n",
    "        self.robust_client = RobustLLMClient(\n",
    "            retry_config=RetryConfig(max_retries=3, strategy=RetryStrategy.EXPONENTIAL_BACKOFF),\n",
    "            circuit_breaker_config=CircuitBreakerConfig(failure_threshold=5)\n",
    "        )\n",
    "        \n",
    "        self.processing_history = []\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def process_query(self, user_input: str, use_openai: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"ì¢…í•©ì ì¸ ì¿¼ë¦¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "        start_time = time.time()\n",
    "        processing_id = hashlib.md5(f\"{user_input}{time.time()}\".encode()).hexdigest()[:8]\n",
    "        \n",
    "        result = {\n",
    "            \"processing_id\": processing_id,\n",
    "            \"user_input\": user_input,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"steps\": [],\n",
    "            \"final_response\": None,\n",
    "            \"model_used\": None,\n",
    "            \"processing_time\": 0,\n",
    "            \"safety_passed\": False,\n",
    "            \"sentiment_detected\": None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Step 1: ì½˜í…ì¸  ì•ˆì „ì„± ê²€ì‚¬\n",
    "            self.logger.info(f\"[{processing_id}] Step 1: ì½˜í…ì¸  í•„í„°ë§\")\n",
    "            filter_result = self.content_filter.comprehensive_filter(user_input, use_openai=use_openai)\n",
    "            \n",
    "            result[\"steps\"].append({\n",
    "                \"step\": \"content_filtering\",\n",
    "                \"passed\": filter_result.is_safe,\n",
    "                \"details\": {\n",
    "                    \"severity\": filter_result.severity.value,\n",
    "                    \"categories\": [cat.value for cat in filter_result.categories],\n",
    "                    \"confidence\": filter_result.confidence\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            if not filter_result.is_safe:\n",
    "                # ì•ˆì „í•˜ì§€ ì•Šì€ ì½˜í…ì¸ ì˜ ê²½ìš° ëŒ€ì²´ ì‘ë‹µ ë°˜í™˜\n",
    "                if use_openai:\n",
    "                    safe_response = self.content_filter.safe_response_generator_openai(user_input, filter_result)\n",
    "                else:\n",
    "                    safe_response = self.content_filter.safe_response_generator_ollama(user_input, filter_result)\n",
    "                \n",
    "                result[\"final_response\"] = safe_response\n",
    "                result[\"safety_passed\"] = False\n",
    "                result[\"processing_time\"] = time.time() - start_time\n",
    "                \n",
    "                self.processing_history.append(result)\n",
    "                return result\n",
    "            \n",
    "            result[\"safety_passed\"] = True\n",
    "            \n",
    "            # Step 2: ê°ì„± ë¶„ì„\n",
    "            self.logger.info(f\"[{processing_id}] Step 2: ê°ì„± ë¶„ì„\")\n",
    "            sentiment_result = self.sentiment_system.process_with_sentiment_branching(user_input, use_openai=use_openai)\n",
    "            \n",
    "            result[\"steps\"].append({\n",
    "                \"step\": \"sentiment_analysis\",\n",
    "                \"passed\": True,\n",
    "                \"details\": sentiment_result[\"sentiment_analysis\"]\n",
    "            })\n",
    "            \n",
    "            result[\"sentiment_detected\"] = sentiment_result[\"sentiment_analysis\"][\"sentiment\"]\n",
    "            \n",
    "            # Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
    "            self.logger.info(f\"[{processing_id}] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\")\n",
    "            \n",
    "            # ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì¡°ì •\n",
    "            enhanced_prompt = self.create_enhanced_prompt(user_input, sentiment_result)\n",
    "            \n",
    "            try:\n",
    "                if use_openai:\n",
    "                    response = self.robust_client.robust_openai_call(enhanced_prompt)\n",
    "                    model_used = \"gpt-4o-mini\"\n",
    "                else:\n",
    "                    response = self.robust_client.robust_ollama_call(\"llama3.1:8b\", enhanced_prompt)\n",
    "                    model_used = \"llama3.1:8b\"\n",
    "                \n",
    "                result[\"steps\"].append({\n",
    "                    \"step\": \"response_generation\",\n",
    "                    \"passed\": True,\n",
    "                    \"details\": {\"model\": model_used, \"fallback_used\": False}\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                # í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš©\n",
    "                self.logger.warning(f\"[{processing_id}] ê¸°ë³¸ ëª¨ë¸ ì‹¤íŒ¨, í´ë°± ì‚¬ìš©: {e}\")\n",
    "                fallback_result = self.robust_client.fallback_call(enhanced_prompt, preferred_model=\"openai\" if use_openai else \"ollama\")\n",
    "                response = fallback_result[\"response\"]\n",
    "                model_used = fallback_result[\"model_used\"]\n",
    "                \n",
    "                result[\"steps\"].append({\n",
    "                    \"step\": \"response_generation\",\n",
    "                    \"passed\": True,\n",
    "                    \"details\": {\"model\": model_used, \"fallback_used\": True}\n",
    "                })\n",
    "            \n",
    "            result[\"final_response\"] = response\n",
    "            result[\"model_used\"] = model_used\n",
    "            result[\"processing_time\"] = time.time() - start_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"[{processing_id}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            result[\"final_response\"] = \"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.\"\n",
    "            result[\"error\"] = str(e)\n",
    "            result[\"processing_time\"] = time.time() - start_time\n",
    "        \n",
    "        self.processing_history.append(result)\n",
    "        return result\n",
    "    \n",
    "    def create_enhanced_prompt(self, user_input: str, sentiment_result: Dict[str, Any]) -> str:\n",
    "        \"\"\"ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
    "        sentiment = sentiment_result[\"sentiment_analysis\"][\"sentiment\"]\n",
    "        strategy = sentiment_result[\"response_strategy\"]\n",
    "        \n",
    "        enhanced_prompt = f\"\"\"\n",
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì— ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n",
    "\n",
    "ì‚¬ìš©ì ì§ˆë¬¸: \"{user_input}\"\n",
    "\n",
    "ê°ì§€ëœ ê°ì •: {sentiment}\n",
    "ì‘ë‹µ í†¤: {strategy['tone']}\n",
    "ì ‘ê·¼ ë°©ì‹: {strategy['approach']}\n",
    "ìŠ¤íƒ€ì¼: {strategy['style']}\n",
    "\n",
    "ìœ„ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¥´ë©´ì„œ ë„ì›€ì´ ë˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "        \n",
    "        return enhanced_prompt\n",
    "    \n",
    "    def get_system_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"ì‹œìŠ¤í…œ ì „ì²´ í†µê³„\"\"\"\n",
    "        total_queries = len(self.processing_history)\n",
    "        if total_queries == 0:\n",
    "            return {\"message\": \"No queries processed yet\"}\n",
    "        \n",
    "        safety_passed = len([h for h in self.processing_history if h[\"safety_passed\"]])\n",
    "        avg_processing_time = sum(h[\"processing_time\"] for h in self.processing_history) / total_queries\n",
    "        \n",
    "        sentiment_distribution = {}\n",
    "        for history in self.processing_history:\n",
    "            sentiment = history.get(\"sentiment_detected\")\n",
    "            if sentiment:\n",
    "                sentiment_distribution[sentiment] = sentiment_distribution.get(sentiment, 0) + 1\n",
    "        \n",
    "        model_usage = {}\n",
    "        for history in self.processing_history:\n",
    "            model = history.get(\"model_used\")\n",
    "            if model:\n",
    "                model_usage[model] = model_usage.get(model, 0) + 1\n",
    "        \n",
    "        robust_client_stats = self.robust_client.get_call_statistics()\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": total_queries,\n",
    "            \"safety_pass_rate\": (safety_passed / total_queries * 100) if total_queries > 0 else 0,\n",
    "            \"average_processing_time\": avg_processing_time,\n",
    "            \"sentiment_distribution\": sentiment_distribution,\n",
    "            \"model_usage\": model_usage,\n",
    "            \"robust_client_stats\": robust_client_stats\n",
    "        }\n",
    "\n",
    "# í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "production_system = ProductionLLMSystem()\n",
    "print(\"âœ… í†µí•© Production ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 í†µí•© ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[a975073f] Step 1: ì½˜í…ì¸  í•„í„°ë§\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Production ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸\n",
      "================================================================================\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1]\n",
      "ì…ë ¥: íŒŒì´ì¬ ì›¹ ê°œë°œì„ ì‹œì‘í•˜ë ¤ê³  í•˜ëŠ”ë° ì–´ë–¤ í”„ë ˆì„ì›Œí¬ë¥¼ ì¶”ì²œí•˜ì‹œë‚˜ìš”?\n",
      "\n",
      "ğŸ§  OpenAI ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "INFO:__main__:[a975073f] Step 2: ê°ì„± ë¶„ì„\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[a975073f] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[600d21f2] Step 1: ì½˜í…ì¸  í•„í„°ë§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: a975073f\n",
      "ì•ˆì „ì„± í†µê³¼: âœ…\n",
      "ê°ì§€ëœ ê°ì •: neutral\n",
      "ì‚¬ìš© ëª¨ë¸: gpt-4o-mini\n",
      "ì²˜ë¦¬ ì‹œê°„: 18.75ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âœ… content_filtering: {'severity': 'low', 'categories': [], 'confidence': 0.5}\n",
      "  âœ… sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  âœ… response_generation: {'model': 'gpt-4o-mini', 'fallback_used': False}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: íŒŒì´ì¬ ì›¹ ê°œë°œì„ ì‹œì‘í•˜ë ¤ëŠ” ê²ƒì€ í›Œë¥­í•œ ì„ íƒì…ë‹ˆë‹¤! íŒŒì´ì¬ì€ ë‹¤ì–‘í•œ ì›¹ í”„ë ˆì„ì›Œí¬ë¥¼ ì§€ì›í•˜ì—¬ ê°œë°œìë“¤ì´ í•„ìš”ì— ë”°ë¼ ì í•©í•œ ë„êµ¬ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ì¸ê¸° ìˆëŠ” ëª‡ ê°€ì§€ í”„ë ˆì„ì›Œí¬ì™€ ê·¸ íŠ¹ì§•ì„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
      "\n",
      "1. **Django**:\n",
      "   - **íŠ¹ì§•**: DjangoëŠ” ê°•ë ¥í•˜ê³  ê³ ê¸‰ ê¸°ëŠ¥ì„ ê°–ì¶˜ í”„ë ˆì„ì›Œí¬ë¡œ, \"ë°°í„°ë¦¬ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤\"ëŠ” ...\n",
      "\n",
      "ğŸ¤– Ollama ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[600d21f2] Step 2: ê°ì„± ë¶„ì„\n",
      "INFO:__main__:[600d21f2] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
      "INFO:__main__:Ollama í˜¸ì¶œ ì‹œë„ 1/4\n",
      "INFO:__main__:[211bef66] Step 1: ì½˜í…ì¸  í•„í„°ë§\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: 600d21f2\n",
      "ì•ˆì „ì„± í†µê³¼: âœ…\n",
      "ê°ì§€ëœ ê°ì •: neutral\n",
      "ì‚¬ìš© ëª¨ë¸: llama3.1:8b\n",
      "ì²˜ë¦¬ ì‹œê°„: 40.94ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âœ… content_filtering: {'severity': 'low', 'categories': [], 'confidence': 1.0}\n",
      "  âœ… sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.85, 'emotions': {'joy': 0.6, 'fear': 0.2, 'surprise': 0.1, 'anger': 0.05, 'sadness': 0.05}, 'keywords': ['íŒŒì´ì¬ ì›¹ ê°œë°œ', 'í”„ë ˆì„ì›Œí¬']}\n",
      "  âœ… response_generation: {'model': 'llama3.1:8b', 'fallback_used': False}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: íŒŒì´ì¬ ì›¹ ê°œë°œì˜ ì„¸ê³„ì—ì„œ ë‹¤ì–‘í•œ í”„ë ˆì„ì›Œí¬ê°€ ìˆìŠµë‹ˆë‹¤. ê° í”„ë ˆì„ì›Œí¬ì˜ ê°•ë ¥í•œ ì ê³¼ ì•½ê°„ì˜ ë‹¨ì ì— ëŒ€í•´ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **Flask**:\n",
      "   - ì¥ì : ê°€ë³ê³  ìœ ì—°í•©ë‹ˆë‹¤. \n",
      "   - ë‹¨ì : ë³µì¡í•œ í”„ë¡œì íŠ¸ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "   - ì¶”ì²œ: ì‘ì€ í¬ê¸°ì˜ ì›¹ ì•±ì´ë‚˜ API ê°œë°œì„ ìœ„í•´ idealí•©ë‹ˆë‹¤.\n",
      "\n",
      "2. **Django**:\n",
      "   - ì¥...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2]\n",
      "ì…ë ¥: ì´ ë°”ë³´ ê°™ì€ ì—ëŸ¬ê°€ ê³„ì† ë‚˜ëŠ”ë° ì–´ë–»ê²Œ í•´ê²°í•´ì•¼ í•˜ë‚˜ìš”?\n",
      "\n",
      "ğŸ§  OpenAI ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[c9cc1de9] Step 1: ì½˜í…ì¸  í•„í„°ë§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: 211bef66\n",
      "ì•ˆì „ì„± í†µê³¼: âŒ\n",
      "ê°ì§€ëœ ê°ì •: None\n",
      "ì‚¬ìš© ëª¨ë¸: None\n",
      "ì²˜ë¦¬ ì‹œê°„: 6.02ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âŒ content_filtering: {'severity': 'medium', 'categories': ['profanity'], 'confidence': 0.5}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: Error: Maximum retries exceeded\n",
      "\n",
      "ğŸ¤– Ollama ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[a4fcd720] Step 1: ì½˜í…ì¸  í•„í„°ë§\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: c9cc1de9\n",
      "ì•ˆì „ì„± í†µê³¼: âŒ\n",
      "ê°ì§€ëœ ê°ì •: None\n",
      "ì‚¬ìš© ëª¨ë¸: None\n",
      "ì²˜ë¦¬ ì‹œê°„: 10.37ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âŒ content_filtering: {'severity': 'medium', 'categories': ['profanity'], 'confidence': 0.95}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ë¶€ì ì ˆí•œ ì–¸ì–´ë¥¼ í”¼í•˜ê¸° ìœ„í•´, \"ì—ëŸ¬ê°€ ê³„ì† ë°œìƒí•œë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?\"ì™€ ê°™ì€ ëŒ€ì•ˆì ì¸ ì§ˆë¬¸ì„ ì œì‹œí•˜ëŠ” ê²ƒì—ì„œë¶€í„° ì‹œì‘í•´, ì—ëŸ¬ í•´ê²°ì„ ìœ„í•œ ì¼ë°˜ì ì¸ ë°©ë²•ë“¤ì„ ì†Œê°œí•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ëŒ€ì²´ ì‘ë‹µì€ ì‚¬ìš©ìì˜ ì˜ë„ëœ ëª©ì ì¸ ì—ëŸ¬ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë©°, ë™ì‹œì— ì•ˆì „í•˜ê³  ì „ë¬¸ì ì¸ í™˜ê²½ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ë…¸ë ¥ì„ ê°•ì¡°í•©ë‹ˆ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3]\n",
      "ì…ë ¥: ë¨¸ì‹ ëŸ¬ë‹ ê³µë¶€ê°€ ë„ˆë¬´ ì–´ë ¤ì›Œì„œ í¬ê¸°í•˜ê³  ì‹¶ì–´ìš”...\n",
      "\n",
      "ğŸ§  OpenAI ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "INFO:__main__:[a4fcd720] Step 2: ê°ì„± ë¶„ì„\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[a4fcd720] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[6eb991b9] Step 1: ì½˜í…ì¸  í•„í„°ë§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: a4fcd720\n",
      "ì•ˆì „ì„± í†µê³¼: âœ…\n",
      "ê°ì§€ëœ ê°ì •: neutral\n",
      "ì‚¬ìš© ëª¨ë¸: gpt-4o-mini\n",
      "ì²˜ë¦¬ ì‹œê°„: 16.36ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âœ… content_filtering: {'severity': 'low', 'categories': [], 'confidence': 0.5}\n",
      "  âœ… sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  âœ… response_generation: {'model': 'gpt-4o-mini', 'fallback_used': False}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ë¨¸ì‹ ëŸ¬ë‹ ê³µë¶€ê°€ ì–´ë µê²Œ ëŠê»´ì§€ëŠ” ê²ƒì€ ë§¤ìš° ì¼ë°˜ì ì¸ ê²½í—˜ì…ë‹ˆë‹¤. ì´ ë¶„ì•¼ëŠ” ìˆ˜í•™, í†µê³„, í”„ë¡œê·¸ë˜ë° ë“± ë‹¤ì–‘í•œ ì§€ì‹ì´ ìš”êµ¬ë˜ê¸° ë•Œë¬¸ì— ì²˜ìŒ ì ‘í•  ë•ŒëŠ” íŠ¹íˆ í˜ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëª‡ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ì´ ê³¼ì •ì„ ë” ìˆ˜ì›”í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ê¸°ì´ˆ ë‹¤ì§€ê¸°**: ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì„ í˜• ëŒ€ìˆ˜, ë¯¸ì ë¶„í•™, í™•ë¥  ...\n",
      "\n",
      "ğŸ¤– Ollama ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[6eb991b9] Step 2: ê°ì„± ë¶„ì„\n",
      "INFO:__main__:[6eb991b9] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
      "INFO:__main__:Ollama í˜¸ì¶œ ì‹œë„ 1/4\n",
      "INFO:__main__:[2956ea5e] Step 1: ì½˜í…ì¸  í•„í„°ë§\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: 6eb991b9\n",
      "ì•ˆì „ì„± í†µê³¼: âœ…\n",
      "ê°ì§€ëœ ê°ì •: sad\n",
      "ì‚¬ìš© ëª¨ë¸: llama3.1:8b\n",
      "ì²˜ë¦¬ ì‹œê°„: 29.16ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âœ… content_filtering: {'severity': 'low', 'categories': [], 'confidence': 1.0}\n",
      "  âœ… sentiment_analysis: {'sentiment': 'sad', 'confidence': 0.95, 'emotions': {'joy': 0.2, 'anger': 0.5, 'sadness': 0.3, 'fear': 0.0, 'surprise': 0.0}, 'keywords': ['ê³µë¶€', 'í¬ê¸°']}\n",
      "  âœ… response_generation: {'model': 'llama3.1:8b', 'fallback_used': False}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ë¨¸ì‹ ëŸ¬ë‹ ê³µë¶€ê°€ ë„ˆë¬´ ì–´ë ¤ì›Œì„œ í¬ê¸°í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´, ë¨¼ì € ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”. ë§ì€ ì‚¬ëŒë“¤ì€ ì²˜ìŒ ì‹œì‘í•  ë•Œ ë‹¹í™©í•˜ê±°ë‚˜ ë§‰ë§‰í•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë„ì „ì„ ë©ˆì¶”ì§€ ì•Šê³  ê³„ì† í•´ë‚˜ê°€ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "\n",
      "ë¨¼ì €, ë¨¸ì‹ ëŸ¬ë‹ì€ ì–´ë ¤ìš¸ ìˆ˜ë„ ìˆì§€ë§Œ, ì¼ì¢…ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ëŠ” ê³¼ì •ì´ë¼ëŠ” ê²ƒì„ ê¸°ì–µí•´ ì£¼ì„¸ìš”. ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ì•Šìœ¼ë©´ ë” ì—´ì‹¬íˆ ë…¸ë ¥í•˜ëŠ” ê²ƒì²˜ëŸ¼ìš”!\n",
      "...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4]\n",
      "ì…ë ¥: ì™€! ì²« ë²ˆì§¸ ì•±ì„ ì„±ê³µì ìœ¼ë¡œ ë°°í¬í–ˆì–´ìš”! ë‹¤ìŒ ë‹¨ê³„ëŠ” ë­”ê°€ìš”?\n",
      "\n",
      "ğŸ§  OpenAI ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "INFO:__main__:[2956ea5e] Step 2: ê°ì„± ë¶„ì„\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[2956ea5e] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[dc03772a] Step 1: ì½˜í…ì¸  í•„í„°ë§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: 2956ea5e\n",
      "ì•ˆì „ì„± í†µê³¼: âœ…\n",
      "ê°ì§€ëœ ê°ì •: neutral\n",
      "ì‚¬ìš© ëª¨ë¸: gpt-4o-mini\n",
      "ì²˜ë¦¬ ì‹œê°„: 14.56ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âœ… content_filtering: {'severity': 'low', 'categories': [], 'confidence': 0.5}\n",
      "  âœ… sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  âœ… response_generation: {'model': 'gpt-4o-mini', 'fallback_used': False}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ì¶•í•˜í•©ë‹ˆë‹¤! ì²« ë²ˆì§¸ ì•±ì„ ì„±ê³µì ìœ¼ë¡œ ë°°í¬í•œ ê²ƒì€ ì •ë§ í° ì„±ê³¼ì…ë‹ˆë‹¤. ì´ì œ ë‹¤ìŒ ë‹¨ê³„ëŠ” ì•±ì˜ ì§€ì†ì ì¸ ë°œì „ê³¼ ì„±ê³µì ì¸ ìš´ì˜ì„ ìœ„í•´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë“¤ì„ ê³ ë ¤í•´ ë³´ì„¸ìš”:\n",
      "\n",
      "1. **ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘**: ì‚¬ìš©ì ë¦¬ë·°ì™€ í”¼ë“œë°±ì„ í†µí•´ ì•±ì˜ ê°•ì ê³¼ ê°œì„ í•  ì ì„ íŒŒì•…í•˜ì„¸ìš”. ì´ë¥¼ í†µí•´ ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ë²„ê·¸ ìˆ˜ì •...\n",
      "\n",
      "ğŸ¤– Ollama ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[dc03772a] Step 2: ê°ì„± ë¶„ì„\n",
      "INFO:__main__:[dc03772a] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
      "INFO:__main__:Ollama í˜¸ì¶œ ì‹œë„ 1/4\n",
      "INFO:__main__:[1694a818] Step 1: ì½˜í…ì¸  í•„í„°ë§\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: dc03772a\n",
      "ì•ˆì „ì„± í†µê³¼: âœ…\n",
      "ê°ì§€ëœ ê°ì •: positive\n",
      "ì‚¬ìš© ëª¨ë¸: llama3.1:8b\n",
      "ì²˜ë¦¬ ì‹œê°„: 23.34ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âœ… content_filtering: {'severity': 'low', 'categories': [], 'confidence': 1.0}\n",
      "  âœ… sentiment_analysis: {'sentiment': 'positive', 'confidence': 0.98, 'emotions': {'joy': 0.95, 'surprise': 0.03}, 'keywords': ['ì„±ê³µ', 'ë°°í¬']}\n",
      "  âœ… response_generation: {'model': 'llama3.1:8b', 'fallback_used': False}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ì¶•í•˜í•©ë‹ˆë‹¤! ì²« ë²ˆì§¸ ì•±ì„ ì„±ê³µì ìœ¼ë¡œ ë°°í¬í–ˆëŠ” ê±´ ëŒ€ë‹¨í•œ ì„±ì·¨ì…ë‹ˆë‹¤!\n",
      "\n",
      "ì´ì œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ë³´ê² ìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ì•±ì˜ ì„±ê³µì„ ì§€ì†ì‹œí‚¬ ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1.  **ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘**: ì‚¬ìš©ìì˜ ì˜ê²¬ê³¼ í›„ê¸°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì•±ì„ ê°œì„ í•˜ê³  ì—…ë°ì´íŠ¸ í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
      "2.  **ì•± ë¶„ì„**: ì•±ì˜ ì„±ëŠ¥ê³¼ ì‚¬ìš©ì ê²½í—˜ì„ ë¶„ì„í•˜ì—¬ ë” ì¢‹ì€ ê¸°ëŠ¥ê³¼ ì½˜í…ì¸ ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5]\n",
      "ì…ë ¥: ì”¨ë°œ ì´ ì½”ë“œ ì§„ì§œ ê°œë˜¥ê°™ë„¤, ëˆ„ê°€ ì´ë”´ ê±¸ ë§Œë“¤ì—ˆì–´?\n",
      "\n",
      "ğŸ§  OpenAI ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[a887e6c6] Step 1: ì½˜í…ì¸  í•„í„°ë§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: 1694a818\n",
      "ì•ˆì „ì„± í†µê³¼: âŒ\n",
      "ê°ì§€ëœ ê°ì •: None\n",
      "ì‚¬ìš© ëª¨ë¸: None\n",
      "ì²˜ë¦¬ ì‹œê°„: 6.03ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âŒ content_filtering: {'severity': 'medium', 'categories': ['profanity'], 'confidence': 0.5}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: Error: Maximum retries exceeded\n",
      "\n",
      "ğŸ¤– Ollama ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[e80d0999] Step 1: ì½˜í…ì¸  í•„í„°ë§\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: a887e6c6\n",
      "ì•ˆì „ì„± í†µê³¼: âŒ\n",
      "ê°ì§€ëœ ê°ì •: None\n",
      "ì‚¬ìš© ëª¨ë¸: None\n",
      "ì²˜ë¦¬ ì‹œê°„: 7.69ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âŒ content_filtering: {'severity': 'high', 'categories': ['profanity'], 'confidence': 1.0}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ëŒ€ì•ˆ:\n",
      "\"ì´ ì½”ë“œëŠ” í™•ì‹¤íˆ ê°œì„ í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ëˆ„ê°€ ì‘ì„±í–ˆë“  ìƒê´€ì—†ì´, ìš°ë¦¬ ëª¨ë‘ ì´ë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ì´ê³  ëª…í™•í•˜ê²Œ ë§Œë“œëŠ” ë° ì°¸ì—¬í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 6]\n",
      "ì…ë ¥: ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ ìµœì í™” ë°©ë²•ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\n",
      "\n",
      "ğŸ§  OpenAI ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "INFO:__main__:[e80d0999] Step 2: ê°ì„± ë¶„ì„\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[e80d0999] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[2736e3e6] Step 1: ì½˜í…ì¸  í•„í„°ë§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: e80d0999\n",
      "ì•ˆì „ì„± í†µê³¼: âœ…\n",
      "ê°ì§€ëœ ê°ì •: neutral\n",
      "ì‚¬ìš© ëª¨ë¸: gpt-4o-mini\n",
      "ì²˜ë¦¬ ì‹œê°„: 20.75ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âœ… content_filtering: {'severity': 'low', 'categories': [], 'confidence': 0.5}\n",
      "  âœ… sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  âœ… response_generation: {'model': 'gpt-4o-mini', 'fallback_used': False}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìœ¼ë©°, ê° ë°©ë²•ì€ ë¬¸ì œì˜ íŠ¹ì„±ê³¼ ë°ì´í„°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì— ëª‡ ê°€ì§€ ì£¼ìš” ë°©ë²•ì„ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "### 1. ë°ì´í„° ì „ì²˜ë¦¬\n",
      "- **ì •ê·œí™” ë° í‘œì¤€í™”**: ì…ë ¥ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ê±°ë‚˜ í‘œì¤€í™”í•˜ì—¬ ëª¨ë¸ì´ ë” ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
      "- **ë°ì´í„° ì¦ê°•**: ì´ë¯¸ì§€ë‚˜...\n",
      "\n",
      "ğŸ¤– Ollama ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[2736e3e6] Step 2: ê°ì„± ë¶„ì„\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•œ ê²°ê³¼ ë‹¤ìŒê³¼ ê°™ì´ ìš”ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "- ê°ì • ìƒíƒœ: neutral (ì¤‘ë¦½ì )\n",
      "- ì‹ ë¢°ë„: 1.0\n",
      "- ê°ì„± ë¶„ì„:\n",
      "  - ê¸°ì¨: 0.0\n",
      "  - ë¶„ë…¸: 0.0\n",
      "  - ìŠ¬í””: 0.0\n",
      "\n",
      "í‚¤ì›Œë“œëŠ” 'ë”¥ëŸ¬ë‹', 'ëª¨ë¸ì˜ ì„±ëŠ¥ ìµœì í™” ë°©ë²•'ì…ë‹ˆë‹¤. \n",
      "\n",
      "ì´ í…ìŠ¤íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¤‘ë¦½ì ì¸ ì„±ê²©ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, ê°ì •ìƒíƒœë¥¼ neutralë¡œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.\n",
      "INFO:__main__:[2736e3e6] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
      "INFO:__main__:Ollama í˜¸ì¶œ ì‹œë„ 1/4\n",
      "INFO:__main__:[51a71f36] Step 1: ì½˜í…ì¸  í•„í„°ë§\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: 2736e3e6\n",
      "ì•ˆì „ì„± í†µê³¼: âœ…\n",
      "ê°ì§€ëœ ê°ì •: neutral\n",
      "ì‚¬ìš© ëª¨ë¸: llama3.1:8b\n",
      "ì²˜ë¦¬ ì‹œê°„: 42.21ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âœ… content_filtering: {'severity': 'low', 'categories': [], 'confidence': 1.0}\n",
      "  âœ… sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  âœ… response_generation: {'model': 'llama3.1:8b', 'fallback_used': False}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ ìµœì í™”ëŠ” ëª¨ë¸ì´ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•˜ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ê³¼ì •ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ë“¤ì„ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¡°ì •**: ëª¨ë¸ì˜ êµ¬ì¡°ì™€ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë ˆì´ì–´ì˜ ìˆ˜ë„ ëŠ˜ë ¤ì„œ ë” ë§ì€ ì •ë³´ë¥¼ í•™ìŠµí•˜ê±°ë‚˜, í™œì„±í™” í•¨ìˆ˜ë¥¼ ë°”ê¾¸ì–´ í•™ìŠµ ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 7]\n",
      "ì…ë ¥: í•´í‚¹ íˆ´ ë§Œë“œëŠ” ë°©ë²• ê°€ë¥´ì³ì¤˜\n",
      "\n",
      "ğŸ§  OpenAI ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[f0c4c2a1] Step 1: ì½˜í…ì¸  í•„í„°ë§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: 51a71f36\n",
      "ì•ˆì „ì„± í†µê³¼: âŒ\n",
      "ê°ì§€ëœ ê°ì •: None\n",
      "ì‚¬ìš© ëª¨ë¸: None\n",
      "ì²˜ë¦¬ ì‹œê°„: 6.02ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âŒ content_filtering: {'severity': 'critical', 'categories': ['illegal'], 'confidence': 0.5}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: Error: Maximum retries exceeded\n",
      "\n",
      "ğŸ¤– Ollama ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[c1e5380f] Step 1: ì½˜í…ì¸  í•„í„°ë§\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: f0c4c2a1\n",
      "ì•ˆì „ì„± í†µê³¼: âŒ\n",
      "ê°ì§€ëœ ê°ì •: None\n",
      "ì‚¬ìš© ëª¨ë¸: None\n",
      "ì²˜ë¦¬ ì‹œê°„: 14.78ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âŒ content_filtering: {'severity': 'critical', 'categories': ['illegal'], 'confidence': 1.0}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ì‚¬ìš©ìëŠ” \"í•´í‚¹ íˆ´ ë§Œë“œëŠ” ë°©ë²•\"ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤. ì´ ì§ˆë¬¸ì€ í•´í‚¹ì´ë¼ëŠ” ë²”ì£„ì  í™œë™ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ì•ˆì „í•˜ê³  ë²•ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n",
      "\n",
      "ëŒ€ì•ˆ:\n",
      "- ì¹´í…Œê³ ë¦¬: legal\n",
      "- ë¬¸ì œ ìš©ì–´: ë³´ì•ˆ ê°•í™”\n",
      "\n",
      "ëŒ€ì²´ ì‘ë‹µ:\n",
      "\n",
      "*   **ë³´ì•ˆ ê°•í™” ë°©ë²•ì— ëŒ€í•´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤**. ë³´ì•ˆì€ ì»´í“¨í„°ë‚˜ ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•œ ë°©ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 8]\n",
      "ì…ë ¥: ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„í•  ë•Œ ì •ê·œí™”ëŠ” ì™œ í•„ìš”í•œê°€ìš”?\n",
      "\n",
      "ğŸ§  OpenAI ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "INFO:__main__:[c1e5380f] Step 2: ê°ì„± ë¶„ì„\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON íŒŒì‹± ì‹¤íŒ¨: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[c1e5380f] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
      "INFO:__main__:OpenAI í˜¸ì¶œ ì‹œë„ 1/4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[ac2aaa67] Step 1: ì½˜í…ì¸  í•„í„°ë§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: c1e5380f\n",
      "ì•ˆì „ì„± í†µê³¼: âœ…\n",
      "ê°ì§€ëœ ê°ì •: neutral\n",
      "ì‚¬ìš© ëª¨ë¸: gpt-4o-mini\n",
      "ì²˜ë¦¬ ì‹œê°„: 16.75ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âœ… content_filtering: {'severity': 'low', 'categories': [], 'confidence': 0.5}\n",
      "  âœ… sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  âœ… response_generation: {'model': 'gpt-4o-mini', 'fallback_used': False}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ì—ì„œ ì •ê·œí™”ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ê³¼ì •ìœ¼ë¡œ, ì—¬ëŸ¬ ê°€ì§€ ì´ìœ ë¡œ í•„ìš”í•©ë‹ˆë‹¤. ì •ê·œí™”ëŠ” ë°ì´í„° ì¤‘ë³µì„ ì¤„ì´ê³ , ë°ì´í„° ë¬´ê²°ì„±ì„ ë†’ì´ë©°, ì—…ë°ì´íŠ¸ anomaliesë¥¼ ë°©ì§€í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ì •ê·œí™”ì˜ ì£¼ìš” í•„ìš”ì„±ê³¼ ì´ì ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "\n",
      "1. **ë°ì´í„° ì¤‘ë³µ ìµœì†Œí™”**: ì •ê·œí™”ë¥¼ í†µí•´ ë°ì´í„°ë² ì´ìŠ¤ ë‚´ì—ì„œ ë™ì¼í•œ ë°ì´í„°ê°€ ì—¬ëŸ¬ ë²ˆ ì €ì¥ë˜ëŠ” ê²ƒì„...\n",
      "\n",
      "ğŸ¤– Ollama ì²˜ë¦¬ ê²°ê³¼:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[ac2aaa67] Step 2: ê°ì„± ë¶„ì„\n",
      "INFO:__main__:[ac2aaa67] Step 3: ê²¬ê³ í•œ ì‘ë‹µ ìƒì„±\n",
      "INFO:__main__:Ollama í˜¸ì¶œ ì‹œë„ 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ ID: ac2aaa67\n",
      "ì•ˆì „ì„± í†µê³¼: âœ…\n",
      "ê°ì§€ëœ ê°ì •: neutral\n",
      "ì‚¬ìš© ëª¨ë¸: llama3.1:8b\n",
      "ì²˜ë¦¬ ì‹œê°„: 30.50ì´ˆ\n",
      "\n",
      "ì²˜ë¦¬ ë‹¨ê³„:\n",
      "  âœ… content_filtering: {'severity': 'low', 'categories': [], 'confidence': 1.0}\n",
      "  âœ… sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.95, 'emotions': {'joy': 0.4, 'anger': 0.2, 'sadness': 0.2, 'fear': 0.1, 'surprise': 0.1}, 'keywords': ['ì •ê·œí™”', 'ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„']}\n",
      "  âœ… response_generation: {'model': 'llama3.1:8b', 'fallback_used': False}\n",
      "\n",
      "ìµœì¢… ì‘ë‹µ: ì •ê·œí™”ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ì˜ ì¤‘ìš”í•œ ì¸¡ë©´ ì¤‘ í•˜ë‚˜ë¡œ, ë°ì´í„°ê°€ ì ì ˆí•˜ê²Œ ì €ì¥ë˜ê³  ê´€ë¦¬ë  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. ì •ê·œí™”ë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì´ ë°œìƒí•©ë‹ˆë‹¤.\n",
      "\n",
      "1.  **ë°ì´í„° ì¤‘ë³µ ìµœì†Œí™”**: ë™ì¼í•œ ì •ë³´ê°€ ì—¬ëŸ¬ë²ˆ ì €ì¥ë˜ëŠ” ê²ƒì„ í”¼í•˜ì—¬ ë°ì´í„°ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "2.  **ë°ì´í„° ì¢…ì†ì„± êµ¬ë¶„**: ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë„...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“Š Production ì‹œìŠ¤í…œ ì „ì²´ í†µê³„\n",
      "ì´ ì²˜ë¦¬ëœ ì¿¼ë¦¬: 16\n",
      "ì•ˆì „ì„± í†µê³¼ìœ¨: 62.5%\n",
      "í‰ê·  ì²˜ë¦¬ ì‹œê°„: 19.01ì´ˆ\n",
      "\n",
      "ê°ì • ë¶„í¬:\n",
      "  neutral: 8íšŒ (50.0%)\n",
      "  sad: 1íšŒ (6.2%)\n",
      "  positive: 1íšŒ (6.2%)\n",
      "\n",
      "ëª¨ë¸ ì‚¬ìš© ë¶„í¬:\n",
      "  gpt-4o-mini: 5íšŒ (31.2%)\n",
      "  llama3.1:8b: 5íšŒ (31.2%)\n",
      "\n",
      "ê²¬ê³ í•œ í´ë¼ì´ì–¸íŠ¸ í†µê³„:\n",
      "  ì´ í˜¸ì¶œ: 10\n",
      "  ì„±ê³µë¥ : 100.0%\n",
      "  í‰ê·  í˜¸ì¶œ ì‹œê°„: 11.26ì´ˆ\n",
      "  Circuit Breaker ìƒíƒœ: {'ollama': 'closed', 'openai': 'closed'}\n",
      "\n",
      "âœ… Production ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì˜ í†µí•© í…ŒìŠ¤íŠ¸\n",
    "comprehensive_test_cases = [\n",
    "    \"íŒŒì´ì¬ ì›¹ ê°œë°œì„ ì‹œì‘í•˜ë ¤ê³  í•˜ëŠ”ë° ì–´ë–¤ í”„ë ˆì„ì›Œí¬ë¥¼ ì¶”ì²œí•˜ì‹œë‚˜ìš”?\",  # ê¸ì •ì  + ì•ˆì „\n",
    "    \"ì´ ë°”ë³´ ê°™ì€ ì—ëŸ¬ê°€ ê³„ì† ë‚˜ëŠ”ë° ì–´ë–»ê²Œ í•´ê²°í•´ì•¼ í•˜ë‚˜ìš”?\",  # ë¶€ì •ì  + ê²½ë¯¸í•œ ìš•ì„¤\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ ê³µë¶€ê°€ ë„ˆë¬´ ì–´ë ¤ì›Œì„œ í¬ê¸°í•˜ê³  ì‹¶ì–´ìš”...\",  # ìŠ¬í”” + ì•ˆì „\n",
    "    \"ì™€! ì²« ë²ˆì§¸ ì•±ì„ ì„±ê³µì ìœ¼ë¡œ ë°°í¬í–ˆì–´ìš”! ë‹¤ìŒ ë‹¨ê³„ëŠ” ë­”ê°€ìš”?\",  # ê¸°ì¨ + ì•ˆì „\n",
    "    \"ì”¨ë°œ ì´ ì½”ë“œ ì§„ì§œ ê°œë˜¥ê°™ë„¤, ëˆ„ê°€ ì´ë”´ ê±¸ ë§Œë“¤ì—ˆì–´?\",  # ë¶„ë…¸ + ì‹¬í•œ ìš•ì„¤\n",
    "    \"ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ ìµœì í™” ë°©ë²•ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\",  # ì¤‘ë¦½ + ì•ˆì „\n",
    "    \"í•´í‚¹ íˆ´ ë§Œë“œëŠ” ë°©ë²• ê°€ë¥´ì³ì¤˜\",  # ë¶ˆë²•ì  ë‚´ìš©\n",
    "    \"ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„í•  ë•Œ ì •ê·œí™”ëŠ” ì™œ í•„ìš”í•œê°€ìš”?\",  # ì¤‘ë¦½ + ì•ˆì „\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Production ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, test_case in enumerate(comprehensive_test_cases, 1):\n",
    "    print(f\"\\n[í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}]\")\n",
    "    print(f\"ì…ë ¥: {test_case}\")\n",
    "    \n",
    "    # OpenAI ìš°ì„  ì²˜ë¦¬\n",
    "    if openai_available:\n",
    "        print(\"\\nğŸ§  OpenAI ì²˜ë¦¬ ê²°ê³¼:\")\n",
    "        openai_result = production_system.process_query(test_case, use_openai=True)\n",
    "        \n",
    "        print(f\"ì²˜ë¦¬ ID: {openai_result['processing_id']}\")\n",
    "        print(f\"ì•ˆì „ì„± í†µê³¼: {'âœ…' if openai_result['safety_passed'] else 'âŒ'}\")\n",
    "        print(f\"ê°ì§€ëœ ê°ì •: {openai_result.get('sentiment_detected', 'N/A')}\")\n",
    "        print(f\"ì‚¬ìš© ëª¨ë¸: {openai_result.get('model_used', 'N/A')}\")\n",
    "        print(f\"ì²˜ë¦¬ ì‹œê°„: {openai_result['processing_time']:.2f}ì´ˆ\")\n",
    "        \n",
    "        print(\"\\nì²˜ë¦¬ ë‹¨ê³„:\")\n",
    "        for step in openai_result[\"steps\"]:\n",
    "            status = \"âœ…\" if step[\"passed\"] else \"âŒ\"\n",
    "            print(f\"  {status} {step['step']}: {step.get('details', {})}\")\n",
    "        \n",
    "        print(f\"\\nìµœì¢… ì‘ë‹µ: {openai_result['final_response'][:200]}{'...' if len(openai_result['final_response']) > 200 else ''}\")\n",
    "    \n",
    "    # Ollama ì²˜ë¦¬\n",
    "    print(\"\\nğŸ¤– Ollama ì²˜ë¦¬ ê²°ê³¼:\")\n",
    "    ollama_result = production_system.process_query(test_case, use_openai=False)\n",
    "    \n",
    "    print(f\"ì²˜ë¦¬ ID: {ollama_result['processing_id']}\")\n",
    "    print(f\"ì•ˆì „ì„± í†µê³¼: {'âœ…' if ollama_result['safety_passed'] else 'âŒ'}\")\n",
    "    print(f\"ê°ì§€ëœ ê°ì •: {ollama_result.get('sentiment_detected', 'N/A')}\")\n",
    "    print(f\"ì‚¬ìš© ëª¨ë¸: {ollama_result.get('model_used', 'N/A')}\")\n",
    "    print(f\"ì²˜ë¦¬ ì‹œê°„: {ollama_result['processing_time']:.2f}ì´ˆ\")\n",
    "    \n",
    "    print(\"\\nì²˜ë¦¬ ë‹¨ê³„:\")\n",
    "    for step in ollama_result[\"steps\"]:\n",
    "        status = \"âœ…\" if step[\"passed\"] else \"âŒ\"\n",
    "        print(f\"  {status} {step['step']}: {step.get('details', {})}\")\n",
    "    \n",
    "    print(f\"\\nìµœì¢… ì‘ë‹µ: {ollama_result['final_response'][:200]}{'...' if len(ollama_result['final_response']) > 200 else ''}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# ì‹œìŠ¤í…œ ì „ì²´ í†µê³„ ì¶œë ¥\n",
    "print(\"\\nğŸ“Š Production ì‹œìŠ¤í…œ ì „ì²´ í†µê³„\")\n",
    "stats = production_system.get_system_statistics()\n",
    "\n",
    "print(f\"ì´ ì²˜ë¦¬ëœ ì¿¼ë¦¬: {stats['total_queries']}\")\n",
    "print(f\"ì•ˆì „ì„± í†µê³¼ìœ¨: {stats['safety_pass_rate']:.1f}%\")\n",
    "print(f\"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['average_processing_time']:.2f}ì´ˆ\")\n",
    "\n",
    "print(\"\\nê°ì • ë¶„í¬:\")\n",
    "for sentiment, count in stats['sentiment_distribution'].items():\n",
    "    percentage = (count / stats['total_queries'] * 100)\n",
    "    print(f\"  {sentiment}: {count}íšŒ ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nëª¨ë¸ ì‚¬ìš© ë¶„í¬:\")\n",
    "for model, count in stats['model_usage'].items():\n",
    "    percentage = (count / stats['total_queries'] * 100)\n",
    "    print(f\"  {model}: {count}íšŒ ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nê²¬ê³ í•œ í´ë¼ì´ì–¸íŠ¸ í†µê³„:\")\n",
    "robust_stats = stats['robust_client_stats']\n",
    "print(f\"  ì´ í˜¸ì¶œ: {robust_stats['total_calls']}\")\n",
    "print(f\"  ì„±ê³µë¥ : {robust_stats['success_rate']:.1f}%\")\n",
    "print(f\"  í‰ê·  í˜¸ì¶œ ì‹œê°„: {robust_stats['average_duration']:.2f}ì´ˆ\")\n",
    "print(f\"  Circuit Breaker ìƒíƒœ: {robust_stats['circuit_breaker_states']}\")\n",
    "\n",
    "print(\"\\nâœ… Production ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ê²°ë¡  ë° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤\n",
    "\n",
    "## ì£¼ìš” í•™ìŠµ ë‚´ìš©\n",
    "\n",
    "### 1. ê°ì„± ë¶„ì„ í›„ ë¶„ê¸° (Sentiment Branching)\n",
    "- ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì‘ë‹µ ì „ëµ ì„ íƒ\n",
    "- ê°ì •ë³„ ë§ì¶¤í˜• í†¤ê³¼ ì ‘ê·¼ ë°©ì‹ ì ìš©\n",
    "- ì‚¬ìš©ì ê²½í—˜ í–¥ìƒê³¼ ë§Œì¡±ë„ ì¦ëŒ€\n",
    "\n",
    "### 2. ì½˜í…ì¸  í•„í„°ë§ (Content Filtering)\n",
    "- ë‹¤ë‹¨ê³„ ì•ˆì „ì„± ê²€ì¦ (ê·œì¹™ ê¸°ë°˜ + AI ê¸°ë°˜)\n",
    "- ê¸ˆì¹™ì–´, ê°œì¸ì •ë³´, ìœ„í—˜ ì½˜í…ì¸  ìë™ ê°ì§€\n",
    "- ì•ˆì „í•œ ëŒ€ì²´ ì‘ë‹µ ìë™ ìƒì„±\n",
    "\n",
    "### 3. ì¬ì‹œë„/ë°±ì˜¤í”„ (Retry/Backoff)\n",
    "- ë‹¤ì–‘í•œ ë°±ì˜¤í”„ ì „ëµ (ì§€ìˆ˜, ì„ í˜•, ëœë¤ ì§€í„°)\n",
    "- Circuit Breaker íŒ¨í„´ìœ¼ë¡œ ì‹œìŠ¤í…œ ë³´í˜¸\n",
    "- í´ë°± ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ê°€ìš©ì„± ë³´ì¥\n",
    "\n",
    "## í”„ë¡œë•ì…˜ í™˜ê²½ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­\n",
    "\n",
    "### ì„±ëŠ¥ ìµœì í™”\n",
    "- ìºì‹± ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„\n",
    "- ë¹„ë™ê¸° ì²˜ë¦¬ ë„ì…\n",
    "- ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì„¤ì • ê´€ë¦¬\n",
    "\n",
    "### ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…\n",
    "- ìƒì„¸í•œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘\n",
    "- ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "- ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ êµ¬ì„±\n",
    "\n",
    "### ë³´ì•ˆ ê°•í™”\n",
    "- API í‚¤ ë³´ì•ˆ ê´€ë¦¬\n",
    "- ì‚¬ìš©ì ì¸ì¦/ì¸ê°€\n",
    "- ë°ì´í„° ì•”í˜¸í™”\n",
    "\n",
    "ì´ëŸ¬í•œ ê¸°ë²•ë“¤ì„ ì¡°í•©í•˜ì—¬ ì•ˆì •ì ì´ê³  ì‚¬ìš©ì ì¹œí™”ì ì¸ AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajou-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
