{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa048d9",
   "metadata": {},
   "source": [
    "# 10ì£¼ì°¨ ì‹¤ìŠµ: LLMOps ë¡œê·¸ ìˆ˜ì§‘ & ê°„ë‹¨ ë¶„ì„\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **9ì£¼ì°¨ FastAPI/ì¶”ë¡  ì‹¤ìŠµ**ì„ ì´ì–´ì„œ, 10ì£¼ì°¨ì— ë°°ìš´ **LLMOps(ë¡œê·¸ ìˆ˜ì§‘Â·ê´€ì°°)**ì„ ì‹¤ìŠµí•˜ê¸° ìœ„í•œ í…œí”Œë¦¿ì…ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ‘‰ ëª©í‘œ:\n",
    "- LLMì„ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ê³ ,\n",
    "- ê° í˜¸ì¶œì„ ê³µí†µ ìŠ¤í‚¤ë§ˆë¡œ **CSV(or Langfuse)**ì— ê¸°ë¡í•˜ê³ ,\n",
    "- ê°„ë‹¨í•œ í†µê³„ë¥¼ ë½‘ì•„ë³´ëŠ” ê²ƒê¹Œì§€ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "â€» ì´ ë…¸íŠ¸ë¶ì€ **ì§ì ‘ ì‹¤í–‰ìš© í…œí”Œë¦¿**ì…ë‹ˆë‹¤. OpenAI í‚¤, Langfuse í‚¤ ë“±ì€ ê°ê° í™˜ê²½ì—ì„œ ì„¸íŒ…í•´ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0cde0",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì¤€ë¹„\n",
    "\n",
    "- Python 3.10+\n",
    "- íŒ¨í‚¤ì§€: `openai`, `pandas`, `python-dotenv`(ì„ íƒ), `langfuse`(ì„ íƒ)\n",
    "\n",
    "í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "```bash\n",
    "pip install openai pandas python-dotenv langfuse\n",
    "```\n",
    "\n",
    "ë˜ëŠ” 9ì£¼ì°¨ì— ì‚¬ìš©í•œ ê°€ìƒí™˜ê²½/conda í™˜ê²½ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54cd0d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# (ì„ íƒ) í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì…€ â€“ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´ ì‹¤í–‰í•  í•„ìš” ì—†ìŠµë‹ˆë‹¤.\n",
    "!pip install -q openai pandas python-dotenv langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d1707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe19b9",
   "metadata": {},
   "source": [
    "## 2. ì„¤ì • & ê³µí†µ ìƒìˆ˜\n",
    "\n",
    "9ì£¼ì°¨ì—ì„œ ì‚¬ìš©í•œ í™˜ê²½ê³¼ ì´ì–´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "- OpenAI API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ `OPENAI_API_KEY`ë¡œ ì„¤ì •í–ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "- (ì„ íƒ) Langfuseë¥¼ ì‚¬ìš©í•  ê²½ìš° `LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, `LANGFUSE_HOST`ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œëŠ” `.env` íŒŒì¼ì— ë‹¤ìŒê³¼ ê°™ì´ ì ì–´ë‘˜ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "```env\n",
    "OPENAI_API_KEY=sk-...\n",
    "LANGFUSE_PUBLIC_KEY=...\n",
    "LANGFUSE_SECRET_KEY=...\n",
    "LANGFUSE_HOST=http://localhost:3000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3b69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    # python-dotenvê°€ ì—†ì–´ë„ ë™ì‘ì€ ê°€ëŠ¥\n",
    "    pass\n",
    "\n",
    "# OpenAI ì„¤ì • (ìƒˆ Python SDK ê¸°ì¤€)\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if OPENAI_API_KEY is None:\n",
    "    print(\"âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ë…¸íŠ¸ë¶ì˜ LLM í˜¸ì¶œ ë¶€ë¶„ì€ ì‹¤í–‰ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "LOG_DIR = Path(\"logs\")\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "CSV_LOG_PATH = LOG_DIR / \"llm_responses.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c49dc8",
   "metadata": {},
   "source": [
    "## 3. LLM í˜¸ì¶œ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "9ì£¼ì°¨ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í–ˆë˜ LLM í˜¸ì¶œ ì½”ë“œë¥¼ í•¨ìˆ˜ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ì…ë ¥: `prompt`, `model`, `temperature`, `max_tokens`\n",
    "- ì¶œë ¥: (response_text, latency_ms, usage_dict)\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” OpenAI Chat Completions APIë¥¼ ì˜ˆì‹œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • ê°€ëŠ¥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61682756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str,\n",
    "             model: str = \"gpt-4o-mini\",\n",
    "             temperature: float = 0.2,\n",
    "             max_tokens: int = 512):\n",
    "    \"\"\"LLMì„ í•œ ë²ˆ í˜¸ì¶œí•˜ê³ , ì‘ë‹µ í…ìŠ¤íŠ¸/ì§€ì—°ì‹œê°„/í† í° ì‚¬ìš©ëŸ‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if OPENAI_API_KEY is None:\n",
    "        raise RuntimeError(\"OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì•„ LLMì„ í˜¸ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    start = time.time()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant for an LLMOps lecture demo.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    end = time.time()\n",
    "\n",
    "    latency_ms = int((end - start) * 1000)\n",
    "\n",
    "    message = completion.choices[0].message\n",
    "    text = message.content\n",
    "\n",
    "    usage = completion.usage\n",
    "    usage_dict = {\n",
    "        \"input_tokens\": usage.prompt_tokens if usage else None,\n",
    "        \"output_tokens\": usage.completion_tokens if usage else None,\n",
    "        \"total_tokens\": usage.total_tokens if usage else None,\n",
    "    }\n",
    "\n",
    "    return text, latency_ms, usage_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841c404",
   "metadata": {},
   "source": [
    "## 4. CSV ê¸°ë°˜ Logger êµ¬í˜„\n",
    "\n",
    "Langfuseë¥¼ ì“°ì§€ ëª»í•˜ëŠ” í™˜ê²½ì—ì„œë„ ì‹¤ìŠµí•  ìˆ˜ ìˆë„ë¡, **CSV ê¸°ë°˜ LLMOps ë¡œê±°**ë¥¼ ë¨¼ì € êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ê° í˜¸ì¶œì„ í•œ ì¤„(row)ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "- ìŠ¤í‚¤ë§ˆëŠ” 10ì£¼ì°¨ ê°•ì˜ì—ì„œ ì •ì˜í•œ ê³µí†µ ìŠ¤í‚¤ë§ˆì˜ ì¶•ì†ŒíŒì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4102875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CSV Logger ì¤€ë¹„ ì™„ë£Œ: logs/llm_responses.csv\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    \"id\", \"timestamp\", \"endpoint\", \"user_id\", \"session_id\",\n",
    "    \"prompt\", \"prompt_version\", \"model\",\n",
    "    \"latency_ms\", \"input_tokens\", \"output_tokens\", \"total_tokens\",\n",
    "    \"cost\", \"status_code\", \"feedback\",\n",
    "]\n",
    "\n",
    "\n",
    "class CsvLogger:\n",
    "    def __init__(self, csv_path: Path, model_price_per_1k_tokens: float = 0.0005):\n",
    "        self.csv_path = csv_path\n",
    "        self.price_per_1k_tokens = model_price_per_1k_tokens\n",
    "\n",
    "        if not self.csv_path.exists():\n",
    "            df = pd.DataFrame(columns=CSV_COLUMNS)\n",
    "            df.to_csv(self.csv_path, index=False)\n",
    "\n",
    "    def _estimate_cost(self, total_tokens: int | None) -> float | None:\n",
    "        if total_tokens is None:\n",
    "            return None\n",
    "        return round(self.price_per_1k_tokens * (total_tokens / 1000.0), 6)\n",
    "\n",
    "    def log(self,\n",
    "            prompt: str,\n",
    "            prompt_version: str,\n",
    "            model: str,\n",
    "            latency_ms: int,\n",
    "            usage: dict | None = None,\n",
    "            endpoint: str = \"/llmops/demo\",\n",
    "            user_id: str = \"demo_user\",\n",
    "            session_id: str | None = None,\n",
    "            status_code: int = 200,\n",
    "            feedback: int | None = None):\n",
    "\n",
    "        if session_id is None:\n",
    "            session_id = str(uuid.uuid4())\n",
    "\n",
    "        usage = usage or {}\n",
    "        input_tokens = usage.get(\"input_tokens\")\n",
    "        output_tokens = usage.get(\"output_tokens\")\n",
    "        total_tokens = usage.get(\"total_tokens\")\n",
    "\n",
    "        cost = self._estimate_cost(total_tokens)\n",
    "\n",
    "        row = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"endpoint\": endpoint,\n",
    "            \"user_id\": user_id,\n",
    "            \"session_id\": session_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"prompt_version\": prompt_version,\n",
    "            \"model\": model,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"cost\": cost,\n",
    "            \"status_code\": status_code,\n",
    "            \"feedback\": feedback,\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame([row])\n",
    "        df.to_csv(self.csv_path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "        return row\n",
    "\n",
    "\n",
    "csv_logger = CsvLogger(CSV_LOG_PATH)\n",
    "print(f\"âœ… CSV Logger ì¤€ë¹„ ì™„ë£Œ: {CSV_LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95e575",
   "metadata": {},
   "source": [
    "## 5. (ì„ íƒ) Langfuse Logger êµ¬í˜„\n",
    "\n",
    "Langfuse ì„œë²„(í´ë¼ìš°ë“œ ë˜ëŠ” ë¡œì»¬)ê°€ ìˆê³ , `LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, `LANGFUSE_HOST`ë¥¼ ì„¤ì •í–ˆë‹¤ë©´\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ì‚¬ìš©í•´ **Langfuse trace**ë„ ë™ì‹œì— ë‚¨ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì„¤ì •ì´ ì—†ìœ¼ë©´ ì´ ì…€ì€ ê²½ê³ ë§Œ ì¶œë ¥í•˜ê³  ë„˜ì–´ê°‘ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68b7459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (v3 ìŠ¤íƒ€ì¼)\n"
     ]
    }
   ],
   "source": [
    "# === (ìˆ˜ì • ë²„ì „) Langfuse ì—°ë™ ===\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "\n",
    "# Langfuse v3ì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ LANGFUSE_BASE_URL ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "LANGFUSE_BASE_URL = os.getenv(\"LANGFUSE_BASE_URL\") or os.getenv(\"LANGFUSE_HOST\")\n",
    "\n",
    "langfuse_client = None\n",
    "\n",
    "if LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY:\n",
    "    try:\n",
    "        # v3 ê¸°ì¤€: ì§ì ‘ Langfuse(...) ëŒ€ì‹  get_client() ì‚¬ìš©\n",
    "        from langfuse import get_client\n",
    "\n",
    "        langfuse_client = get_client()\n",
    "        print(\"âœ… Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (v3 ìŠ¤íƒ€ì¼)\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Langfuse ì´ˆê¸°í™” ì‹¤íŒ¨:\", e)\n",
    "        langfuse_client = None\n",
    "else:\n",
    "    print(\"â„¹ï¸ Langfuse í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CSV ë¡œê¹…ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "def log_to_langfuse(\n",
    "    prompt: str,\n",
    "    response: str,\n",
    "    prompt_version: str,\n",
    "    model: str,\n",
    "    latency_ms: int,\n",
    "    usage: dict | None = None,\n",
    "    user_id: str = \"demo_user\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Langfuseê°€ ì„¤ì •ëœ ê²½ìš°, í•˜ë‚˜ì˜ span(=ê´€ì°°)ì„ ë‚¨ê¹ë‹ˆë‹¤.\n",
    "    v3 SDKì—ì„œëŠ” client.trace(...)ê°€ ì•„ë‹ˆë¼ start_as_current_span(...)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if langfuse_client is None:\n",
    "        return None\n",
    "\n",
    "    usage = usage or {}\n",
    "    try:\n",
    "        # ê°„ë‹¨í•˜ê²Œ span í•˜ë‚˜ë¥¼ ë§Œë“¤ê³ , input/output + ë©”íƒ€ë°ì´í„°ë¥¼ ë‚¨ê¹€\n",
    "        with langfuse_client.start_as_current_span(\n",
    "            name=\"week10-llmops-demo\",\n",
    "            input={\n",
    "                \"prompt\": prompt,\n",
    "                \"prompt_version\": prompt_version,\n",
    "                \"model\": model,\n",
    "            },\n",
    "        ) as span:\n",
    "            # span ìì²´ì— output/ë©”íƒ€ë°ì´í„° ê¸°ë¡\n",
    "            span.update(\n",
    "                output={\"response\": response},\n",
    "                metadata={\n",
    "                    \"prompt_version\": prompt_version,\n",
    "                    \"model\": model,\n",
    "                    \"latency_ms\": latency_ms,\n",
    "                    \"input_tokens\": usage.get(\"input_tokens\"),\n",
    "                    \"output_tokens\": usage.get(\"output_tokens\"),\n",
    "                    \"total_tokens\": usage.get(\"total_tokens\"),\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # trace ë ˆë²¨ ì†ì„±ë„ ì—…ë°ì´íŠ¸ ê°€ëŠ¥ (user_id ë“±)\n",
    "            span.update_trace(\n",
    "                user_id=user_id,\n",
    "                tags=[\"week10-llmops-demo\"],\n",
    "            )\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # Langfuseë§Œ ì‹¤íŒ¨í•´ë„ ì „ì²´ ì‹¤ìŠµì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ ê·¸ëƒ¥ ê²½ê³ ë§Œ ì¶œë ¥\n",
    "        print(\"âš ï¸ Langfuse ë¡œê¹… ì‹¤íŒ¨:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a00001",
   "metadata": {},
   "source": [
    "## 6. í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ë¡œ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•´ë³´ê¸°\n",
    "\n",
    "ì´ì œ ì‹¤ì œë¡œ **í”„ë¡¬í”„íŠ¸ ë²„ì „ A/B**ë¥¼ ì •ì˜í•˜ê³ , ê°ê° ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ë©´ì„œ ë¡œê·¸ë¥¼ ë‚¨ê²¨ë´…ë‹ˆë‹¤.\n",
    "\n",
    "- `PROMPT_V1`, `PROMPT_V2` ë‘ ê°€ì§€ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "- ê° ë²„ì „ë‹¹ 3íšŒ ì´ìƒ í˜¸ì¶œí•´ë³´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
    "- Langfuseê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ Langfuse + CSV, ì•„ë‹ˆë©´ CSVë§Œ ê¸°ë¡ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3852ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt version: v1 ===\n",
      "  - call 1/3... âœ… 4920 ms, tokens=165\n",
      "  - call 2/3... âœ… 2926 ms, tokens=164\n",
      "  - call 3/3... âœ… 3161 ms, tokens=156\n",
      "\n",
      "=== Prompt version: v2 ===\n",
      "  - call 1/3... âœ… 4222 ms, tokens=193\n",
      "  - call 2/3... âœ… 2812 ms, tokens=181\n",
      "  - call 3/3... âœ… 2852 ms, tokens=198\n",
      "ì¤€ë¹„ ì™„ë£Œ âœ…\n",
      "ì‹¤ì œ LLM í˜¸ì¶œì€ ìœ„ run_batch_calls(...) ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•´ë³´ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "PROMPT_V1 = \"LLMOpsë¥¼ ì²˜ìŒ ë°°ìš°ëŠ” ëŒ€í•™ìƒì—ê²Œ, ì•„ì£¼ ê°„ë‹¨í•˜ê²Œ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜.\"\n",
    "PROMPT_V2 = \"ë‹¹ì‹ ì€ ë¨¸ì‹ ëŸ¬ë‹ ìš´ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. MLOpsì™€ LLMOpsì˜ ì°¨ì´ë¥¼ í•µì‹¬ í‚¤ì›Œë“œ ìœ„ì£¼ë¡œ 3ì¤„ ì´ë‚´ë¡œ ì •ë¦¬í•´ì¤˜.\"\n",
    "\n",
    "def run_batch_calls(prompts: list[tuple[str, str]],\n",
    "                    model: str = \"gpt-4o-mini\",\n",
    "                    n_calls_per_prompt: int = 3):\n",
    "    \"\"\"(prompt_text, prompt_version) ëª©ë¡ì„ ë°›ì•„ ê° ë²„ì „ë§ˆë‹¤ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    rows = []\n",
    "    for prompt_text, prompt_version in prompts:\n",
    "        print(f\"\\n=== Prompt version: {prompt_version} ===\")\n",
    "        for i in range(n_calls_per_prompt):\n",
    "            print(f\"  - call {i+1}/{n_calls_per_prompt}...\", end=\" \")\n",
    "            try:\n",
    "                response, latency_ms, usage = call_llm(prompt_text, model=model)\n",
    "                row = csv_logger.log(\n",
    "                    prompt=prompt_text,\n",
    "                    prompt_version=prompt_version,\n",
    "                    model=model,\n",
    "                    latency_ms=latency_ms,\n",
    "                    usage=usage,\n",
    "                )\n",
    "                log_to_langfuse(\n",
    "                    prompt=prompt_text,\n",
    "                    response=response,\n",
    "                    prompt_version=prompt_version,\n",
    "                    model=model,\n",
    "                    latency_ms=latency_ms,\n",
    "                    usage=usage,\n",
    "                )\n",
    "                print(f\"âœ… {latency_ms} ms, tokens={usage.get('total_tokens') if usage else None}\")\n",
    "                rows.append(row)\n",
    "            except Exception as e:\n",
    "                print(\"âŒ í˜¸ì¶œ ì‹¤íŒ¨:\", e)\n",
    "    return rows\n",
    "\n",
    "# ì‹¤ìŠµ ë•Œ ì§ì ‘ ì‹¤í–‰í•˜ì„¸ìš” (API í‚¤ í•„ìš”)\n",
    "_ = run_batch_calls([\n",
    "    (PROMPT_V1, \"v1\"),\n",
    "    (PROMPT_V2, \"v2\"),\n",
    "], model=\"gpt-4o-mini\", n_calls_per_prompt=3)\n",
    "\n",
    "print(\"ì¤€ë¹„ ì™„ë£Œ âœ…\\nì‹¤ì œ LLM í˜¸ì¶œì€ ìœ„ run_batch_calls(...) ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•´ë³´ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97867d01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cce0f8b",
   "metadata": {},
   "source": [
    "## 7. CSV ë¡œê·¸ ë¶ˆëŸ¬ì™€ì„œ ê°„ë‹¨ ë¶„ì„í•˜ê¸°\n",
    "\n",
    "ì´ì œ `logs/llm_responses.csv` íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ê°„ë‹¨í•œ í†µê³„ë¥¼ ê³„ì‚°í•´ ë´…ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆì‹œ:\n",
    "- í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ í‰ê·  ì§€ì—°ì‹œê°„\n",
    "- í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ í‰ê·  í† í° ìˆ˜\n",
    "- ëª¨ë¸ë³„ í‰ê·  ë¹„ìš©\n",
    "\n",
    "â€» LLM í˜¸ì¶œì„ ì‹¤ì œë¡œ ì‹¤í–‰í•œ ë’¤ì— ì´ ì…€ì„ ëŒë ¤ì•¼ ì˜ë¯¸ìˆëŠ” ê°’ì´ ë‚˜ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bfc0052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¡œê·¸ ë ˆì½”ë“œ ìˆ˜: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_version</th>\n",
       "      <th>model</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>cost</th>\n",
       "      <th>status_code</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, timestamp, endpoint, user_id, session_id, prompt, prompt_version, model, latency_ms, input_tokens, output_tokens, total_tokens, cost, status_code, feedback]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CSV_LOG_PATH.exists():\n",
    "    df = pd.read_csv(CSV_LOG_PATH)\n",
    "    print(\"ë¡œê·¸ ë ˆì½”ë“œ ìˆ˜:\", len(df))\n",
    "    display(df.tail())\n",
    "else:\n",
    "    print(\"ì•„ì§ CSV ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € LLM í˜¸ì¶œì„ ì‹¤í–‰í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00c07e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ í‰ê·  latency_ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "prompt_version\n",
       "v1    3359.833333\n",
       "v2    3854.166667\n",
       "Name: latency_ms, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ í‰ê·  total_tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "prompt_version\n",
       "v1    158.0\n",
       "v2    189.0\n",
       "Name: total_tokens, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ëª¨ë¸ë³„ í‰ê·  cost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model\n",
       "gpt-4o-mini    0.000087\n",
       "Name: cost, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CSV_LOG_PATH.exists():\n",
    "    df = pd.read_csv(CSV_LOG_PATH)\n",
    "\n",
    "    print(\"\\nğŸ“Š í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ í‰ê·  latency_ms\")\n",
    "    if \"prompt_version\" in df.columns and \"latency_ms\" in df.columns:\n",
    "        display(df.groupby(\"prompt_version\")[\"latency_ms\"].mean())\n",
    "    else:\n",
    "        print(\"í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    print(\"\\nğŸ“Š í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ í‰ê·  total_tokens\")\n",
    "    if \"prompt_version\" in df.columns and \"total_tokens\" in df.columns:\n",
    "        display(df.groupby(\"prompt_version\")[\"total_tokens\"].mean())\n",
    "    else:\n",
    "        print(\"í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    print(\"\\nğŸ“Š ëª¨ë¸ë³„ í‰ê·  cost\")\n",
    "    if \"model\" in df.columns and \"cost\" in df.columns:\n",
    "        display(df.groupby(\"model\")[\"cost\"].mean())\n",
    "    else:\n",
    "        print(\"í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e8ea7",
   "metadata": {},
   "source": [
    "## 8. ë¦¬í¬íŠ¸ í…œí”Œë¦¿ (ê³¼ì œìš©)\n",
    "\n",
    "ì•„ë˜ í…ìŠ¤íŠ¸ ì…€ì„ ë³µì‚¬í•´ì„œ `report.md` ë˜ëŠ” ê³¼ì œ ì œì¶œìš© ë¬¸ì„œì— ë¶™ì—¬ë„£ê³ , ìì‹ ì˜ ê²°ê³¼ë¥¼ ì±„ì›Œë³´ì„¸ìš”.\n",
    "\n",
    "```markdown\n",
    "# 10ì£¼ì°¨ LLMOps ì‹¤ìŠµ ë¦¬í¬íŠ¸\n",
    "\n",
    "## 1. ì‹¤í—˜ ì„¤ì •\n",
    "- ì‚¬ìš© ëª¨ë¸: gpt-4o-mini\n",
    "- í”„ë¡¬í”„íŠ¸ ë²„ì „:\n",
    "  - v1: ...\n",
    "  - v2: ...\n",
    "- ê° ë²„ì „ë‹¹ í˜¸ì¶œ íšŸìˆ˜: 3íšŒ\n",
    "\n",
    "## 2. ê²°ê³¼ ìš”ì•½\n",
    "- v1 í‰ê·  latency: XXX ms\n",
    "- v2 í‰ê·  latency: YYY ms\n",
    "- v1 í‰ê·  total_tokens: AAA\n",
    "- v2 í‰ê·  total_tokens: BBB\n",
    "- v1/v2 ë¹„ìš© ì°¨ì´: ...\n",
    "\n",
    "## 3. ì¸ì‚¬ì´íŠ¸\n",
    "- v2ëŠ” ë‹µë³€ì´ ë” ê¸¸ì–´ì„œ í† í°/ë¹„ìš©ì´ ëŠ˜ì–´ë‚¬ì§€ë§Œ, ì„¤ëª…ì´ ë” ì¹œì ˆí–ˆë‹¤.\n",
    "- v1ì€ ì§§ê³  ë¹ ë¥´ì§€ë§Œ, LLMOpsë¥¼ ì²˜ìŒ ë°°ìš°ëŠ” ì‚¬ëŒ ì…ì¥ì—ì„œëŠ” ì´í•´ê°€ ì–´ë µë‹¤ëŠ” í”¼ë“œë°±ì´ ìˆì—ˆë‹¤.\n",
    "- ìš´ì˜ ê´€ì ì—ì„œëŠ” v2 í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ë˜, í† í°ì„ ì¤„ì´ê¸° ìœ„í•œ ê°œì„ ì´ í•„ìš”í•´ ë³´ì¸ë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d103b",
   "metadata": {},
   "source": [
    "## 9. ë§ˆë¬´ë¦¬\n",
    "\n",
    "- ì´ ë…¸íŠ¸ë¶ì„ í†µí•´ **LLM í˜¸ì¶œ â†’ ë¡œê·¸ ë‚¨ê¸°ê¸° â†’ í†µê³„ ë³´ê¸°**ê¹Œì§€ í•œ ë²ˆì˜ ì‚¬ì´í´ì„ ê²½í—˜í–ˆìŠµë‹ˆë‹¤.\n",
    "- 11ì£¼ì°¨ì—ì„œëŠ” ì´ ë¡œê·¸/ì‘ë‹µë“¤ì„ í™œìš©í•´ **í‰ê°€(Evaluation)**ì™€ **RAG í’ˆì§ˆ ì¸¡ì •**ì„ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "í•„ìš”í•˜ë‹¤ë©´ ì´ ë…¸íŠ¸ë¶ì„ ë³µì‚¬í•´ì„œ ìì‹ ì˜ í”„ë¡œì íŠ¸/ê³¼ì œì— ë§ê²Œ **í”„ë¡¬í”„íŠ¸, ëª¨ë¸, ë¡œê·¸ ìŠ¤í‚¤ë§ˆ**ë¥¼ ìˆ˜ì •í•´ë³´ì„¸ìš”. ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajou-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
