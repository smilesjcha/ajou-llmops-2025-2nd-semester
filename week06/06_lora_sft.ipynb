{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e52e69",
   "metadata": {},
   "source": [
    "\n",
    "# Week06 — LoRA 기반 SFT 실습 (Local 환경용)\n",
    "\n",
    "**목표**\n",
    "- 작은 예제 데이터로 **LoRA 기반 SFT**를 로컬에서 수행해보고,\n",
    "- **Base vs LoRA** 추론 결과를 비교/기록합니다.\n",
    "- (선택) 로컬 **Ollama `llama3.1:8b-instruct`**와 결과를 간단 비교합니다.\n",
    "\n",
    "> ⚙️ 본 노트북은 **로컬 실행**을 전제로 하며, GPU가 있으면 `4/8bit` 로딩을 활용합니다. CPU 환경에서도 **작은 모델**로 데모는 가능합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea6d80e",
   "metadata": {},
   "source": [
    "## 0) 환경 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e634ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "Platform: macOS-15.6.1-arm64-arm-64bit\n",
      "CUDA available: False\n",
      "Torch: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !pip install -q -U transformers datasets peft accelerate bitsandbytes sentencepiece#   langfuse python-dotenv pandas requests torch --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "# ↑ 필요 시 주석 해제. CUDA 버전에 맞춰 torch 인덱스는 조정하세요.\n",
    "import sys, platform, torch, os, json, time, math, random\n",
    "from pathlib import Path\n",
    "print('Python:', sys.version)\n",
    "print('Platform:', platform.platform())\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('Torch:', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4a984",
   "metadata": {},
   "source": [
    "## 1) .env 로드 (OpenAI/Langfuse/Pinecone 키 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ff492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env from: /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/.env\n",
      "OpenAI key set: True\n",
      "Langfuse host: https://cloud.langfuse.com\n",
      "Pinecone key set: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# 프로젝트 루트에서 .env 탐색\n",
    "env_path = find_dotenv(usecwd=True)\n",
    "if not env_path:\n",
    "    # 상위 디렉토리 탐색(노트북이 /week06 안에 있을 수 있으므로)\n",
    "    cur = Path.cwd()\n",
    "    for p in [cur] + list(cur.parents):\n",
    "        cand = p / '.env'\n",
    "        if cand.exists():\n",
    "            env_path = str(cand)\n",
    "            break\n",
    "\n",
    "if env_path:\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"Loaded .env from: {env_path}\")\n",
    "else:\n",
    "    print(\"⚠️ .env를 찾지 못했습니다. 프로젝트 루트에 .env를 생성/배치하세요.\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv('LANGFUSE_PUBLIC_KEY', '')\n",
    "LANGFUSE_SECRET_KEY = os.getenv('LANGFUSE_SECRET_KEY', '')\n",
    "LANGFUSE_HOST = os.getenv('LANGFUSE_HOST', 'https://cloud.langfuse.com')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY', '')\n",
    "\n",
    "print('OpenAI key set:', bool(OPENAI_API_KEY))\n",
    "print('Langfuse host:', LANGFUSE_HOST)\n",
    "print('Pinecone key set:', bool(PINECONE_API_KEY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43299e",
   "metadata": {},
   "source": [
    "## 2) 실험 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d2eee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CFG(project_root=PosixPath('/Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/week06'), data_dir=PosixPath('data'), out_dir=PosixPath('runs/lora_sft'), base_model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', use_qlora=True, max_seq_len=512, lora_r=8, lora_alpha=16, lora_dropout=0.05, target_modules=('q_proj', 'v_proj'), lr=0.0002, warmup_ratio=0.05, num_epochs=1, per_device_train_batch_size=2, grad_accum_steps=8, logging_steps=10, save_steps=50, eval_ratio=0.1, seed=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # 실행/경로\n",
    "    project_root: Path = Path.cwd().resolve()\n",
    "    data_dir: Path = Path('data')\n",
    "    out_dir: Path = Path('runs/lora_sft')\n",
    "    # 모델/토크나이저 (작은 모델 권장)\n",
    "    base_model: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # CPU/GPU 모두 가능한 예시\n",
    "    use_qlora: bool = True  # GPU 있으면 QLoRA, 아니면 자동 CPU\n",
    "    # 토큰/길이\n",
    "    max_seq_len: int = 512\n",
    "    # LoRA 하이퍼파라미터\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: tuple = (\"q_proj\",\"v_proj\")\n",
    "    # 학습 하이퍼\n",
    "    lr: float = 2e-4\n",
    "    warmup_ratio: float = 0.05\n",
    "    num_epochs: int = 1\n",
    "    per_device_train_batch_size: int = 2\n",
    "    grad_accum_steps: int = 8\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 50\n",
    "    eval_ratio: float = 0.1  # 간이 검증셋 비율\n",
    "    seed: int = 42\n",
    "\n",
    "cfg = CFG()\n",
    "cfg.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7beb50a",
   "metadata": {},
   "source": [
    "## 3) 데이터 준비 (알파카 스타일 JSONL, 없으면 미니 예제 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bea5928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lines: 80\n",
      "val   lines: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "train_path = cfg.data_dir / 'train.jsonl'\n",
    "val_path   = cfg.data_dir / 'val.jsonl'\n",
    "\n",
    "def write_demo_jsonl(path: Path, n=60):\n",
    "    demo = []\n",
    "    for i in range(n):\n",
    "        demo.append({\n",
    "            \"instruction\": \"요청한 주제에 대해 한 문단으로 간결히 설명하세요.\",\n",
    "            \"input\": f\"주제: 데이터 품질과 라벨 마스킹의 중요성 #{i}\",\n",
    "            \"output\": \"데이터 품질은 모델 성능의 상한을 결정하며, 라벨 마스킹 오류는 수렴을 방해한다. 일관된 템플릿과 PII/Toxic 필터는 필수다.\"\n",
    "        })\n",
    "    with path.open('w', encoding='utf-8') as f:\n",
    "        for r in demo:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# 파일이 없으면 데모 생성\n",
    "if not train_path.exists():\n",
    "    write_demo_jsonl(train_path, n=80)\n",
    "if not val_path.exists():\n",
    "    write_demo_jsonl(val_path, n=10)\n",
    "\n",
    "# 간단 확인\n",
    "print('train lines:', sum(1 for _ in open(train_path, 'r', encoding='utf-8')))\n",
    "print('val   lines:', sum(1 for _ in open(val_path,   'r', encoding='utf-8')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84560552",
   "metadata": {},
   "source": [
    "## 4) 토크나이저/데이터셋 로드 & 템플릿 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54578b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 80/80 [00:00<00:00, 1740.53 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 1865.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 80\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 10\n",
       " }))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def read_jsonl(p: Path):\n",
    "    rows = []\n",
    "    with p.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                rows.append(json.loads(line))\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "train_ds = read_jsonl(train_path)\n",
    "val_ds = read_jsonl(val_path)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "INSTR_TMPL = \"### Instruction\\n{instruction}\\n\\n### Input\\n{input}\\n\\n### Response\\n\"\n",
    "\n",
    "def format_example(ex):\n",
    "    prompt = INSTR_TMPL.format(instruction=ex.get(\"instruction\",\"\").strip(),\n",
    "                               input=ex.get(\"input\",\"\").strip())\n",
    "    target = ex.get(\"output\",\"\").strip()\n",
    "    text = prompt + target\n",
    "    enc = tokenizer(text, max_length=cfg.max_seq_len, truncation=True, padding='max_length')\n",
    "    # 라벨 마스킹: prompt 부분은 -100으로\n",
    "    prompt_ids = tokenizer(prompt, max_length=cfg.max_seq_len, truncation=True, padding='max_length')['input_ids']\n",
    "    labels = [-100]*len(prompt_ids)\n",
    "    full_ids = enc['input_ids']\n",
    "    # prompt 길이 이후는 정답 라벨\n",
    "    for i in range(len(full_ids)):\n",
    "        if i >= len(prompt_ids) or prompt_ids[i] != full_ids[i]:\n",
    "            labels[i] = full_ids[i]\n",
    "    enc['labels'] = labels\n",
    "    return enc\n",
    "\n",
    "train_tok = train_ds.map(format_example, remove_columns=train_ds.column_names)\n",
    "val_tok = val_ds.map(format_example, remove_columns=val_ds.column_names)\n",
    "train_tok, val_tok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0531b",
   "metadata": {},
   "source": [
    "## 5) 모델 로드(4/8bit 옵션) & LoRA 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908cccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 1,126,400 / Total: 1,101,174,784 (0.10%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "device_map = 'auto'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "load_in_8bit = False\n",
    "load_in_4bit = False\n",
    "bnb_config = None\n",
    "\n",
    "if use_cuda and cfg.use_qlora:\n",
    "    try:\n",
    "        from bitsandbytes.config import BitsAndBytesConfig\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        load_in_4bit = True\n",
    "        print(\"Using 4bit QLoRA.\")\n",
    "    except Exception as e:\n",
    "        print(\"4bit 불가 → 8bit 시도:\", e)\n",
    "        load_in_8bit = True\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.base_model,\n",
    "    # CPU/MPS에서는 device_map=None + float32 로 깔끔히 로딩\n",
    "    device_map=None,\n",
    "    low_cpu_mem_usage=False,          # ← meta 텐서 경로 회피\n",
    "    dtype=torch.float32,\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    quantization_config=bnb_config if load_in_4bit else None,\n",
    ").to(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "if (load_in_4bit or load_in_8bit) and hasattr(model, \"enable_input_require_grads\"):\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=cfg.lora_r,\n",
    "    lora_alpha=cfg.lora_alpha,\n",
    "    lora_dropout=cfg.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=list(cfg.target_modules),\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / Total: {total:,} ({100*trainable/total:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de18316",
   "metadata": {},
   "source": [
    "## 6) 학습 실행 (Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc8b0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/ajou-llmops/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import math, os\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(cfg.out_dir),\n",
    "    per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=cfg.grad_accum_steps,\n",
    "    learning_rate=cfg.lr,\n",
    "    num_train_epochs=cfg.num_epochs,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    logging_steps=cfg.logging_steps,\n",
    "    save_steps=cfg.save_steps,\n",
    "    optim=\"paged_adamw_8bit\" if (load_in_4bit or load_in_8bit) else \"adamw_torch\",\n",
    "    warmup_ratio=cfg.warmup_ratio,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok.select(range(max(1, int(len(val_tok)*cfg.eval_ratio)))),\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "res = trainer.train()\n",
    "trainer.save_model(cfg.out_dir / \"adapter\")  # LoRA 어댑터 저장\n",
    "(tokenizer.save_pretrained(cfg.out_dir / \"adapter\"))\n",
    "\n",
    "# 간단 로그\n",
    "final_loss = res.training_loss if hasattr(res, 'training_loss') else None\n",
    "print(\"Final training loss:\", final_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc9730",
   "metadata": {},
   "source": [
    "## 7) 추론: Base vs LoRA 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def build_prompt(instruction:str, _input:str):\n",
    "    return INSTR_TMPL.format(instruction=instruction.strip(), input=_input.strip())\n",
    "\n",
    "# 7-1) Base 모델 (어댑터 미적용) 로드\n",
    "base_tok = tokenizer  # 같은 토크나이저 사용\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.base_model,\n",
    "    device_map=device_map,\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    quantization_config=bnb_config if load_in_4bit else None,\n",
    ")\n",
    "\n",
    "# 7-2) LoRA 적용 모델은 trainer.model 재사용\n",
    "gen_kwargs = dict(max_new_tokens=200, do_sample=False, temperature=0.0, top_p=1.0)\n",
    "\n",
    "tests = [\n",
    "    (\"회의록 요약\", \"아래 회의 메모를 3줄로 요약하세요. 메모: 데이터 검수, 템플릿 고정, β 스윕 계획\"),\n",
    "    (\"데이터 품질 체크리스트\", \"SFT 데이터셋을 제출하기 전 수행해야 할 체크 5가지를 불릿으로.\"),\n",
    "]\n",
    "\n",
    "def generate(m, tok, prompt):\n",
    "    ids = tok(prompt, return_tensors='pt').to(m.device)\n",
    "    with torch.no_grad():\n",
    "        out = m.generate(**ids, **gen_kwargs)\n",
    "    return tok.decode(out[0], skip_special_tokens=True).split(\"### Response\")[-1].strip()\n",
    "\n",
    "for name, content in tests:\n",
    "    prompt = build_prompt(name, content)\n",
    "    base_out = generate(base_model, base_tok, prompt)\n",
    "    lora_out = generate(model, tokenizer, prompt)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROMPT:\", name)\n",
    "    print(\"- Base:\"); print(base_out[:1000])\n",
    "    print(\"- LoRA:\"); print(lora_out[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b66426",
   "metadata": {},
   "source": [
    "## (선택) 8) Ollama `llama3.1:8b-instruct`와 간단 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ca9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests, json, os\n",
    "\n",
    "OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'http://localhost:11434')\n",
    "OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'llama3.1:8b-instruct')\n",
    "\n",
    "def ollama_generate(prompt, model=OLLAMA_MODEL, host=OLLAMA_HOST):\n",
    "    try:\n",
    "        resp = requests.post(f\"{host}/api/generate\", json={\"model\": model, \"prompt\": prompt, \"stream\": False}, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json().get(\"response\",\"\").strip()\n",
    "    except Exception as e:\n",
    "        print(\"Ollama 요청 실패:\", e)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p = build_prompt(\"데이터 검증 규칙\", \"JSONL 스키마와 라벨 마스킹 체크 항목을 5개로 요약.\")\n",
    "    print(\"Ollama 요청...\")\n",
    "    o = ollama_generate(p)\n",
    "    if o:\n",
    "        print(o)\n",
    "    else:\n",
    "        print(\"⚠️ Ollama 서버가 실행 중인지 확인하세요 (default: localhost:11434).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889bb80",
   "metadata": {},
   "source": [
    "## 9) 결과 기록 저장 (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3faa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, time\n",
    "\n",
    "records = []\n",
    "for name, content in tests:\n",
    "    prompt = build_prompt(name, content)\n",
    "    base_out = generate(base_model, base_tok, prompt)\n",
    "    lora_out = generate(model, tokenizer, prompt)\n",
    "    records.append({\n",
    "        \"prompt_name\": name,\n",
    "        \"prompt_input\": content,\n",
    "        \"base_out\": base_out,\n",
    "        \"lora_out\": lora_out,\n",
    "        \"base_len\": len(base_out),\n",
    "        \"lora_len\": len(lora_out),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "out_csv = cfg.out_dir / f\"compare_{int(time.time())}.csv\"\n",
    "df.to_csv(out_csv, index=False, encoding='utf-8')\n",
    "print(\"Saved:\", out_csv)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d1eb0",
   "metadata": {},
   "source": [
    "## (선택) 10) Langfuse 간단 로깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY:\n",
    "    try:\n",
    "        from langfuse import Langfuse\n",
    "        lf = Langfuse(public_key=LANGFUSE_PUBLIC_KEY, secret_key=LANGFUSE_SECRET_KEY, host=LANGFUSE_HOST)\n",
    "        obs = lf.trace(name=\"week06_lora_sft\")\n",
    "        obs.generation(name=\"config\", metadata={\"cfg\": {**cfg.__dict__}})\n",
    "        for r in records:\n",
    "            obs.event(name=\"compare\", input=r[\"prompt_input\"], output=r[\"lora_out\"], metadata={\"base_len\": r[\"base_len\"], \"lora_len\": r[\"lora_len\"]})\n",
    "        obs.end()\n",
    "        print(\"Logged to Langfuse.\")\n",
    "    except Exception as e:\n",
    "        print(\"Langfuse 로깅 실패:\", e)\n",
    "else:\n",
    "    print(\"Langfuse 키가 없어 로깅 생략.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b2c18",
   "metadata": {},
   "source": [
    "\n",
    "## 11) 로컬 실행 팁\n",
    "\n",
    "- **GPU 권장**: QLoRA(4bit) 사용 시 8–12GB VRAM에서도 1B 모델 학습이 가능합니다.  \n",
    "- **CPU만 있을 때**: `cfg.use_qlora=False`로 두고, 매우 작은 모델(예: 500M〜1B)을 사용하세요. 속도는 느리지만 개념 체득은 가능합니다.\n",
    "- **메모리 부족(OOM)**: `max_seq_len↓`, `per_device_train_batch_size↓`, `grad_accum_steps↑`, 또는 8/4bit 로딩을 활용하세요.\n",
    "- **템플릿 일관성**: 학습/평가/서빙에서 동일 템플릿(BOS/EOS/SEP/구분자)을 사용하세요.\n",
    "- **데이터 품질**: SFT의 성패는 데이터에 달려 있습니다. JSONL 스키마/중복/금지어를 꼭 점검하세요.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajou-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
